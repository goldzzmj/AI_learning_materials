# Mini-VLLM (Nano-vLLM) å®Œæ•´å­¦ä¹ æŒ‡å—

## ğŸ“š é¡¹ç›®æ¦‚è¿°

**Nano-vLLM** æ˜¯ä¸€ä¸ªç”± DeepSeek ç ”ç©¶å‘˜å¼€å‘çš„è½»é‡çº§ VLLM æ¨ç†å¼•æ“å®ç°ï¼Œä»…çº¦ **1200 è¡Œ Python ä»£ç **ï¼Œå®Œæ•´å®ç°äº†ç”Ÿäº§çº§ VLLM çš„æ ¸å¿ƒåŠŸèƒ½ã€‚å®ƒæ˜¯å­¦ä¹ å¤§æ¨¡å‹æ¨ç†å¼•æ“çš„ç»ä½³æ•™æã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸš€ **é«˜æ€§èƒ½ç¦»çº¿æ¨ç†** - é€Ÿåº¦åª²ç¾åŸç‰ˆ VLLM
- ğŸ“– **ä»£ç ç®€æ´æ˜“è¯»** - çº¦ 1200 è¡Œ Python ä»£ç 
- âš¡ **å®Œæ•´ä¼˜åŒ–å¥—ä»¶** - å‰ç¼€ç¼“å­˜ã€å¼ é‡å¹¶è¡Œã€Torch ç¼–è¯‘ã€CUDA Graph ç­‰

---

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
nano-vllm/
â”œâ”€â”€ nanovllm/
â”‚   â”œâ”€â”€ __init__.py              # åŒ…å…¥å£ï¼Œå¯¼å‡º LLM å’Œ SamplingParams
â”‚   â”œâ”€â”€ config.py                # é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ llm.py                   # LLM ç±»ï¼ˆç»§æ‰¿è‡ª LLMEngineï¼‰
â”‚   â”œâ”€â”€ sampling_params.py       # é‡‡æ ·å‚æ•°é…ç½®
â”‚   â”œâ”€â”€ engine/                  # æ¨ç†å¼•æ“æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ sequence.py          # åºåˆ—ï¼ˆè¯·æ±‚ï¼‰çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ block_manager.py     # KV Cache å—ç®¡ç†å™¨ï¼ˆPagedAttentionï¼‰
â”‚   â”‚   â”œâ”€â”€ scheduler.py         # è¯·æ±‚è°ƒåº¦å™¨
â”‚   â”‚   â”œâ”€â”€ model_runner.py      # æ¨¡å‹è¿è¡Œå™¨
â”‚   â”‚   â””â”€â”€ llm_engine.py        # LLM å¼•æ“ä¸»ç±»
â”‚   â”œâ”€â”€ layers/                  # ç¥ç»ç½‘ç»œå±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ attention.py         # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå« Triton Kernelï¼‰
â”‚   â”‚   â”œâ”€â”€ linear.py            # çº¿æ€§å±‚ï¼ˆæ”¯æŒå¼ é‡å¹¶è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ layernorm.py         # RMSNorm å±‚
â”‚   â”‚   â”œâ”€â”€ activation.py        # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ rotary_embedding.py  # æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ embed_head.py        # è¯åµŒå…¥å’Œè¾“å‡ºå¤´
â”‚   â”‚   â””â”€â”€ sampler.py           # é‡‡æ ·å™¨
â”‚   â”œâ”€â”€ models/                  # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ qwen3.py             # Qwen3 æ¨¡å‹å®ç°
â”‚   â””â”€â”€ utils/                   # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ context.py           # å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†
â”‚       â””â”€â”€ loader.py            # æ¨¡å‹æƒé‡åŠ è½½
â”œâ”€â”€ example.py                   # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ bench.py                     # æ€§èƒ½æµ‹è¯•
```

---

## ğŸ¯ æ¨èå­¦ä¹ é¡ºåº

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¦‚å¿µï¼ˆå»ºç«‹æ•´ä½“è®¤çŸ¥ï¼‰
1. **sampling_params.py** - ç†è§£é‡‡æ ·å‚æ•°
2. **config.py** - ç†è§£é…ç½®ç³»ç»Ÿ
3. **utils/context.py** - ç†è§£å…¨å±€ä¸Šä¸‹æ–‡

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæ•°æ®ç»“æ„ï¼ˆç†è§£è¯·æ±‚ç®¡ç†ï¼‰
4. **engine/sequence.py** - ç†è§£åºåˆ—ï¼ˆè¯·æ±‚ï¼‰çš„ç”Ÿå‘½å‘¨æœŸ
5. **engine/block_manager.py** - ç†è§£ PagedAttention æ ¸å¿ƒ

### ç¬¬ä¸‰é˜¶æ®µï¼šè°ƒåº¦ç³»ç»Ÿï¼ˆç†è§£æ‰¹å¤„ç†ï¼‰
6. **engine/scheduler.py** - ç†è§£è¯·æ±‚è°ƒåº¦ç­–ç•¥

### ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹æ‰§è¡Œï¼ˆç†è§£æ¨ç†æµç¨‹ï¼‰
7. **layers/linear.py** - ç†è§£å¼ é‡å¹¶è¡Œ
8. **layers/layernorm.py** - ç†è§£å½’ä¸€åŒ–å±‚
9. **layers/activation.py** - ç†è§£æ¿€æ´»å‡½æ•°
10. **layers/rotary_embedding.py** - ç†è§£ä½ç½®ç¼–ç 
11. **layers/attention.py** - ç†è§£æ³¨æ„åŠ›è®¡ç®—
12. **layers/embed_head.py** - ç†è§£åµŒå…¥å±‚
13. **layers/sampler.py** - ç†è§£é‡‡æ ·ç­–ç•¥

### ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹æ¶æ„
14. **models/qwen3.py** - ç†è§£å®Œæ•´æ¨¡å‹ç»“æ„
15. **utils/loader.py** - ç†è§£æƒé‡åŠ è½½

### ç¬¬å…­é˜¶æ®µï¼šå¼•æ“æ ¸å¿ƒ
16. **engine/model_runner.py** - ç†è§£æ¨¡å‹è¿è¡Œ
17. **engine/llm_engine.py** - ç†è§£å¼•æ“ä¸»å¾ªç¯
18. **llm.py** - æœ€ç»ˆæ¥å£

---

## ğŸ” é€è¡Œä»£ç è¯¦è§£


### 1. sampling_params.py - é‡‡æ ·å‚æ•°é…ç½®

```python
# å¯¼å…¥ dataclass è£…é¥°å™¨ï¼Œç”¨äºåˆ›å»ºç®€æ´çš„æ•°æ®ç±»
from dataclasses import dataclass


@dataclass  # è‡ªåŠ¨åˆ›å»º __init__, __repr__, __eq__ ç­‰æ–¹æ³•
class SamplingParams:
    """
    é‡‡æ ·å‚æ•°ç±» - æ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„éšæœºæ€§å’Œé•¿åº¦
    
    åœ¨ LLM æ¨ç†ä¸­ï¼Œé‡‡æ ·å‚æ•°å†³å®šäº†æ¨¡å‹å¦‚ä½•ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼š
    - temperature: æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¾“å‡ºè¶Šå¤šæ ·
    - max_tokens: ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦é™åˆ¶
    - ignore_eos: æ˜¯å¦å¿½ç•¥ç»“æŸæ ‡è®°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    """
    temperature: float = 1.0      # æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤1.0è¡¨ç¤ºæ ‡å‡†é‡‡æ ·
    max_tokens: int = 64          # æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé»˜è®¤64
    ignore_eos: bool = False      # æ˜¯å¦å¿½ç•¥EOSæ ‡è®°ï¼Œé»˜è®¤False

    def __post_init__(self):
        """
        åˆå§‹åŒ–åéªŒè¯å‚æ•°æœ‰æ•ˆæ€§
        
        ä¸ºä»€ä¹ˆ temperature ä¸èƒ½å¤ªå°ï¼Ÿ
        - temperature â†’ 0 æ—¶ï¼Œsoftmax é€€åŒ–ä¸º argmaxï¼ˆè´ªå©ªè§£ç ï¼‰
        - æœ¬é¡¹ç›®ä¸ºäº†ç®€åŒ–ï¼Œç¦æ­¢ä½¿ç”¨è´ªå©ªè§£ç 
        """
        assert self.temperature > 1e-10, "greedy sampling is not permitted"
```

**æ ¸å¿ƒæ¦‚å¿µè§£é‡Šï¼š**

| å‚æ•° | ä½œç”¨ | å…¸å‹å€¼ |
|------|------|--------|
| temperature | æ§åˆ¶é‡‡æ ·éšæœºæ€§ | 0.6-1.0 |
| max_tokens | é™åˆ¶ç”Ÿæˆé•¿åº¦ | 64-2048 |
| ignore_eos | æµ‹è¯•æ—¶å¿½ç•¥ç»“æŸæ ‡è®° | False |

**æ¸©åº¦å‚æ•°è¯¦è§£ï¼š**
```
temperature = 1.0: æ ‡å‡†éšæœºé‡‡æ ·
temperature < 1.0: æ›´ä¿å®ˆï¼Œå€¾å‘äºé«˜æ¦‚ç‡è¯
temperature > 1.0: æ›´éšæœºï¼Œå¢åŠ å¤šæ ·æ€§
```

---

### 2. config.py - é…ç½®ç³»ç»Ÿ

```python
import os                          # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè·¯å¾„æ£€æŸ¥
from dataclasses import dataclass  # æ•°æ®ç±»è£…é¥°å™¨
from transformers import AutoConfig  # HuggingFace é…ç½®åŠ è½½å™¨


@dataclass
class Config:
    """
    Nano-vLLM å…¨å±€é…ç½®ç±»
    
    åŒ…å«æ‰€æœ‰å½±å“æ¨ç†è¡Œä¸ºçš„å‚æ•°ï¼Œåˆ†ä¸ºå‡ ç±»ï¼š
    1. æ‰¹å¤„ç†å‚æ•°ï¼šæ§åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡å’Œtokenæ•°
    2. æ¨¡å‹å‚æ•°ï¼šæ¨¡å‹è·¯å¾„å’Œé•¿åº¦é™åˆ¶
    3. æ˜¾å­˜å‚æ•°ï¼šGPU å†…å­˜ä½¿ç”¨ç­–ç•¥
    4. å¹¶è¡Œå‚æ•°ï¼šå¼ é‡å¹¶è¡Œè®¾ç½®
    5. ä¼˜åŒ–å‚æ•°ï¼šCUDA Graph ç­‰ä¼˜åŒ–å¼€å…³
    6. KV Cacheå‚æ•°ï¼šå—å¤§å°å’Œæ•°é‡
    """
    
    # ==================== åŸºç¡€å‚æ•° ====================
    model: str                              # æ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€å‚æ•°ï¼‰
    
    # ==================== æ‰¹å¤„ç†å‚æ•° ====================
    max_num_batched_tokens: int = 16384     # å•æ¬¡è¿­ä»£æœ€å¤§tokenæ•°
    max_num_seqs: int = 512                 # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    
    # ==================== æ¨¡å‹å‚æ•° ====================
    max_model_len: int = 4096               # æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    
    # ==================== æ˜¾å­˜å‚æ•° ====================
    gpu_memory_utilization: float = 0.9     # GPUæ˜¾å­˜ä½¿ç”¨ç‡ï¼ˆ0-1ï¼‰
    
    # ==================== å¹¶è¡Œå‚æ•° ====================
    tensor_parallel_size: int = 1           # å¼ é‡å¹¶è¡Œåº¦ï¼ˆGPUæ•°ï¼‰
    
    # ==================== ä¼˜åŒ–å‚æ•° ====================
    enforce_eager: bool = False             # å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼ï¼ˆç¦ç”¨CUDA Graphï¼‰
    
    # ==================== å†…éƒ¨çŠ¶æ€ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰====================
    hf_config: AutoConfig | None = None     # HuggingFaceæ¨¡å‹é…ç½®
    eos: int = -1                           # ç»“æŸæ ‡è®°IDï¼ˆä»tokenizerè·å–ï¼‰
    
    # ==================== KV Cacheå‚æ•° ====================
    kvcache_block_size: int = 256           # æ¯ä¸ªKVå—å­˜å‚¨çš„tokenæ•°
    num_kvcache_blocks: int = -1            # KVå—æ€»æ•°ï¼ˆè¿è¡Œæ—¶è®¡ç®—ï¼‰

    def __post_init__(self):
        """
        é…ç½®éªŒè¯å’Œåˆå§‹åŒ–
        
        æ‰§è¡Œä»¥ä¸‹æ£€æŸ¥ï¼š
        1. æ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯æœ‰æ•ˆç›®å½•
        2. å—å¤§å°å¿…é¡»æ˜¯256çš„å€æ•°ï¼ˆå¯¹é½GPUå†…å­˜ï¼‰
        3. å¼ é‡å¹¶è¡Œåº¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        4. åŠ è½½HuggingFaceé…ç½®
        5. ç¡®ä¿max_model_lenä¸è¶…è¿‡æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦
        6. ç¡®ä¿æ‰¹å¤„ç†tokenæ•°ä¸Šé™ä¸å°äºæ¨¡å‹é•¿åº¦
        """
        # éªŒè¯æ¨¡å‹è·¯å¾„å­˜åœ¨ä¸”æ˜¯ç›®å½•
        assert os.path.isdir(self.model)
        
        # å—å¤§å°å¿…é¡»æ˜¯256çš„å€æ•° - è¿™æ˜¯GPUå†…å­˜å¯¹é½çš„è¦æ±‚
        # 256ä¸ªtokençš„å—å¤§å°æ˜¯æ€§èƒ½å’Œå†…å­˜ç®¡ç†çš„å¹³è¡¡ç‚¹
        assert self.kvcache_block_size % 256 == 0
        
        # å¼ é‡å¹¶è¡Œåº¦é™åˆ¶ï¼šè‡³å°‘1ä¸ªGPUï¼Œæœ€å¤š8ä¸ª
        assert 1 <= self.tensor_parallel_size <= 8
        
        # ä» HuggingFace åŠ è½½æ¨¡å‹é…ç½®
        # åŒ…å«ï¼šå±‚æ•°ã€éšè—ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ã€vocabå¤§å°ç­‰
        self.hf_config = AutoConfig.from_pretrained(self.model)
        
        # å–ç”¨æˆ·è®¾ç½®å’Œæ¨¡å‹æ”¯æŒçš„æœ€å°å€¼ä½œä¸ºå®é™…æœ€å¤§é•¿åº¦
        # é˜²æ­¢ç”¨æˆ·è®¾ç½®è¶…è¿‡æ¨¡å‹èƒ½åŠ›çš„é•¿åº¦
        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)
        
        # ç¡®ä¿æ‰¹å¤„ç†tokenæ•°ä¸Šé™è¶³å¤Ÿå¤§
        # å¦åˆ™æ— æ³•å¤„ç†é•¿åºåˆ—
        assert self.max_num_batched_tokens >= self.max_model_len
```

**é…ç½®å‚æ•°è¯¦è§£ï¼š**

| å‚æ•°ç±»åˆ« | å‚æ•°å | ä½œç”¨ | é»˜è®¤å€¼ |
|---------|--------|------|--------|
| æ‰¹å¤„ç† | max_num_batched_tokens | å•æ¬¡å‰å‘ä¼ æ’­æœ€å¤§tokenæ•° | 16384 |
| æ‰¹å¤„ç† | max_num_seqs | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° | 512 |
| æ˜¾å­˜ | gpu_memory_utilization | GPUæ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹ | 0.9 |
| å¹¶è¡Œ | tensor_parallel_size | å¼ é‡å¹¶è¡ŒGPUæ•° | 1 |
| KV Cache | kvcache_block_size | æ¯å—å­˜å‚¨tokenæ•° | 256 |

---

### 3. utils/context.py - å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†

```python
from dataclasses import dataclass    # æ•°æ®ç±»è£…é¥°å™¨
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶


@dataclass
class Context:
    """
    æ¨ç†ä¸Šä¸‹æ–‡ - åœ¨æ¨¡å‹å‰å‘ä¼ æ’­æ—¶ä¼ é€’å…³é”®ä¿¡æ¯
    
    ä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ
    - æ³¨æ„åŠ›è®¡ç®—éœ€è¦çŸ¥é“å½“å‰æ˜¯ prefill è¿˜æ˜¯ decode é˜¶æ®µ
    - éœ€è¦ä¼ é€’åºåˆ—é•¿åº¦ã€å—è¡¨ç­‰è¿è¡Œæ—¶ä¿¡æ¯
    - é¿å…é€šè¿‡å‡½æ•°å‚æ•°å±‚å±‚ä¼ é€’ï¼Œç®€åŒ–ä»£ç 
    
    ç±»æ¯”ï¼šå°±åƒå‡½æ•°è°ƒç”¨çš„"ç¯å¢ƒå˜é‡"
    """
    
    # ==================== é˜¶æ®µæ ‡è¯† ====================
    is_prefill: bool = False            # True=é¢„å¡«å……é˜¶æ®µï¼ŒFalse=è§£ç é˜¶æ®µ
    
    # ==================== Prefillé˜¶æ®µå‚æ•° ====================
    # cu_seqlens: cumulative sequence lengthsï¼ˆç´¯ç§¯åºåˆ—é•¿åº¦ï¼‰
    # ç”¨äºå˜é•¿åºåˆ—çš„æ‰¹å¤„ç†ï¼Œæ ¼å¼ï¼š[0, len1, len1+len2, ...]
    cu_seqlens_q: torch.Tensor | None = None   # Queryåºåˆ—ç´¯ç§¯é•¿åº¦
    cu_seqlens_k: torch.Tensor | None = None   # Keyåºåˆ—ç´¯ç§¯é•¿åº¦
    max_seqlen_q: int = 0                      # æœ€å¤§Queryåºåˆ—é•¿åº¦
    max_seqlen_k: int = 0                      # æœ€å¤§Keyåºåˆ—é•¿åº¦
    
    # ==================== KV Cacheå‚æ•° ====================
    # slot_mapping: æ¯ä¸ªtokenåœ¨KV Cacheä¸­çš„å­˜å‚¨ä½ç½®
    # ç”¨äºå°†æ–°è®¡ç®—çš„KVå€¼å†™å…¥æ­£ç¡®çš„ä½ç½®
    slot_mapping: torch.Tensor | None = None
    
    # ==================== Decodeé˜¶æ®µå‚æ•° ====================
    # context_lens: æ¯ä¸ªåºåˆ—çš„å½“å‰é•¿åº¦ï¼ˆç”¨äºdecodeé˜¶æ®µï¼‰
    context_lens: torch.Tensor | None = None
    
    # ==================== å—è¡¨å‚æ•° ====================
    # block_tables: é€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„è¡¨
    # shape: [batch_size, max_num_blocks]
    block_tables: torch.Tensor | None = None


# ==================== å…¨å±€ä¸Šä¸‹æ–‡å®ä¾‹ ====================
# ä½¿ç”¨å…¨å±€å˜é‡å­˜å‚¨å½“å‰ä¸Šä¸‹æ–‡
# æ³¨æ„ï¼šè¿™æ˜¯å•çº¿ç¨‹è®¾è®¡ï¼Œå¤šçº¿ç¨‹éœ€è¦ä¿®æ”¹
_CONTEXT = Context()


def get_context():
    """
    è·å–å½“å‰å…¨å±€ä¸Šä¸‹æ–‡
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - Attention.forward() ä¸­åˆ¤æ–­å½“å‰é˜¶æ®µ
    - è·å–slot_mappingå†™å…¥KV Cache
    """
    return _CONTEXT


def set_context(is_prefill, cu_seqlens_q=None, cu_seqlens_k=None, 
                max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, 
                context_lens=None, block_tables=None):
    """
    è®¾ç½®å…¨å±€ä¸Šä¸‹æ–‡
    
    åœ¨æ¯æ¬¡æ¨¡å‹è¿è¡Œå‰è°ƒç”¨ï¼Œè®¾ç½®æ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    å‚æ•°è®¾è®¡åŸç†ï¼š
    - is_prefill æ˜¯å¿…éœ€çš„ï¼Œå…¶ä»–éƒ½æ˜¯å¯é€‰çš„
    - Prefillé˜¶æ®µéœ€è¦ cu_seqlens å’Œ max_seqlen
    - Decodeé˜¶æ®µéœ€è¦ context_lens å’Œ block_tables
    """
    global _CONTEXT
    _CONTEXT = Context(is_prefill, cu_seqlens_q, cu_seqlens_k, 
                       max_seqlen_q, max_seqlen_k, slot_mapping, 
                       context_lens, block_tables)


def reset_context():
    """
    é‡ç½®ä¸Šä¸‹æ–‡ä¸ºé»˜è®¤å€¼
    
    åœ¨æ¯æ¬¡æ¨ç†å®Œæˆåè°ƒç”¨ï¼Œé˜²æ­¢æ±¡æŸ“ä¸‹ä¸€æ¬¡æ¨ç†
    """
    global _CONTEXT
    _CONTEXT = Context()
```

**ä¸Šä¸‹æ–‡ä½¿ç”¨æµç¨‹ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prefill é˜¶æ®µ                                            â”‚
â”‚  set_context(is_prefill=True, cu_seqlens_q=..., ...)    â”‚
â”‚  model.forward(input_ids, positions)                    â”‚
â”‚  Attention å†…éƒ¨: get_context() è·å–ä¿¡æ¯                  â”‚
â”‚  reset_context()                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decode é˜¶æ®µ                                             â”‚
â”‚  set_context(is_prefill=False, context_lens=..., ...)   â”‚
â”‚  model.forward(input_ids, positions)                    â”‚
â”‚  Attention å†…éƒ¨: get_context() è·å–ä¿¡æ¯                  â”‚
â”‚  reset_context()                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4. engine/sequence.py - åºåˆ—ï¼ˆè¯·æ±‚ï¼‰çŠ¶æ€ç®¡ç†

```python
from copy import copy               # æµ…æ‹·è´å‡½æ•°
from enum import Enum, auto         # æšä¸¾ç±»å‹
from itertools import count         # è®¡æ•°å™¨ç”Ÿæˆå™¨

from nanovllm.sampling_params import SamplingParams  # é‡‡æ ·å‚æ•°


class SequenceStatus(Enum):
    """
    åºåˆ—çŠ¶æ€æšä¸¾
    
    WAITING:  ç­‰å¾…è°ƒåº¦ï¼ˆåˆšåŠ å…¥æˆ–è¢«æ‰“æ–­ï¼‰
    RUNNING:  æ­£åœ¨è¿è¡Œï¼ˆæ­£åœ¨GPUä¸Šè®¡ç®—ï¼‰
    FINISHED: å·²å®Œæˆç”Ÿæˆ
    
    çŠ¶æ€è½¬æ¢å›¾ï¼š
    WAITING â†’ RUNNING â†’ FINISHED
        â†‘___________|
        (è¢«æŠ¢å æ—¶å›é€€)
    """
    WAITING = auto()    # è‡ªåŠ¨åˆ†é…é€’å¢çš„æ•´æ•°å€¼
    RUNNING = auto()
    FINISHED = auto()


class Sequence:
    """
    åºåˆ—ç±» - è¡¨ç¤ºä¸€ä¸ªæ¨ç†è¯·æ±‚
    
    ä¸€ä¸ª Sequence å¯¹åº”ç”¨æˆ·çš„ä¸€æ¬¡è¯·æ±‚ï¼ŒåŒ…å«ï¼š
    - è¾“å…¥çš„ prompt tokens
    - ç”Ÿæˆçš„ completion tokens
    - å½“å‰çš„æ‰§è¡ŒçŠ¶æ€
    - åˆ†é…çš„ KV Cache å—è¡¨
    
    ç±»æ¯”ï¼šå°±åƒä¸€ä¸ª"ä»»åŠ¡å¡ç‰‡"ï¼Œè®°å½•äº†ä»»åŠ¡çš„æ‰€æœ‰ä¿¡æ¯
    """
    
    # ==================== ç±»å±æ€§ ====================
    block_size = 256                    # å—å¤§å°ï¼ˆæ‰€æœ‰åºåˆ—å…±äº«ï¼‰
    counter = count()                   # åºåˆ—IDç”Ÿæˆå™¨ï¼Œä»0å¼€å§‹é€’å¢
    
    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        """
        åˆå§‹åŒ–åºåˆ—
        
        Args:
            token_ids: prompt çš„ token ID åˆ—è¡¨
            sampling_params: é‡‡æ ·å‚æ•°
        """
        # -------------------- åŸºç¡€ä¿¡æ¯ --------------------
        self.seq_id = next(Sequence.counter)           # å”¯ä¸€åºåˆ—ID
        self.status = SequenceStatus.WAITING           # åˆå§‹çŠ¶æ€ï¼šç­‰å¾…
        
        # -------------------- Tokenåºåˆ— --------------------
        self.token_ids = copy(token_ids)               # æ‰€æœ‰tokenï¼ˆprompt+ç”Ÿæˆï¼‰
        self.last_token = token_ids[-1]                # æœ€åä¸€ä¸ªtokenï¼ˆç”¨äºdecodeï¼‰
        self.num_tokens = len(self.token_ids)          # å½“å‰æ€»tokenæ•°
        self.num_prompt_tokens = len(token_ids)        # promptçš„tokenæ•°
        
        # -------------------- å‰ç¼€ç¼“å­˜ --------------------
        # num_cached_tokens: å‘½ä¸­å‰ç¼€ç¼“å­˜çš„tokenæ•°
        # è¿™äº›tokenä¸éœ€è¦é‡æ–°è®¡ç®—ï¼Œç›´æ¥ä»ç¼“å­˜è¯»å–
        self.num_cached_tokens = 0
        
        # -------------------- å—è¡¨ --------------------
        # block_table: é€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„
        # ä¾‹å¦‚ï¼š[7, 3, 5] è¡¨ç¤ºé€»è¾‘å—0â†’ç‰©ç†å—7ï¼Œé€»è¾‘å—1â†’ç‰©ç†å—3...
        self.block_table = []
        
        # -------------------- é‡‡æ ·å‚æ•° --------------------
        self.temperature = sampling_params.temperature
        self.max_tokens = sampling_params.max_tokens
        self.ignore_eos = sampling_params.ignore_eos

    # ==================== é­”æœ¯æ–¹æ³• ====================
    def __len__(self):
        """è¿”å›å½“å‰åºåˆ—é•¿åº¦ï¼ˆtokenæ•°ï¼‰"""
        return self.num_tokens

    def __getitem__(self, key):
        """æ”¯æŒç´¢å¼•è®¿é—® token_ids"""
        return self.token_ids[key]

    # ==================== å±æ€§ ====================
    @property
    def is_finished(self):
        """æ£€æŸ¥åºåˆ—æ˜¯å¦å·²å®Œæˆ"""
        return self.status == SequenceStatus.FINISHED

    @property
    def num_completion_tokens(self):
        """å·²ç”Ÿæˆçš„tokenæ•°ï¼ˆä¸å«promptï¼‰"""
        return self.num_tokens - self.num_prompt_tokens

    @property
    def prompt_token_ids(self):
        """è·å–promptéƒ¨åˆ†çš„token IDs"""
        return self.token_ids[:self.num_prompt_tokens]

    @property
    def completion_token_ids(self):
        """è·å–ç”Ÿæˆéƒ¨åˆ†çš„token IDs"""
        return self.token_ids[self.num_prompt_tokens:]

    @property
    def num_cached_blocks(self):
        """å‘½ä¸­ç¼“å­˜çš„å—æ•°"""
        return self.num_cached_tokens // self.block_size

    @property
    def num_blocks(self):
        """å½“å‰éœ€è¦çš„æ€»å—æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰"""
        # (num_tokens + block_size - 1) // block_size æ˜¯å‘ä¸Šå–æ•´å…¬å¼
        return (self.num_tokens + self.block_size - 1) // self.block_size

    @property
    def last_block_num_tokens(self):
        """æœ€åä¸€ä¸ªå—ä¸­çš„tokenæ•°"""
        return self.num_tokens - (self.num_blocks - 1) * self.block_size

    # ==================== æ–¹æ³• ====================
    def block(self, i):
        """
        è·å–ç¬¬ i ä¸ªé€»è¾‘å—ä¸­çš„ token IDs
        
        Args:
            i: å—ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
        
        Returns:
            è¯¥å—åŒ…å«çš„ token ID åˆ—è¡¨
        """
        assert 0 <= i < self.num_blocks
        start = i * self.block_size
        end = (i + 1) * self.block_size
        return self.token_ids[start:end]

    def append_token(self, token_id: int):
        """
        è¿½åŠ ä¸€ä¸ªæ–°tokenåˆ°åºåˆ—
        
        åœ¨ decode é˜¶æ®µï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°tokenå°±è°ƒç”¨æ­¤æ–¹æ³•
        
        Args:
            token_id: æ–°ç”Ÿæˆçš„token ID
        """
        self.token_ids.append(token_id)     # æ·»åŠ åˆ°tokenåˆ—è¡¨
        self.last_token = token_id           # æ›´æ–°æœ€åä¸€ä¸ªtoken
        self.num_tokens += 1                 # å¢åŠ tokenè®¡æ•°

    # ==================== åºåˆ—åŒ–æ”¯æŒ ====================
    def __getstate__(self):
        """
        è‡ªå®šä¹‰åºåˆ—åŒ– - ç”¨äºè¿›ç¨‹é—´é€šä¿¡
        
        ä¼˜åŒ–ç‚¹ï¼š
        - å¦‚æœåºåˆ—å·²å¼€å§‹ç”Ÿæˆï¼Œåªä¿å­˜æœ€åä¸€ä¸ªtokenè€Œä¸æ˜¯å…¨éƒ¨
        - å¤§å¹…å‡å°‘å¤šGPUé€šä¿¡æ—¶çš„æ•°æ®é‡
        
        è¿”å›çš„å…ƒç»„ï¼š
        (num_tokens, num_prompt_tokens, num_cached_tokens, block_table, token_data)
        """
        token_data = self.token_ids if self.num_completion_tokens == 0 else self.last_token
        return (self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, 
                self.block_table, token_data)

    def __setstate__(self, state):
        """
        è‡ªå®šä¹‰ååºåˆ—åŒ–
        
        æ ¹æ®åºåˆ—åŒ–æ—¶çš„çŠ¶æ€æ¢å¤å®Œæ•´åºåˆ—
        """
        # è§£åŒ…å‰4ä¸ªå›ºå®šå­—æ®µ
        self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table = state[:-1]
        
        # æ ¹æ®æ˜¯å¦å·²å¼€å§‹ç”Ÿæˆï¼Œæ¢å¤tokenæ•°æ®
        if self.num_completion_tokens == 0:
            # è¿˜æœªå¼€å§‹ç”Ÿæˆï¼Œæ¢å¤å®Œæ•´çš„tokenåˆ—è¡¨
            self.token_ids = state[-1]
        else:
            # å·²å¼€å§‹ç”Ÿæˆï¼Œæœ€åä¸€ä¸ªtokenå°±æ˜¯å½“å‰token
            self.last_token = state[-1]
```

**Sequence æ ¸å¿ƒæ¦‚å¿µå›¾ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sequence ç»“æ„                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  seq_id: 0                                                      â”‚
â”‚  status: RUNNING                                                â”‚
â”‚  token_ids: [The, cat, sat, on, the, mat, and, looked]         â”‚
â”‚  num_tokens: 8                                                  â”‚
â”‚  num_prompt_tokens: 5  (The cat sat on the)                    â”‚
â”‚  num_completion_tokens: 3  (mat, and, looked)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Block Table (block_size=4)                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Logical 0   â”‚ Logical 1   â”‚ Logical 2   â”‚                   â”‚
â”‚  â”‚  â†’ Phys 7   â”‚  â†’ Phys 3   â”‚  â†’ Phys 5   â”‚                   â”‚
â”‚  â”‚ [The,cat,   â”‚ [sat,on,    â”‚ [mat,and,   â”‚                   â”‚
â”‚  â”‚  sat,on]    â”‚  the,mat]   â”‚  looked]    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  num_blocks: 2 (å®é™…éœ€è¦2ä¸ªå®Œæ•´å—)                              â”‚
â”‚  last_block_num_tokens: 3 (ç¬¬2ä¸ªå—åªæœ‰3ä¸ªtoken)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---


### 5. engine/block_manager.py - PagedAttention æ ¸å¿ƒå®ç°

```python
from collections import deque        # åŒç«¯é˜Ÿåˆ—ï¼Œç”¨äºé«˜æ•ˆçš„å¤´å°¾æ“ä½œ
import xxhash                        # é«˜æ€§èƒ½éåŠ å¯†å“ˆå¸Œåº“
import numpy as np                   # æ•°å€¼è®¡ç®—åº“

from nanovllm.engine.sequence import Sequence  # åºåˆ—ç±»


class Block:
    """
    KV Cache ç‰©ç†å—
    
    ç±»æ¯”ï¼šå°±åƒå†…å­˜åˆ†é¡µç³»ç»Ÿä¸­çš„"ç‰©ç†é¡µæ¡†"
    æ¯ä¸ªå—å­˜å‚¨å›ºå®šæ•°é‡çš„tokençš„KVå€¼
    
    å…³é”®è®¾è®¡ï¼š
    - ref_count: å¼•ç”¨è®¡æ•°ï¼Œæ”¯æŒå—å…±äº«ï¼ˆcopy-on-writeï¼‰
    - hash: å—å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºå‰ç¼€ç¼“å­˜æŸ¥æ‰¾
    - token_ids: å—ä¸­å­˜å‚¨çš„token IDsï¼ˆç”¨äºéªŒè¯ç¼“å­˜å‘½ä¸­ï¼‰
    """
    
    def __init__(self, block_id):
        """
        åˆå§‹åŒ–å—
        
        Args:
            block_id: ç‰©ç†å—IDï¼ˆåœ¨blocksæ•°ç»„ä¸­çš„ç´¢å¼•ï¼‰
        """
        self.block_id = block_id        # ç‰©ç†å—ID
        self.ref_count = 0              # å¼•ç”¨è®¡æ•°ï¼Œ0è¡¨ç¤ºç©ºé—²
        self.hash = -1                  # å†…å®¹å“ˆå¸Œï¼Œ-1è¡¨ç¤ºæœªè®¡ç®—
        self.token_ids = []             # å—ä¸­çš„token IDs

    def update(self, hash: int, token_ids: list[int]):
        """
        æ›´æ–°å—çš„å†…å®¹ä¿¡æ¯
        
        åœ¨å—è¢«å¡«æ»¡æ—¶è°ƒç”¨ï¼Œè®¡ç®—å¹¶å­˜å‚¨å“ˆå¸Œå€¼
        
        Args:
            hash: å—å†…å®¹çš„å“ˆå¸Œå€¼
            token_ids: å—ä¸­çš„token IDs
        """
        self.hash = hash
        self.token_ids = token_ids

    def reset(self):
        """
        é‡ç½®å—ä¸ºåˆå§‹çŠ¶æ€
        
        åœ¨åˆ†é…å—æ—¶è°ƒç”¨
        """
        self.ref_count = 1              # æ–°åˆ†é…çš„å—å¼•ç”¨è®¡æ•°ä¸º1
        self.hash = -1                  # å“ˆå¸Œæœªè®¡ç®—
        self.token_ids = []             # æ¸…ç©ºtokenåˆ—è¡¨


class BlockManager:
    """
    å—ç®¡ç†å™¨ - PagedAttention çš„æ ¸å¿ƒå®ç°
    
    ç±»æ¯”ï¼šå°±åƒæ“ä½œç³»ç»Ÿçš„å†…å­˜ç®¡ç†å™¨
    - ç®¡ç†ç‰©ç†å—çš„åˆ†é…å’Œå›æ”¶
    - ç»´æŠ¤é€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„
    - å®ç°å‰ç¼€ç¼“å­˜ï¼ˆé€šè¿‡å“ˆå¸ŒæŸ¥æ‰¾ï¼‰
    
    æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š
    1. blocks: æ‰€æœ‰ç‰©ç†å—çš„æ•°ç»„
    2. hash_to_block_id: å“ˆå¸Œå€¼åˆ°ç‰©ç†å—IDçš„æ˜ å°„ï¼ˆå‰ç¼€ç¼“å­˜ï¼‰
    3. free_block_ids: ç©ºé—²å—IDé˜Ÿåˆ—
    4. used_block_ids: å·²ä½¿ç”¨å—IDé›†åˆ
    """
    
    def __init__(self, num_blocks: int, block_size: int):
        """
        åˆå§‹åŒ–å—ç®¡ç†å™¨
        
        Args:
            num_blocks: ç‰©ç†å—æ€»æ•°ï¼ˆç”±æ˜¾å­˜å¤§å°å†³å®šï¼‰
            block_size: æ¯ä¸ªå—å­˜å‚¨çš„tokenæ•°
        """
        self.block_size = block_size
        
        # åˆ›å»ºæ‰€æœ‰ç‰©ç†å—
        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]
        
        # å‰ç¼€ç¼“å­˜ï¼šå“ˆå¸Œå€¼ â†’ ç‰©ç†å—ID
        # ç”¨äºå¿«é€ŸæŸ¥æ‰¾ç›¸åŒå†…å®¹çš„å—
        self.hash_to_block_id: dict[int, int] = dict()
        
        # ç©ºé—²å—é˜Ÿåˆ— - ä½¿ç”¨dequeå®ç°O(1)çš„popleft
        self.free_block_ids: deque[int] = deque(range(num_blocks))
        
        # å·²ä½¿ç”¨å—é›†åˆ - ç”¨äºå¿«é€Ÿåˆ¤æ–­å—çŠ¶æ€
        self.used_block_ids: set[int] = set()

    @classmethod
    def compute_hash(cls, token_ids: list[int], prefix: int = -1):
        """
        è®¡ç®—tokenåºåˆ—çš„å“ˆå¸Œå€¼
        
        ä½¿ç”¨xxhash64ç®—æ³•ï¼Œç‰¹ç‚¹ï¼š
        - é€Ÿåº¦å¿«ï¼ˆæ¯”MD5/SHAå¿«å¾—å¤šï¼‰
        - ç¢°æ’ç‡ä½ï¼ˆè¶³å¤Ÿç”¨äºç¼“å­˜ï¼‰
        - éåŠ å¯†ï¼ˆä¸éœ€è¦å®‰å…¨æ€§ï¼‰
        
        æ”¯æŒå‰ç¼€å“ˆå¸Œé“¾ï¼š
        - prefixå‚æ•°æ˜¯å‰ä¸€ä¸ªå—çš„å“ˆå¸Œå€¼
        - è¿™æ ·å¯ä»¥æ£€æµ‹è¿ç»­å—åºåˆ—æ˜¯å¦åŒ¹é…
        
        Args:
            token_ids: token IDåˆ—è¡¨
            prefix: å‰ç¼€å“ˆå¸Œå€¼ï¼ˆ-1è¡¨ç¤ºæ— å‰ç¼€ï¼‰
        
        Returns:
            64ä½å“ˆå¸Œå€¼
        """
        h = xxhash.xxh64()
        if prefix != -1:
            # å°†å‰ç¼€å“ˆå¸Œä½œä¸ºç§å­ï¼Œå®ç°å“ˆå¸Œé“¾
            h.update(prefix.to_bytes(8, "little"))
        # å°†tokenæ•°ç»„è½¬ä¸ºå­—èŠ‚åºåˆ—
        h.update(np.array(token_ids).tobytes())
        return h.intdigest()

    def _allocate_block(self, block_id: int) -> Block:
        """
        åˆ†é…æŒ‡å®šIDçš„å—
        
        å†…éƒ¨æ–¹æ³•ï¼Œå°†å—ä»ç©ºé—²é˜Ÿåˆ—ç§»åˆ°ä½¿ç”¨é›†åˆ
        
        Args:
            block_id: è¦åˆ†é…çš„å—ID
        
        Returns:
            åˆ†é…åçš„Blockå¯¹è±¡
        """
        block = self.blocks[block_id]
        # æ–­è¨€ï¼šåªæœ‰ç©ºé—²å—æ‰èƒ½è¢«åˆ†é…
        assert block.ref_count == 0
        block.reset()                           # é‡ç½®å—çŠ¶æ€
        self.free_block_ids.remove(block_id)    # ä»ç©ºé—²é˜Ÿåˆ—ç§»é™¤
        self.used_block_ids.add(block_id)       # æ·»åŠ åˆ°ä½¿ç”¨é›†åˆ
        return self.blocks[block_id]

    def _deallocate_block(self, block_id: int) -> Block:
        """
        é‡Šæ”¾æŒ‡å®šIDçš„å—
        
        å†…éƒ¨æ–¹æ³•ï¼Œå°†å—ä»ä½¿ç”¨é›†åˆç§»åˆ°ç©ºé—²é˜Ÿåˆ—
        
        Args:
            block_id: è¦é‡Šæ”¾çš„å—ID
        """
        # æ–­è¨€ï¼šåªæœ‰å¼•ç”¨è®¡æ•°ä¸º0çš„å—æ‰èƒ½è¢«é‡Šæ”¾
        assert self.blocks[block_id].ref_count == 0
        self.used_block_ids.remove(block_id)    # ä»ä½¿ç”¨é›†åˆç§»é™¤
        self.free_block_ids.append(block_id)    # æ·»åŠ åˆ°ç©ºé—²é˜Ÿåˆ—å°¾éƒ¨

    def can_allocate(self, seq: Sequence) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—²å—åˆ†é…ç»™åºåˆ—
        
        Args:
            seq: è¦åˆ†é…çš„åºåˆ—
        
        Returns:
            æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—²å—
        """
        return len(self.free_block_ids) >= seq.num_blocks

    def allocate(self, seq: Sequence):
        """
        ä¸ºåºåˆ—åˆ†é…å— - PagedAttentionçš„æ ¸å¿ƒç®—æ³•
        
        è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿæœ€å¤æ‚çš„é€»è¾‘ä¹‹ä¸€ï¼š
        1. éå†åºåˆ—çš„æ¯ä¸ªé€»è¾‘å—
        2. è®¡ç®—å—çš„å“ˆå¸Œå€¼ï¼ˆæ”¯æŒå‰ç¼€é“¾ï¼‰
        3. å°è¯•ä»ç¼“å­˜ä¸­æ‰¾åˆ°åŒ¹é…çš„ç‰©ç†å—
        4. å¦‚æœç¼“å­˜å‘½ä¸­ï¼Œå¢åŠ å¼•ç”¨è®¡æ•°
        5. å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ†é…æ–°å—
        
        Args:
            seq: è¦åˆ†é…å—çš„åºåˆ—
        """
        # æ–­è¨€ï¼šåºåˆ—ä¸èƒ½å·²æœ‰å—è¡¨ï¼ˆé˜²æ­¢é‡å¤åˆ†é…ï¼‰
        assert not seq.block_table
        
        h = -1                      # åˆå§‹å“ˆå¸Œå€¼ï¼ˆæ— å‰ç¼€ï¼‰
        cache_miss = False          # æ˜¯å¦å‘ç”Ÿç¼“å­˜æœªå‘½ä¸­
        
        # éå†åºåˆ—çš„æ¯ä¸ªé€»è¾‘å—
        for i in range(seq.num_blocks):
            # è·å–å½“å‰é€»è¾‘å—çš„token IDs
            token_ids = seq.block(i)
            
            # è®¡ç®—å—çš„å“ˆå¸Œå€¼
            # åªæœ‰å®Œæ•´å—ï¼ˆå¡«æ»¡block_sizeä¸ªtokenï¼‰æ‰è®¡ç®—å“ˆå¸Œ
            # ä¸å®Œæ•´å—å“ˆå¸Œè®¾ä¸º-1ï¼Œä¸å‚ä¸ç¼“å­˜
            if len(token_ids) == self.block_size:
                # ä½¿ç”¨å‰ç¼€å“ˆå¸Œé“¾
                h = self.compute_hash(token_ids, h)
                block_id = self.hash_to_block_id.get(h, -1)
            else:
                h = -1
                block_id = -1
            
            # éªŒè¯ç¼“å­˜å‘½ä¸­ï¼ˆå“ˆå¸Œå¯èƒ½ç¢°æ’ï¼Œéœ€è¦äºŒæ¬¡éªŒè¯ï¼‰
            if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
                # ç¼“å­˜æœªå‘½ä¸­
                cache_miss = True
            
            if cache_miss:
                # ç¼“å­˜æœªå‘½ä¸­ï¼šåˆ†é…æ–°å—
                block_id = self.free_block_ids[0]
                block = self._allocate_block(block_id)
            else:
                # ç¼“å­˜å‘½ä¸­ï¼
                seq.num_cached_tokens += self.block_size  # å¢åŠ ç¼“å­˜å‘½ä¸­è®¡æ•°
                
                if block_id in self.used_block_ids:
                    # å—å·²è¢«ä½¿ç”¨ï¼Œå¢åŠ å¼•ç”¨è®¡æ•°ï¼ˆå…±äº«ï¼‰
                    block = self.blocks[block_id]
                    block.ref_count += 1
                else:
                    # å—åœ¨ç¼“å­˜ä½†æœªè¢«ä½¿ç”¨ï¼Œé‡æ–°åˆ†é…
                    block = self._allocate_block(block_id)
            
            # å¦‚æœæ˜¯å®Œæ•´å—ï¼Œæ›´æ–°å—çš„å“ˆå¸Œå’Œtokenä¿¡æ¯
            if h != -1:
                block.update(h, token_ids)
                self.hash_to_block_id[h] = block_id
            
            # å°†ç‰©ç†å—IDæ·»åŠ åˆ°åºåˆ—çš„å—è¡¨
            seq.block_table.append(block_id)

    def deallocate(self, seq: Sequence):
        """
        é‡Šæ”¾åºåˆ—å ç”¨çš„æ‰€æœ‰å—
        
        åœ¨åºåˆ—å®Œæˆæˆ–è¢«æŠ¢å æ—¶è°ƒç”¨
        
        Args:
            seq: è¦é‡Šæ”¾çš„åºåˆ—
        """
        # é€†åºéå†å—è¡¨ï¼ˆä»åå¾€å‰é‡Šæ”¾ï¼‰
        for block_id in reversed(seq.block_table):
            block = self.blocks[block_id]
            block.ref_count -= 1            # å‡å°‘å¼•ç”¨è®¡æ•°
            
            # å¼•ç”¨è®¡æ•°ä¸º0æ—¶ï¼ŒçœŸæ­£é‡Šæ”¾å—
            if block.ref_count == 0:
                self._deallocate_block(block_id)
        
        # é‡ç½®åºåˆ—çš„ç¼“å­˜çŠ¶æ€
        seq.num_cached_tokens = 0
        seq.block_table.clear()

    def can_append(self, seq: Sequence) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘åºåˆ—è¿½åŠ token
        
        åœ¨decodeé˜¶æ®µï¼Œå¯èƒ½éœ€è¦åˆ†é…æ–°å—
        
        Args:
            seq: è¦è¿½åŠ çš„åºåˆ—
        
        Returns:
            æ˜¯å¦å¯ä»¥è¿½åŠ 
        """
        # æ¡ä»¶ï¼šåºåˆ—é•¿åº¦å¯¹block_sizeå–æ¨¡ç­‰äº1æ—¶ï¼Œéœ€è¦æ–°å—
        # ä¾‹å¦‚ï¼šblock_size=4ï¼Œå½“len=5,9,13...æ—¶éœ€è¦æ–°å—
        # len % block_size == 1 è¡¨ç¤ºåˆšè¿›å…¥æ–°å—
        return len(self.free_block_ids) >= (len(seq) % self.block_size == 1)

    def may_append(self, seq: Sequence):
        """
        å¤„ç†åºåˆ—è¿½åŠ tokenæ—¶çš„å—æ“ä½œ
        
        åœ¨decodeé˜¶æ®µï¼Œå½“åºåˆ—è¿½åŠ æ–°tokenæ—¶ï¼š
        1. å¦‚æœåˆšè¿›å…¥æ–°å—ï¼Œåˆ†é…æ–°å—
        2. å¦‚æœåˆšå¡«æ»¡å—ï¼Œè®¡ç®—å¹¶å­˜å‚¨å“ˆå¸Œ
        
        Args:
            seq: æ­£åœ¨è¿½åŠ çš„åºåˆ—
        """
        block_table = seq.block_table
        last_block = self.blocks[block_table[-1]]
        
        # æƒ…å†µ1ï¼šåˆšè¿›å…¥æ–°å—ï¼ˆéœ€è¦åˆ†é…ï¼‰
        if len(seq) % self.block_size == 1:
            # æ–­è¨€ï¼šä¸Šä¸€ä¸ªå—å¿…é¡»æœ‰å“ˆå¸Œï¼ˆå·²å¡«æ»¡ï¼‰
            assert last_block.hash != -1
            # åˆ†é…æ–°å—
            block_id = self.free_block_ids[0]
            self._allocate_block(block_id)
            block_table.append(block_id)
        
        # æƒ…å†µ2ï¼šåˆšå¡«æ»¡å—ï¼ˆéœ€è¦è®¡ç®—å“ˆå¸Œï¼‰
        elif len(seq) % self.block_size == 0:
            # æ–­è¨€ï¼šå½“å‰å—ä¸åº”è¯¥æœ‰å“ˆå¸Œï¼ˆåˆšå¡«æ»¡ï¼‰
            assert last_block.hash == -1
            
            # è·å–å½“å‰å—çš„æ‰€æœ‰token
            token_ids = seq.block(seq.num_blocks - 1)
            
            # è·å–å‰ç¼€å“ˆå¸Œï¼ˆå¦‚æœæœ‰å‰ä¸€ä¸ªå—ï¼‰
            if len(block_table) > 1:
                prefix = self.blocks[block_table[-2]].hash
            else:
                prefix = -1
            
            # è®¡ç®—å¹¶å­˜å‚¨å“ˆå¸Œ
            h = self.compute_hash(token_ids, prefix)
            last_block.update(h, token_ids)
            self.hash_to_block_id[h] = last_block.block_id
        
        # æƒ…å†µ3ï¼šå—æœªå¡«æ»¡ï¼ˆæ— éœ€æ“ä½œï¼‰
        else:
            assert last_block.hash == -1
```

**PagedAttention æ ¸å¿ƒæ¦‚å¿µå›¾è§£ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PagedAttention åŸç†                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  é€»è¾‘å—ï¼ˆLogical Blocksï¼‰                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Block 0    â”‚  â”‚  Block 1    â”‚  â”‚  Block 2    â”‚                     â”‚
â”‚  â”‚ [The,cat,   â”‚  â”‚ [sat,on,    â”‚  â”‚ [mat,and,   â”‚                     â”‚
â”‚  â”‚  sat,on]    â”‚  â”‚  the,mat]   â”‚  â”‚  looked]    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                â”‚                â”‚                            â”‚
â”‚         â–¼                â–¼                â–¼                            â”‚
â”‚  Block Table: [7, 3, 5]  â† é€»è¾‘åˆ°ç‰©ç†çš„æ˜ å°„                              â”‚
â”‚                                                                         â”‚
â”‚  ç‰©ç†å—ï¼ˆPhysical Blocksï¼‰                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  0   1   2   3   4   5   6   7   8   9   10  11  ...       â”‚       â”‚
â”‚  â”‚ [ ] [ ] [ ] [B] [ ] [C] [ ] [A] [ ] [ ] [ ] [ ] ...       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         A=Block0  B=Block1  C=Block2                                   â”‚
â”‚                                                                         â”‚
â”‚  å…³é”®ç‰¹æ€§ï¼š                                                              â”‚
â”‚  1. ç‰©ç†å—ä¸è¿ç»­ - è§£å†³å†…å­˜ç¢ç‰‡é—®é¢˜                                       â”‚
â”‚  2. å—è¡¨æ˜ å°„ - çµæ´»ç®¡ç†å†…å­˜                                               â”‚
â”‚  3. å¼•ç”¨è®¡æ•° - æ”¯æŒå—å…±äº«ï¼ˆcopy-on-writeï¼‰                               â”‚
â”‚  4. å“ˆå¸Œç¼“å­˜ - å‰ç¼€åŒ¹é…é¿å…é‡å¤è®¡ç®—                                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å‰ç¼€ç¼“å­˜ç¤ºä¾‹ï¼š**

```
åºåˆ—A: "The cat sat on the mat" â†’ å—è¡¨ [7, 3]
åºåˆ—B: "The cat sat on the table" â†’ å—è¡¨ [7, 5]  (å…±äº«Block 0)

å“ˆå¸Œè¡¨:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hash("The cat...")  â”‚ â†’ 7      â”‚
â”‚ Hash("sat on...")   â”‚ â†’ 3      â”‚
â”‚ Hash("sat on...")   â”‚ â†’ 5      â”‚ (ä¸åŒå†…å®¹ï¼Œç›¸åŒå‰ç¼€é•¿åº¦)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 6. engine/scheduler.py - è¯·æ±‚è°ƒåº¦å™¨

```python
from collections import deque         # åŒç«¯é˜Ÿåˆ—

from nanovllm.config import Config    # é…ç½®ç±»
from nanovllm.engine.sequence import Sequence, SequenceStatus  # åºåˆ—ç›¸å…³
from nanovllm.engine.block_manager import BlockManager         # å—ç®¡ç†å™¨


class Scheduler:
    """
    è°ƒåº¦å™¨ - å†³å®šå“ªäº›è¯·æ±‚åœ¨ä½•æ—¶æ‰§è¡Œ
    
    ç±»æ¯”ï¼šå°±åƒæ“ä½œç³»ç»Ÿçš„è¿›ç¨‹è°ƒåº¦å™¨
    - ç®¡ç†ç­‰å¾…é˜Ÿåˆ—å’Œè¿è¡Œé˜Ÿåˆ—
    - å†³å®šä¸‹ä¸€ä¸ªæ‰§è¡Œå“ªä¸ªè¯·æ±‚
    - å¤„ç†èµ„æºä¸è¶³æ—¶çš„æŠ¢å 
    
    è°ƒåº¦ç­–ç•¥ï¼š
    1. ä¼˜å…ˆæ‰§è¡Œ prefillï¼ˆæ–°è¯·æ±‚ï¼‰
    2. ç„¶åæ‰§è¡Œ decodeï¼ˆç”Ÿæˆä¸­è¯·æ±‚ï¼‰
    3. èµ„æºä¸è¶³æ—¶æŠ¢å ä½ä¼˜å…ˆçº§è¯·æ±‚
    
    è¿™ç§ç­–ç•¥ä¿è¯äº†ï¼š
    - æ–°è¯·æ±‚èƒ½å¿«é€Ÿå¾—åˆ°å“åº”ï¼ˆä½å»¶è¿Ÿï¼‰
    - ç”Ÿæˆä¸­çš„è¯·æ±‚èƒ½æŒç»­è¿›è¡Œï¼ˆé«˜ååï¼‰
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            config: å…¨å±€é…ç½®
        """
        # ä»é…ç½®è¯»å–è°ƒåº¦å‚æ•°
        self.max_num_seqs = config.max_num_seqs                   # æœ€å¤§å¹¶å‘æ•°
        self.max_num_batched_tokens = config.max_num_batched_tokens  # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
        self.eos = config.eos                                      # ç»“æŸæ ‡è®°ID
        
        # åˆ›å»ºå—ç®¡ç†å™¨
        self.block_manager = BlockManager(
            config.num_kvcache_blocks, 
            config.kvcache_block_size
        )
        
        # ç­‰å¾…é˜Ÿåˆ— - æ–°è¯·æ±‚æˆ–è¢«æ‰“æ–­çš„è¯·æ±‚
        self.waiting: deque[Sequence] = deque()
        
        # è¿è¡Œé˜Ÿåˆ— - æ­£åœ¨GPUä¸Šæ‰§è¡Œçš„è¯·æ±‚
        self.running: deque[Sequence] = deque()

    def is_finished(self):
        """
        æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯·æ±‚éƒ½å·²å®Œæˆ
        
        Returns:
            True if ç­‰å¾…é˜Ÿåˆ—å’Œè¿è¡Œé˜Ÿåˆ—éƒ½ä¸ºç©º
        """
        return not self.waiting and not self.running

    def add(self, seq: Sequence):
        """
        æ·»åŠ æ–°è¯·æ±‚åˆ°ç­‰å¾…é˜Ÿåˆ—
        
        Args:
            seq: è¦æ·»åŠ çš„åºåˆ—
        """
        self.waiting.append(seq)

    def schedule(self) -> tuple[list[Sequence], bool]:
        """
        è°ƒåº¦è¯·æ±‚ - æ ¸å¿ƒè°ƒåº¦ç®—æ³•
        
        è¿”å›è¦æ‰§è¡Œçš„è¯·æ±‚åˆ—è¡¨å’Œæ˜¯å¦æ˜¯prefillé˜¶æ®µ
        
        è°ƒåº¦ç­–ç•¥ï¼š
        1. é¦–å…ˆå°è¯•è°ƒåº¦ waiting é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ï¼ˆprefillï¼‰
        2. å¦‚æœæ²¡æœ‰æ–°è¯·æ±‚ï¼Œè°ƒåº¦ running é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ï¼ˆdecodeï¼‰
        3. èµ„æºä¸è¶³æ—¶ï¼ŒæŠ¢å  running é˜Ÿåˆ—æœ«å°¾çš„è¯·æ±‚
        
        Returns:
            (scheduled_seqs, is_prefill): è°ƒåº¦çš„åºåˆ—åˆ—è¡¨å’Œæ˜¯å¦æ˜¯prefill
        """
        # ==================== Phase 1: Prefill ====================
        # ä¼˜å…ˆå¤„ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„æ–°è¯·æ±‚
        scheduled_seqs = []           # æœ¬æ¬¡è°ƒåº¦çš„åºåˆ—
        num_seqs = 0                  # å·²è°ƒåº¦åºåˆ—æ•°
        num_batched_tokens = 0        # å·²è°ƒåº¦tokenæ•°
        
        # å¾ªç¯ä»ç­‰å¾…é˜Ÿåˆ—å–è¯·æ±‚ï¼Œç›´åˆ°è¾¾åˆ°ä¸Šé™
        while self.waiting and num_seqs < self.max_num_seqs:
            # æŸ¥çœ‹é˜Ÿåˆ—å¤´éƒ¨è¯·æ±‚ï¼ˆä¸å–å‡ºï¼‰
            seq = self.waiting[0]
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ‰¹å¤„ç†tokenä¸Šé™
            # æ³¨æ„ï¼šåªè®¡ç®—æœªç¼“å­˜çš„tokenï¼ˆç¼“å­˜çš„ä¸éœ€è¦è®¡ç®—ï¼‰
            new_tokens = len(seq) - seq.num_cached_tokens
            if num_batched_tokens + new_tokens > self.max_num_batched_tokens:
                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå—åˆ†é…ç»™è¿™ä¸ªè¯·æ±‚
            if not self.block_manager.can_allocate(seq):
                break
            
            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œæ­£å¼è°ƒåº¦è¿™ä¸ªè¯·æ±‚
            num_seqs += 1
            self.block_manager.allocate(seq)           # åˆ†é…KVå—
            num_batched_tokens += new_tokens           # ç´¯åŠ tokenæ•°
            
            # æ›´æ–°åºåˆ—çŠ¶æ€
            seq.status = SequenceStatus.RUNNING
            
            # ä»ç­‰å¾…é˜Ÿåˆ—ç§»é™¤ï¼ŒåŠ å…¥è¿è¡Œé˜Ÿåˆ—
            self.waiting.popleft()
            self.running.append(seq)
            scheduled_seqs.append(seq)
        
        # å¦‚æœè°ƒåº¦äº†ä»»ä½•è¯·æ±‚ï¼Œè¿”å›è¿›è¡Œprefill
        if scheduled_seqs:
            return scheduled_seqs, True
        
        # ==================== Phase 2: Decode ====================
        # æ²¡æœ‰æ–°è¯·æ±‚ï¼Œå¤„ç†æ­£åœ¨ç”Ÿæˆçš„è¯·æ±‚
        # Decodeé˜¶æ®µæ¯ä¸ªè¯·æ±‚åªå¤„ç†1ä¸ªtoken
        while self.running and num_seqs < self.max_num_seqs:
            # ä»è¿è¡Œé˜Ÿåˆ—å¤´éƒ¨å–è¯·æ±‚
            seq = self.running.popleft()
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿½åŠ tokenï¼ˆå¯èƒ½éœ€è¦æ–°å—ï¼‰
            while not self.block_manager.can_append(seq):
                # èµ„æºä¸è¶³ï¼Œéœ€è¦æŠ¢å 
                if self.running:
                    # æŠ¢å è¿è¡Œé˜Ÿåˆ—æœ«å°¾çš„è¯·æ±‚ï¼ˆæœ€å°‘ä¼˜å…ˆï¼‰
                    self.preempt(self.running.pop())
                else:
                    # æ²¡æœ‰å…¶ä»–è¯·æ±‚å¯æŠ¢å ï¼Œåªèƒ½æŠ¢å å½“å‰è¯·æ±‚
                    self.preempt(seq)
                    break
            else:
                # can_appendè¿”å›Trueï¼Œå¯ä»¥æ‰§è¡Œdecode
                num_seqs += 1
                self.block_manager.may_append(seq)     # å¤„ç†å¯èƒ½çš„å—åˆ†é…
                scheduled_seqs.append(seq)
        
        # Decodeé˜¶æ®µå¿…é¡»è‡³å°‘è°ƒåº¦ä¸€ä¸ªè¯·æ±‚
        assert scheduled_seqs
        
        # å°†è°ƒåº¦çš„è¯·æ±‚æ”¾å›è¿è¡Œé˜Ÿåˆ—å¤´éƒ¨ï¼ˆä¿æŒé¡ºåºï¼‰
        self.running.extendleft(reversed(scheduled_seqs))
        
        return scheduled_seqs, False

    def preempt(self, seq: Sequence):
        """
        æŠ¢å è¯·æ±‚ - å°†è¿è¡Œä¸­çš„è¯·æ±‚æ”¾å›ç­‰å¾…é˜Ÿåˆ—
        
        åœ¨èµ„æºä¸è¶³æ—¶è°ƒç”¨ï¼Œé‡Šæ”¾è¯¥è¯·æ±‚å ç”¨çš„å—
        
        Args:
            seq: è¦æŠ¢å çš„åºåˆ—
        """
        seq.status = SequenceStatus.WAITING        # æ”¹å›ç­‰å¾…çŠ¶æ€
        self.block_manager.deallocate(seq)         # é‡Šæ”¾æ‰€æœ‰å—
        self.waiting.appendleft(seq)               # æ”¾åˆ°ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨ï¼ˆä¼˜å…ˆè°ƒåº¦ï¼‰

    def postprocess(self, seqs: list[Sequence], token_ids: list[int]) -> list[bool]:
        """
        åå¤„ç† - å¤„ç†æ¨¡å‹ç”Ÿæˆçš„token
        
        å°†ç”Ÿæˆçš„tokenæ·»åŠ åˆ°åºåˆ—ï¼Œæ£€æŸ¥æ˜¯å¦å®Œæˆ
        
        Args:
            seqs: æœ¬æ¬¡å¤„ç†çš„åºåˆ—
            token_ids: ç”Ÿæˆçš„token IDåˆ—è¡¨
        """
        for seq, token_id in zip(seqs, token_ids):
            # è¿½åŠ ç”Ÿæˆçš„token
            seq.append_token(token_id)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»“æŸæ¡ä»¶
            should_finish = False
            
            # æ¡ä»¶1ï¼šç”Ÿæˆäº†EOSæ ‡è®°ï¼ˆä¸”ä¸å…è®¸å¿½ç•¥ï¼‰
            if not seq.ignore_eos and token_id == self.eos:
                should_finish = True
            
            # æ¡ä»¶2ï¼šè¾¾åˆ°æœ€å¤§ç”Ÿæˆé•¿åº¦
            if seq.num_completion_tokens == seq.max_tokens:
                should_finish = True
            
            if should_finish:
                seq.status = SequenceStatus.FINISHED
                self.block_manager.deallocate(seq)     # é‡Šæ”¾å—
                self.running.remove(seq)                # ä»è¿è¡Œé˜Ÿåˆ—ç§»é™¤
```

**è°ƒåº¦æµç¨‹å›¾è§£ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           è°ƒåº¦æµç¨‹                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   ç­‰å¾…é˜Ÿåˆ— (Waiting)        è¿è¡Œé˜Ÿåˆ— (Running)         GPU              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚   â”‚  Seq A      â”‚          â”‚  Seq D      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º Prefill/Decode    â”‚
â”‚   â”‚  Seq B      â”‚          â”‚  Seq E      â”‚                             â”‚
â”‚   â”‚  Seq C      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼ schedule()                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Phase 1: Prefill                                               â”‚  â”‚
â”‚   â”‚  - ä»waitingå–è¯·æ±‚                                              â”‚  â”‚
â”‚   â”‚  - æ£€æŸ¥tokenä¸Šé™å’Œå—å¯ç”¨æ€§                                       â”‚  â”‚
â”‚   â”‚  - åˆ†é…å—ï¼ŒåŠ å…¥running                                          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                                                              â”‚
â”‚          â”‚ (å¦‚æœæ²¡æœ‰æ–°è¯·æ±‚)                                             â”‚
â”‚          â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Phase 2: Decode                                                â”‚  â”‚
â”‚   â”‚  - ä»runningå–è¯·æ±‚                                              â”‚  â”‚
â”‚   â”‚  - æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿½åŠ token                                         â”‚  â”‚
â”‚   â”‚  - èµ„æºä¸è¶³æ—¶æŠ¢å æœ«å°¾è¯·æ±‚                                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   æŠ¢å ç¤ºä¾‹ï¼š                                                             â”‚
â”‚   running = [D, E, F], éœ€è¦å—ä½†ä¸è¶³                                      â”‚
â”‚   â†’ æŠ¢å  F â†’ waiting = [F, A, B, C]                                     â”‚
â”‚   â†’ D, E ç»§ç»­ decode                                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---


### 7. layers/linear.py - çº¿æ€§å±‚ï¼ˆå¼ é‡å¹¶è¡Œå®ç°ï¼‰

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F       # ç¥ç»ç½‘ç»œå‡½æ•°
import torch.distributed as dist      # åˆ†å¸ƒå¼è®­ç»ƒ/æ¨ç†


def divide(numerator, denominator):
    """
    æ•´é™¤æ–­è¨€å‡½æ•°
    
    ç¡®ä¿åˆ†å­èƒ½è¢«åˆ†æ¯æ•´é™¤ï¼Œç”¨äºå¼ é‡å¹¶è¡Œåˆ‡åˆ†éªŒè¯
    
    Args:
        numerator: è¢«é™¤æ•°
        denominator: é™¤æ•°
    
    Returns:
        æ•´é™¤ç»“æœ
    """
    assert numerator % denominator == 0
    return numerator // denominator


class LinearBase(nn.Module):
    """
    çº¿æ€§å±‚åŸºç±» - æä¾›å¼ é‡å¹¶è¡Œçš„åŸºç¡€åŠŸèƒ½
    
    å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelism, TPï¼‰åŸç†ï¼š
    - å°†å¤§çŸ©é˜µæŒ‰è¡Œæˆ–åˆ—åˆ‡åˆ†åˆ°å¤šä¸ªGPU
    - æ¯ä¸ªGPUåªå­˜å‚¨éƒ¨åˆ†æƒé‡
    - å‰å‘ä¼ æ’­åé€šè¿‡all-reduceåˆå¹¶ç»“æœ
    
    ç±»æ¯”ï¼šå°±åƒæŠŠä¸€ä¸ªå¤§ä»»åŠ¡åˆ†ç»™å¤šä¸ªäººåšï¼Œæœ€åæ±‡æ€»ç»“æœ
    """
    
    def __init__(
        self,
        input_size: int,                  # è¾“å…¥ç»´åº¦
        output_size: int,                 # è¾“å‡ºç»´åº¦
        bias: bool = False,               # æ˜¯å¦ä½¿ç”¨åç½®
        tp_dim: int | None = None,        # å¼ é‡å¹¶è¡Œåˆ‡åˆ†ç»´åº¦ï¼ˆ0=åˆ—ï¼Œ1=è¡Œï¼‰
    ):
        super().__init__()
        self.tp_dim = tp_dim              # åˆ‡åˆ†ç»´åº¦
        self.tp_rank = dist.get_rank()    # å½“å‰GPUçš„rank
        self.tp_size = dist.get_world_size()  # æ€»GPUæ•°
        
        # åˆ›å»ºæƒé‡å‚æ•°
        self.weight = nn.Parameter(torch.empty(output_size, input_size))
        # é™„åŠ weight_loaderæ–¹æ³•ï¼Œç”¨äºåŠ è½½åˆ‡åˆ†åçš„æƒé‡
        self.weight.weight_loader = self.weight_loader
        
        # å¯é€‰çš„åç½®
        if bias:
            self.bias = nn.Parameter(torch.empty(output_size))
            self.bias.weight_loader = self.weight_loader
        else:
            self.register_parameter("bias", None)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å­ç±»å¿…é¡»å®ç°å‰å‘ä¼ æ’­"""
        raise NotImplementedError


class ReplicatedLinear(LinearBase):
    """
    å¤åˆ¶çº¿æ€§å±‚ - æƒé‡åœ¨æ‰€æœ‰GPUä¸Šå®Œå…¨ç›¸åŒ
    
    ç”¨äºä¸éœ€è¦åˆ‡åˆ†çš„å±‚ï¼ˆå¦‚æœ€ç»ˆçš„è¾“å‡ºå±‚ï¼‰
    """
    
    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        super().__init__(input_size, output_size, bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        """
        åŠ è½½æƒé‡ - ç›´æ¥å¤åˆ¶å®Œæ•´æƒé‡
        
        Args:
            param: ç›®æ ‡å‚æ•°
            loaded_weight: åŠ è½½çš„æƒé‡
        """
        param.data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """æ ‡å‡†çº¿æ€§å˜æ¢"""
        return F.linear(x, self.weight, self.bias)


class ColumnParallelLinear(LinearBase):
    """
    åˆ—å¹¶è¡Œçº¿æ€§å±‚ - æŒ‰è¾“å‡ºç»´åº¦åˆ‡åˆ†
    
    åˆ‡åˆ†æ–¹å¼ï¼šoutput_size â†’ output_size / tp_size
    
    æ•°å­¦åŸç†ï¼š
    Y = X @ W^T
    å°† W æŒ‰åˆ—åˆ‡åˆ†ï¼š[W1, W2, ..., Wn]
    æ¯ä¸ªGPUè®¡ç®—ï¼šYi = X @ Wi^T
    ç»“æœåœ¨è¾“å‡ºç»´åº¦æ‹¼æ¥
    
    å›¾ç¤ºï¼š
    è¾“å…¥ X: [batch, input_size]
    æƒé‡ W: [output_size, input_size]
    
    GPU 0: W0 = W[0:output//2, :]      GPU 1: W1 = W[output//2:, :]
           â†“                                   â†“
    Y0 = X @ W0^T                      Y1 = X @ W1^T
           â†“                                   â†“
    Y = [Y0, Y1]  (åœ¨è¾“å‡ºç»´åº¦æ‹¼æ¥)
    """
    
    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        # è®¡ç®—åˆ‡åˆ†åçš„è¾“å‡ºç»´åº¦
        tp_size = dist.get_world_size()
        super().__init__(input_size, divide(output_size, tp_size), bias, 0)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        """
        åŠ è½½åˆ—åˆ‡åˆ†çš„æƒé‡
        
        Args:
            param: ç›®æ ‡å‚æ•°ï¼ˆå·²åˆ‡åˆ†å¤§å°ï¼‰
            loaded_weight: å®Œæ•´æƒé‡
        """
        param_data = param.data
        # è®¡ç®—å½“å‰GPUè´Ÿè´£çš„åˆ‡ç‰‡
        shard_size = param_data.size(self.tp_dim)  # åˆ‡åˆ†åçš„å¤§å°
        start_idx = self.tp_rank * shard_size      # èµ·å§‹ç´¢å¼•
        # ä»å®Œæ•´æƒé‡ä¸­åˆ‡å–å¯¹åº”éƒ¨åˆ†
        loaded_weight = loaded_weight.narrow(self.tp_dim, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """åˆ—å¹¶è¡Œçº¿æ€§å˜æ¢"""
        return F.linear(x, self.weight, self.bias)


class MergedColumnParallelLinear(ColumnParallelLinear):
    """
    åˆå¹¶åˆ—å¹¶è¡Œçº¿æ€§å±‚ - æ”¯æŒå¤šä¸ªè¾“å‡ºåˆå¹¶åˆ‡åˆ†
    
    ç”¨äº QKV æŠ•å½±ä¸­çš„ gate_proj + up_proj åˆå¹¶
    
    å…¸å‹ç”¨ä¾‹ï¼š
    gate_up_proj = MergedColumnParallelLinear(hidden_size, [inter_size, inter_size])
    å®é™…åˆ›å»ºå¤§å°ä¸º 2 * inter_size çš„æƒé‡ï¼Œé€»è¾‘ä¸Šåˆ†ä¸ºä¸¤éƒ¨åˆ†
    """
    
    def __init__(
        self,
        input_size: int,
        output_sizes: list[int],          # å¤šä¸ªè¾“å‡ºå¤§å°
        bias: bool = False,
    ):
        self.output_sizes = output_sizes
        # æ€»è¾“å‡ºå¤§å° = æ‰€æœ‰è¾“å‡ºå¤§å°ä¹‹å’Œ
        super().__init__(input_size, sum(output_sizes), bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int):
        """
        åŠ è½½åˆå¹¶æƒé‡çš„ç‰¹å®šéƒ¨åˆ†
        
        Args:
            param: ç›®æ ‡å‚æ•°
            loaded_weight: åŠ è½½çš„æƒé‡
            loaded_shard_id: è¦åŠ è½½çš„éƒ¨åˆ†ç´¢å¼•ï¼ˆ0æˆ–1ï¼‰
        """
        param_data = param.data
        # è®¡ç®—è¯¥éƒ¨åˆ†åœ¨åˆ‡åˆ†åæƒé‡ä¸­çš„åç§»
        shard_offset = sum(self.output_sizes[:loaded_shard_id]) // self.tp_size
        shard_size = self.output_sizes[loaded_shard_id] // self.tp_size
        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)
        
        # å¯¹åŠ è½½çš„æƒé‡ä¹Ÿè¿›è¡Œåˆ‡åˆ†
        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]
        param_data.copy_(loaded_weight)


class QKVParallelLinear(ColumnParallelLinear):
    """
    QKVå¹¶è¡Œçº¿æ€§å±‚ - ä¸“é—¨ç”¨äºæ³¨æ„åŠ›QKVæŠ•å½±
    
    å°† Qã€Kã€V ä¸‰ä¸ªæŠ•å½±åˆå¹¶ä¸ºä¸€ä¸ªçº¿æ€§å±‚
    è¾“å‡ºæ ¼å¼ï¼š[Q_part, K_part, V_part]
    
    è®¾è®¡åŸå› ï¼š
    1. å‡å°‘kernel launchå¼€é”€ï¼ˆä¸€ä¸ªçŸ©é˜µä¹˜ä»£æ›¿ä¸‰ä¸ªï¼‰
    2. ç»Ÿä¸€å¤„ç†å¼ é‡å¹¶è¡Œåˆ‡åˆ†
    
    æƒé‡å¸ƒå±€ï¼š
    [Q_heads * head_dim, K_heads * head_dim, V_heads * head_dim]
    """
    
    def __init__(
        self,
        hidden_size: int,                 # éšè—å±‚å¤§å°
        head_size: int,                   # æ¯ä¸ªå¤´çš„å¤§å°
        total_num_heads: int,             # æ€»å¤´æ•°ï¼ˆQï¼‰
        total_num_kv_heads: int | None = None,  # K,Vå¤´æ•°ï¼ˆå¯èƒ½å°‘äºQï¼‰
        bias: bool = False,
    ):
        tp_size = dist.get_world_size()
        total_num_kv_heads = total_num_kv_heads or total_num_heads
        
        self.head_size = head_size
        # æ¯ä¸ªGPUè´Ÿè´£çš„å¤´æ•°
        self.num_heads = divide(total_num_heads, tp_size)
        self.num_kv_heads = divide(total_num_kv_heads, tp_size)
        
        # æ€»è¾“å‡ºå¤§å° = Qå¤§å° + Kå¤§å° + Vå¤§å°
        output_size = (total_num_heads + 2 * total_num_kv_heads) * head_size
        super().__init__(hidden_size, output_size, bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str):
        """
        åŠ è½½Qã€Kæˆ–Vçš„æƒé‡
        
        Args:
            param: ç›®æ ‡å‚æ•°
            loaded_weight: åŠ è½½çš„æƒé‡
            loaded_shard_id: "q", "k", æˆ– "v"
        """
        param_data = param.data
        assert loaded_shard_id in ["q", "k", "v"]
        
        # è®¡ç®—è¯¥éƒ¨åˆ†åœ¨åˆå¹¶æƒé‡ä¸­çš„ä½ç½®
        if loaded_shard_id == "q":
            shard_size = self.num_heads * self.head_size
            shard_offset = 0
        elif loaded_shard_id == "k":
            shard_size = self.num_kv_heads * self.head_size
            shard_offset = self.num_heads * self.head_size
        else:  # "v"
            shard_size = self.num_kv_heads * self.head_size
            shard_offset = self.num_heads * self.head_size + self.num_kv_heads * self.head_size
        
        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)
        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]
        param_data.copy_(loaded_weight)


class RowParallelLinear(LinearBase):
    """
    è¡Œå¹¶è¡Œçº¿æ€§å±‚ - æŒ‰è¾“å…¥ç»´åº¦åˆ‡åˆ†
    
    åˆ‡åˆ†æ–¹å¼ï¼šinput_size â†’ input_size / tp_size
    
    æ•°å­¦åŸç†ï¼š
    Y = X @ W^T
    å°† W æŒ‰è¡Œåˆ‡åˆ†ï¼ŒX ä¹Ÿå¯¹åº”åˆ‡åˆ†
    
    æ¯ä¸ªGPUè®¡ç®—ï¼šYi = Xi @ Wi^T
    ç»“æœé€šè¿‡ all-reduce æ±‚å’Œ
    
    å›¾ç¤ºï¼š
    GPU 0: X0 = X[:, 0:input//2]       GPU 1: X1 = X[:, input//2:]
           W0 = W[:, 0:input//2]            W1 = W[:, input//2:]
           â†“                                   â†“
    Y0 = X0 @ W0^T                     Y1 = X1 @ W1^T
           â†“                                   â†“
    Y = Y0 + Y1  (all-reduceæ±‚å’Œ)
    
    æ³¨æ„ï¼šè¡Œå¹¶è¡Œéœ€è¦ all-reduceï¼Œåˆ—å¹¶è¡Œä¸éœ€è¦
    """
    
    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        tp_size = dist.get_world_size()
        # è®¡ç®—åˆ‡åˆ†åçš„è¾“å…¥ç»´åº¦
        super().__init__(divide(input_size, tp_size), output_size, bias, 1)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        """åŠ è½½è¡Œåˆ‡åˆ†çš„æƒé‡"""
        param_data = param.data
        shard_size = param_data.size(self.tp_dim)
        start_idx = self.tp_rank * shard_size
        loaded_weight = loaded_weight.narrow(self.tp_dim, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¡Œå¹¶è¡Œçº¿æ€§å˜æ¢
        
        æ³¨æ„ï¼šåªæœ‰rank 0çš„åç½®æœ‰æ•ˆï¼Œé¿å…é‡å¤åŠ 
        """
        # çº¿æ€§å˜æ¢
        y = F.linear(x, self.weight, self.bias if self.tp_rank == 0 else None)
        
        # å¤šGPUæ—¶ï¼Œé€šè¿‡all-reduceåˆå¹¶ç»“æœ
        if self.tp_size > 1:
            dist.all_reduce(y)
        
        return y
```

**å¼ é‡å¹¶è¡Œå¯¹æ¯”å›¾ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å¼ é‡å¹¶è¡Œç­–ç•¥å¯¹æ¯”                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Column Parallel (åˆ—å¹¶è¡Œ)                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  è¾“å…¥ X         â”‚                                                    â”‚
â”‚  â”‚  [batch, in]    â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚           â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚     â–¼           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚GPU 0  â”‚  â”‚GPU 1  â”‚                                                   â”‚
â”‚  â”‚W0     â”‚  â”‚W1     â”‚  W = [W0; W1] æŒ‰è¡Œæ‹¼æ¥                            â”‚
â”‚  â”‚[in,outâ”‚  â”‚[in,outâ”‚                                                   â”‚
â”‚  â”‚  /2]  â”‚  â”‚  /2]  â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜                                                   â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â–¼          â–¼                                                       â”‚
â”‚  Y0 = X@W0   Y1 = X@W1                                                  â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â–¼                                                             â”‚
â”‚       Y = [Y0, Y1]  åœ¨è¾“å‡ºç»´åº¦æ‹¼æ¥                                       â”‚
â”‚                                                                         â”‚
â”‚  ç‰¹ç‚¹ï¼šæ— éœ€é€šä¿¡ï¼Œè¾“å‡ºç»´åº¦ç¿»å€                                            â”‚
â”‚                                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Row Parallel (è¡Œå¹¶è¡Œ)                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  è¾“å…¥ X         â”‚                                                    â”‚
â”‚  â”‚  [batch, in]    â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚           â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚     â–¼           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚GPU 0  â”‚  â”‚GPU 1  â”‚                                                   â”‚
â”‚  â”‚X0     â”‚  â”‚X1     â”‚  X = [X0, X1] æŒ‰åˆ—æ‹¼æ¥                            â”‚
â”‚  â”‚W0     â”‚  â”‚W1     â”‚  W = [W0, W1] æŒ‰åˆ—æ‹¼æ¥                            â”‚
â”‚  â”‚[in/2, â”‚  â”‚[in/2, â”‚                                                   â”‚
â”‚  â”‚ out]  â”‚  â”‚ out]  â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜                                                   â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â–¼          â–¼                                                       â”‚
â”‚  Y0 = X0@W0  Y1 = X1@W1                                                 â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â–¼                                                             â”‚
â”‚       Y = Y0 + Y1  (all-reduceæ±‚å’Œ)                                      â”‚
â”‚                                                                         â”‚
â”‚  ç‰¹ç‚¹ï¼šéœ€è¦all-reduceï¼Œè¾“å‡ºç»´åº¦ä¸å˜                                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8. layers/layernorm.py - RMSNorm å±‚

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—


class RMSNorm(nn.Module):
    """
    RMSNorm (Root Mean Square Layer Normalization)
    
    ç›¸æ¯”ä¼ ç»Ÿ LayerNormï¼ŒRMSNorm å»æ‰äº†å‡å€¼è®¡ç®—ï¼Œåªä½¿ç”¨å‡æ–¹æ ¹ï¼š
    - LayerNorm: (x - mean) / sqrt(var + eps)
    - RMSNorm: x / sqrt(mean(x^2) + eps)
    
    ä¼˜åŠ¿ï¼š
    1. è®¡ç®—æ›´ç®€å•ï¼ˆå°‘ä¸€æ¬¡å‡å€¼è®¡ç®—ï¼‰
    2. åœ¨LLMä¸­æ•ˆæœç›¸å½“ç”šè‡³æ›´ä¼˜
    3. ä¸æ®‹å·®è¿æ¥é…åˆæ›´å¥½
    
    å…¬å¼ï¼šRMSNorm(x) = x / RMS(x) * weight
          å…¶ä¸­ RMS(x) = sqrt(mean(x^2))
    
    ç±»æ¯”ï¼šå°±åƒå¯¹å‘é‡åš"é•¿åº¦å½’ä¸€åŒ–"ï¼Œä¿æŒæ–¹å‘ä¸å˜
    """
    
    def __init__(
        self,
        hidden_size: int,               # éšè—å±‚å¤§å°
        eps: float = 1e-6,              # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
    ) -> None:
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º1
        self.weight = nn.Parameter(torch.ones(hidden_size))

    @torch.compile                      # PyTorch 2.0 ç¼–è¯‘ä¼˜åŒ–
    def rms_forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        """
        æ ‡å‡†RMSNormå‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ [..., hidden_size]
        
        Returns:
            å½’ä¸€åŒ–åçš„å¼ é‡
        """
        # ä¿å­˜åŸå§‹æ•°æ®ç±»å‹ï¼ˆå¯èƒ½æ˜¯fp16/bf16ï¼‰
        orig_dtype = x.dtype
        
        # è½¬ä¸ºfloat32è¿›è¡Œè®¡ç®—ï¼ˆæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
        x = x.float()
        
        # è®¡ç®—å‡æ–¹å€¼ï¼šmean(x^2, dim=-1, keepdim=True)
        var = x.pow(2).mean(dim=-1, keepdim=True)
        
        # å½’ä¸€åŒ–ï¼šx / sqrt(var + eps)
        # rsqrt = 1 / sqrtï¼Œæ•ˆç‡æ›´é«˜
        x.mul_(torch.rsqrt(var + self.eps))
        
        # è½¬å›åŸå§‹ç±»å‹ï¼Œå¹¶åº”ç”¨å¯å­¦ä¹ æƒé‡
        x = x.to(orig_dtype).mul_(self.weight)
        
        return x

    @torch.compile
    def add_rms_forward(
        self,
        x: torch.Tensor,
        residual: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        èåˆæ®‹å·®è¿æ¥çš„RMSNorm
        
        å°† x + residual å’Œ RMSNorm èåˆä¸ºä¸€æ­¥ï¼Œå‡å°‘å†…å­˜è®¿é—®
        
        æ ‡å‡†æµç¨‹ï¼š
        hidden = x + residual
        residual = hidden
        hidden = RMSNorm(hidden)
        
        èåˆåï¼š
        hidden = RMSNorm(x + residual), residual = x + residual
        
        Args:
            x: è¾“å…¥å¼ é‡
            residual: æ®‹å·®è¿æ¥
        
        Returns:
            (å½’ä¸€åŒ–ç»“æœ, æ›´æ–°åçš„æ®‹å·®)
        """
        orig_dtype = x.dtype
        
        # èåˆåŠ æ³•ï¼šx + residual
        x = x.float().add_(residual.float())
        
        # ä¿å­˜æ®‹å·®ï¼ˆç”¨äºä¸‹ä¸€å±‚ï¼‰
        residual = x.to(orig_dtype)
        
        # RMSNormè®¡ç®—
        var = x.pow(2).mean(dim=-1, keepdim=True)
        x.mul_(torch.rsqrt(var + self.eps))
        x = x.to(orig_dtype).mul_(self.weight)
        
        return x, residual

    def forward(
        self,
        x: torch.Tensor,
        residual: torch.Tensor | None = None,
    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
        """
        ç»Ÿä¸€å‰å‘æ¥å£
        
        Args:
            x: è¾“å…¥å¼ é‡
            residual: å¯é€‰çš„æ®‹å·®è¿æ¥
        
        Returns:
            æ— residual: å½’ä¸€åŒ–ç»“æœ
            æœ‰residual: (å½’ä¸€åŒ–ç»“æœ, æ›´æ–°åçš„æ®‹å·®)
        """
        if residual is None:
            return self.rms_forward(x)
        else:
            return self.add_rms_forward(x, residual)
```

**RMSNorm vs LayerNormï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  LayerNorm                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  x = [1, 2, 3]  â”‚                                                    â”‚
â”‚  â”‚  mean = 2       â”‚  â† è®¡ç®—å‡å€¼                                        â”‚
â”‚  â”‚  var = 2/3      â”‚  â† è®¡ç®—æ–¹å·®                                        â”‚
â”‚  â”‚  x_norm =       â”‚                                                    â”‚
â”‚  â”‚  (x-mean)/sqrt  â”‚                                                    â”‚
â”‚  â”‚  (var+eps)      â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚                                                                         â”‚
â”‚  RMSNorm (LLMå¸¸ç”¨)                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  x = [1, 2, 3]  â”‚                                                    â”‚
â”‚  â”‚  rms = sqrt(    â”‚  â† åªè®¡ç®—å‡æ–¹æ ¹                                    â”‚
â”‚  â”‚    mean(xÂ²))    â”‚                                                    â”‚
â”‚  â”‚  = sqrt(14/3)   â”‚                                                    â”‚
â”‚  â”‚  x_norm = x/rms â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚                                                                         â”‚
â”‚  åŒºåˆ«ï¼šRMSNormå»æ‰äº†å‡å‡å€¼æ“ä½œï¼Œè®¡ç®—æ›´å¿«                                  â”‚
â”‚  åŸç†ï¼šåœ¨é¢„è®­ç»ƒTransformerä¸­ï¼Œå‡å€¼ä¸º0çš„å‡è®¾å¾€å¾€æˆç«‹                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 9. layers/activation.py - æ¿€æ´»å‡½æ•°

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F       # ç¥ç»ç½‘ç»œå‡½æ•°


class SiluAndMul(nn.Module):
    """
    SwiGLU æ¿€æ´»å‡½æ•° - SiLU + é€å…ƒç´ ä¹˜æ³•
    
    è¿™æ˜¯ LLaMAã€Qwen ç­‰ç°ä»£ LLM ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°
    
    å…¬å¼ï¼šSwiGLU(x) = SiLU(x1) * x2
          å…¶ä¸­ x = [x1, x2]ï¼ˆåœ¨æœ€åä¸€ç»´åˆ‡åˆ†ï¼‰
          SiLU(x) = x * sigmoid(x)
    
    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
    1. SiLUï¼ˆSwishï¼‰æ˜¯å¹³æ»‘çš„éçº¿æ€§æ¿€æ´»
    2. é—¨æ§æœºåˆ¶ï¼ˆä¸x2ç›¸ä¹˜ï¼‰æ§åˆ¶ä¿¡æ¯æµ
    3. åœ¨Transformerä¸­è¡¨ç°ä¼˜äºReLU/GELU
    
    ç±»æ¯”ï¼šå°±åƒä¸€ä¸ª"æ™ºèƒ½é˜€é—¨"ï¼Œæ ¹æ®è¾“å…¥å†³å®šé€šè¿‡å¤šå°‘ä¿¡æ¯
    """
    
    def __init__(self):
        super().__init__()

    @torch.compile                      # ç¼–è¯‘ä¼˜åŒ–
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        SwiGLUå‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œæœ€åä¸€ç»´å¤§å°ä¸º2*nï¼ˆä¼šè¢«åˆ‡åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼‰
        
        Returns:
            SiLU(å‰åŠéƒ¨åˆ†) * ååŠéƒ¨åˆ†
        """
        # åœ¨æœ€åä¸€ç»´å°†xåˆ‡åˆ†ä¸ºä¸¤åŠ
        # ä¾‹å¦‚ï¼šx.shape = [batch, 2*inter_size]
        # x1.shape = x2.shape = [batch, inter_size]
        x1, x2 = x.chunk(2, -1)
        
        # SiLU(x1) * x2
        # SiLU(x) = x * sigmoid(x)ï¼Œæ˜¯å¹³æ»‘çš„é—¨æ§å‡½æ•°
        return F.silu(x1) * x2
```

**æ¿€æ´»å‡½æ•°å¯¹æ¯”ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ¿€æ´»å‡½æ•°å¯¹æ¯”                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ReLU:    f(x) = max(0, x)                                              â”‚
â”‚           ç®€å•ä½†ä¸å¤Ÿå¹³æ»‘ï¼Œè´Ÿæ•°å®Œå…¨æˆªæ–­                                    â”‚
â”‚                                                                         â”‚
â”‚  GELU:    f(x) = x * Î¦(x)  (Î¦æ˜¯æ ‡å‡†æ­£æ€CDF)                             â”‚
â”‚           å¹³æ»‘ï¼Œè®¡ç®—è¾ƒå¤æ‚                                                â”‚
â”‚                                                                         â”‚
â”‚  SiLU/Swish: f(x) = x * sigmoid(x)                                      â”‚
â”‚           å¹³æ»‘ï¼Œè‡ªé—¨æ§ï¼ŒLLMå¸¸ç”¨                                           â”‚
â”‚           å½¢çŠ¶ï¼šå…ˆä¸‹é™åä¸Šå‡ï¼Œç±»ä¼¼Swish                                   â”‚
â”‚                                                                         â”‚
â”‚  SwiGLU:  f(x, y) = SiLU(x) * y                                         â”‚
â”‚           åŒè¾“å…¥é—¨æ§ï¼Œç°ä»£LLMæ ‡é…                                         â”‚
â”‚           gate_projå’Œup_projåˆå¹¶åä½¿ç”¨                                    â”‚
â”‚                                                                         â”‚
â”‚  å›¾ç¤ºï¼š                                                                 â”‚
â”‚  ReLU     â–ˆâ–ˆâ–ˆâ–ˆ          GELU      â–„â–„â–„â–„                                  â”‚
â”‚          â–ˆ             (å¹³æ»‘ç‰ˆReLU)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚                                                                         â”‚
â”‚  SiLU      â•­â”€â•®           SwiGLUéœ€è¦ä¸¤ä¸ªè¾“å…¥                             â”‚
â”‚           â•±   â•²                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€     x â”€â”€â–º SiLU â”€â”€â”                                   â”‚
â”‚          (å¹³æ»‘ä¸‹å‡¹)                  * â”€â”€â–º è¾“å‡º                          â”‚
â”‚                              y â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 10. layers/rotary_embedding.py - æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)

```python
from functools import lru_cache      # ç¼“å­˜è£…é¥°å™¨
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—


def apply_rotary_emb(
    x: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
) -> torch.Tensor:
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°è¾“å…¥å¼ é‡
    
    RoPE æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æ—‹è½¬çŸ©é˜µå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°Query/Keyä¸­
    
    æ•°å­¦åŸç†ï¼š
    å¯¹äºäºŒç»´å‘é‡ [x1, x2]ï¼Œæ—‹è½¬Î¸è§’åº¦ï¼š
    [x1']   [cosÎ¸  -sinÎ¸] [x1]
    [x2'] = [sinÎ¸   cosÎ¸] [x2]
    
    å³ï¼šx1' = x1*cosÎ¸ - x2*sinÎ¸
        x2' = x2*cosÎ¸ + x1*sinÎ¸
    
    æ‰©å±•åˆ°é«˜ç»´ï¼šå°†ç‰¹å¾ç»´åº¦ä¸¤ä¸¤é…å¯¹ï¼Œæ¯å¯¹åº”ç”¨ä¸åŒé¢‘ç‡çš„æ—‹è½¬
    
    Args:
        x: è¾“å…¥å¼ é‡ [num_tokens, num_heads, head_dim]
        cos: ä½™å¼¦å€¼ [num_tokens, 1, head_dim//2]
        sin: æ­£å¼¦å€¼ [num_tokens, 1, head_dim//2]
    
    Returns:
        åº”ç”¨æ—‹è½¬ç¼–ç åçš„å¼ é‡
    """
    # å°†æœ€åä¸€ç»´åˆ‡åˆ†ä¸ºä¸¤åŠ
    # x1, x2 å½¢çŠ¶: [num_tokens, num_heads, head_dim//2]
    x1, x2 = torch.chunk(x.float(), 2, dim=-1)
    
    # åº”ç”¨æ—‹è½¬çŸ©é˜µ
    # y1 = x1 * cos - x2 * sin
    # y2 = x2 * cos + x1 * sin
    y1 = x1 * cos - x2 * sin
    y2 = x2 * cos + x1 * sin
    
    # æ‹¼æ¥å›åŸå§‹ç»´åº¦
    return torch.cat((y1, y2), dim=-1).to(x.dtype)


class RotaryEmbedding(nn.Module):
    """
    æ—‹è½¬ä½ç½®ç¼–ç æ¨¡å—
    
    RoPE çš„ä¼˜åŠ¿ï¼š
    1. ç›¸å¯¹ä½ç½®ç¼–ç ï¼šå†…ç§¯åªä¸ç›¸å¯¹ä½ç½®æœ‰å…³
    2. é•¿åºåˆ—å¤–æ¨ï¼šå¯ä»¥å¤„ç†æ¯”è®­ç»ƒæ›´é•¿çš„åºåˆ—
    3. ä¸æ³¨æ„åŠ›å¤©ç„¶ç»“åˆï¼šç›´æ¥ä½œç”¨äºQã€K
    
    é¢‘ç‡è®¡ç®—å…¬å¼ï¼š
    Î¸_i = base^(-2i/d)  å…¶ä¸­ i âˆˆ [0, d/2), d = head_dim
    
    ä½ç½® m çš„æ—‹è½¬è§’åº¦ï¼šm * Î¸_i
    """
    
    def __init__(
        self,
        head_size: int,                 # æ¯ä¸ªå¤´çš„å¤§å°
        rotary_dim: int,                # åº”ç”¨æ—‹è½¬ç¼–ç çš„ç»´åº¦
        max_position_embeddings: int,   # æœ€å¤§ä½ç½®æ•°
        base: float,                    # é¢‘ç‡åŸºæ•°ï¼ˆé€šå¸¸æ˜¯10000æˆ–1000000ï¼‰
    ) -> None:
        super().__init__()
        self.head_size = head_size
        
        # å½“å‰å®ç°è¦æ±‚ rotary_dim == head_size
        # éƒ¨åˆ†å®ç°æ”¯æŒåªæ—‹è½¬éƒ¨åˆ†ç»´åº¦
        assert rotary_dim == head_size
        
        # è®¡ç®—é¢‘ç‡çš„å€’æ•°ï¼ˆé€†é¢‘ç‡ï¼‰
        # å…¬å¼ï¼š1 / (base^(i/rotary_dim))ï¼Œå…¶ä¸­ i = 0, 2, 4, ..., rotary_dim-2
        # å½¢çŠ¶: [rotary_dim//2]
        inv_freq = 1.0 / (base ** (torch.arange(0, rotary_dim, 2, dtype=torch.float) / rotary_dim))
        
        # æ‰€æœ‰å¯èƒ½çš„ä½ç½®ç´¢å¼• [0, 1, 2, ..., max_position-1]
        t = torch.arange(max_position_embeddings, dtype=torch.float)
        
        # è®¡ç®—æ¯ä¸ªä½ç½®ã€æ¯ä¸ªç»´åº¦çš„é¢‘ç‡
        # freqs[m, i] = m * inv_freq[i]
        # å½¢çŠ¶: [max_position, rotary_dim//2]
        freqs = torch.einsum("i,j -> ij", t, inv_freq)
        
        # è®¡ç®—ä½™å¼¦å’Œæ­£å¼¦å€¼
        cos = freqs.cos()
        sin = freqs.sin()
        
        # æ‹¼æ¥coså’Œsinï¼Œå¹¶å¢åŠ ç»´åº¦ç”¨äºå¹¿æ’­
        # å½¢çŠ¶: [max_position, 1, rotary_dim]
        cache = torch.cat((cos, sin), dim=-1).unsqueeze_(1)
        
        # æ³¨å†Œä¸ºbufferï¼ˆä¸æ˜¯å‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰
        self.register_buffer("cos_sin_cache", cache, persistent=False)

    @torch.compile
    def forward(
        self,
        positions: torch.Tensor,        # ä½ç½®ç´¢å¼• [num_tokens]
        query: torch.Tensor,            # Query [num_tokens, num_heads, head_dim]
        key: torch.Tensor,              # Key [num_tokens, num_kv_heads, head_dim]
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°Qå’ŒK
        
        Args:
            positions: æ¯ä¸ªtokençš„ä½ç½®ç´¢å¼•
            query: Queryå¼ é‡
            key: Keyå¼ é‡
        
        Returns:
            (æ—‹è½¬åçš„Query, æ—‹è½¬åçš„Key)
        """
        # æ ¹æ®ä½ç½®ç´¢å¼•è·å–å¯¹åº”çš„cos/sinå€¼
        # cos_sinå½¢çŠ¶: [num_tokens, 1, head_dim]
        cos_sin = self.cos_sin_cache[positions]
        
        # åˆ‡åˆ†ä¸ºcoså’Œsin
        # å½¢çŠ¶: [num_tokens, 1, head_dim//2]
        cos, sin = cos_sin.chunk(2, dim=-1)
        
        # åˆ†åˆ«åº”ç”¨åˆ°Qå’ŒK
        query = apply_rotary_emb(query, cos, sin)
        key = apply_rotary_emb(key, cos, sin)
        
        return query, key


@lru_cache(1)
def get_rope(
    head_size: int,
    rotary_dim: int,
    max_position: int,
    base: float,
    rope_scaling: dict | None = None,
):
    """
    è·å–RoPEå®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    ä½¿ç”¨lru_cacheç¡®ä¿ç›¸åŒå‚æ•°çš„RoPEåªåˆ›å»ºä¸€æ¬¡
    èŠ‚çœå†…å­˜ï¼Œæé«˜æ€§èƒ½
    
    Args:
        head_size: å¤´å¤§å°
        rotary_dim: æ—‹è½¬ç»´åº¦
        max_position: æœ€å¤§ä½ç½®
        base: é¢‘ç‡åŸºæ•°
        rope_scaling: ä½ç½®æ’å€¼é…ç½®ï¼ˆå½“å‰ä¸æ”¯æŒï¼‰
    
    Returns:
        RotaryEmbeddingå®ä¾‹
    """
    assert rope_scaling is None  # å½“å‰ä¸æ”¯æŒä½ç½®æ’å€¼
    rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base)
    return rotary_emb
```

**RoPE åŸç†å›¾è§£ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) åŸç†                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æ—‹è½¬å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å‘é‡ä¸­                                  â”‚
â”‚                                                                         â”‚
â”‚  äºŒç»´æ—‹è½¬ç¤ºä¾‹ï¼š                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚                                         â”‚                           â”‚
â”‚  â”‚      â•±â”‚â•²  æ—‹è½¬Î¸å   â•±â”‚â•²                 â”‚                           â”‚
â”‚  â”‚     â•± â”‚ â•²  â”€â”€â”€â”€â”€â”€â–º â•± â”‚ â•²                â”‚                           â”‚
â”‚  â”‚    â•±  â”‚  â•²        â•±  â”‚  â•²               â”‚                           â”‚
â”‚  â”‚   â•±   â”‚   â•²      â•±   â”‚   â•²              â”‚                           â”‚
â”‚  â”‚  â•±â”€â”€â”€â”€â”¼â”€â”€â”€â”€â•²    â•±â”€â”€â”€â”€â”¼â”€â”€â”€â”€â•²             â”‚                           â”‚
â”‚  â”‚       x            x'                   â”‚                           â”‚
â”‚  â”‚                                         â”‚                           â”‚
â”‚  â”‚  [x1, x2] â”€â”€æ—‹è½¬Î¸â”€â”€â–º [x1*cosÎ¸-x2*sinÎ¸,  â”‚                           â”‚
â”‚  â”‚                       x2*cosÎ¸+x1*sinÎ¸]  â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                         â”‚
â”‚  é«˜ç»´æ‰©å±•ï¼šå°†head_dimç»´åˆ†æˆhead_dim/2å¯¹ï¼Œæ¯å¯¹ç‹¬ç«‹æ—‹è½¬                      â”‚
â”‚  æ—‹è½¬é¢‘ç‡ï¼šÎ¸_i = position * base^(-2i/head_dim)                          â”‚
â”‚                                                                         â”‚
â”‚  ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ                                                            â”‚
â”‚  1. ç›¸å¯¹ä½ç½®ï¼šdot(q_m, k_n) åªä¸ (m-n) æœ‰å…³                               â”‚
â”‚  2. é•¿åºåˆ—å¤–æ¨ï¼šå¯ä»¥å¤„ç†è¶…è¿‡è®­ç»ƒé•¿åº¦çš„åºåˆ—                                â”‚
â”‚  3. ä¸æ³¨æ„åŠ›å¤©ç„¶ç»“åˆï¼šç›´æ¥ä¿®æ”¹Qã€K                                        â”‚
â”‚                                                                         â”‚
â”‚  é¢‘ç‡å¯è§†åŒ–ï¼š                                                            â”‚
â”‚  dim 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ é«˜é¢‘ï¼ˆæ—‹è½¬å¿«ï¼‰                               â”‚
â”‚  dim 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                              â”‚
â”‚  dim 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    â”‚
â”‚  ...                                                                â”‚
â”‚  dim d: â–ˆâ–ˆ ä½é¢‘ï¼ˆæ—‹è½¬æ…¢ï¼‰                                                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---


### 11. layers/attention.py - æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå«Triton Kernelï¼‰

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
import triton                         # Triton GPUç¼–ç¨‹è¯­è¨€
import triton.language as tl          # Tritonè¯­è¨€æ¥å£

# FlashAttentioné«˜æ•ˆå®ç°
from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
from nanovllm.utils.context import get_context  # å…¨å±€ä¸Šä¸‹æ–‡


# ==================== Triton Kernel ====================
@triton.jit
def store_kvcache_kernel(
    key_ptr,           # Keyå¼ é‡æŒ‡é’ˆ
    key_stride,        # Keyçš„æ­¥é•¿
    value_ptr,         # Valueå¼ é‡æŒ‡é’ˆ
    value_stride,      # Valueçš„æ­¥é•¿
    k_cache_ptr,       # K CacheæŒ‡é’ˆ
    v_cache_ptr,       # V CacheæŒ‡é’ˆ
    slot_mapping_ptr,  # æ§½ä½æ˜ å°„æŒ‡é’ˆ
    D: tl.constexpr,   # å¤´ç»´åº¦ï¼ˆç¼–è¯‘æ—¶å¸¸æ•°ï¼‰
):
    """
    Triton Kernelï¼šå°†è®¡ç®—çš„KVå€¼å­˜å‚¨åˆ°Paged KV Cache
    
    ä¸ºä»€ä¹ˆç”¨Tritonï¼Ÿ
    1. æ¯”PyTorchæ›´é«˜æ•ˆï¼ˆé¿å…ä¸­é—´å¼ é‡ï¼‰
    2. ç›´æ¥æ§åˆ¶å†…å­˜è®¿é—®æ¨¡å¼
    3. èåˆå¤šä¸ªæ“ä½œ
    
    æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªtokençš„ä¸€ä¸ªå¤´
    """
    # è·å–å½“å‰çº¿ç¨‹çš„ç´¢å¼•ï¼ˆå¯¹åº”ä¸€ä¸ªtokenï¼‰
    idx = tl.program_id(0)
    
    # åŠ è½½è¯¥tokenåœ¨KV Cacheä¸­çš„å­˜å‚¨ä½ç½®
    slot = tl.load(slot_mapping_ptr + idx)
    
    # slot = -1 è¡¨ç¤ºä¸éœ€è¦å­˜å‚¨ï¼ˆå·²ç¼“å­˜ï¼‰
    if slot == -1:
        return
    
    # è®¡ç®—Key/Valueçš„å†…å­˜åç§»
    # key_offsets: è¯¥tokençš„æ‰€æœ‰å¤´çš„æ‰€æœ‰ç»´åº¦
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    
    # ä»è¾“å…¥åŠ è½½Keyå’ŒValue
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)
    
    # è®¡ç®—KV Cacheçš„å­˜å‚¨ä½ç½®
    cache_offsets = slot * D + tl.arange(0, D)
    
    # å­˜å‚¨åˆ°KV Cache
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)


def store_kvcache(
    key: torch.Tensor, 
    value: torch.Tensor, 
    k_cache: torch.Tensor, 
    v_cache: torch.Tensor, 
    slot_mapping: torch.Tensor
):
    """
    å°†KVå€¼å­˜å‚¨åˆ°Paged KV Cacheï¼ˆPythonæ¥å£ï¼‰
    
    Args:
        key: è®¡ç®—çš„Key [num_tokens, num_heads, head_dim]
        value: è®¡ç®—çš„Value [num_tokens, num_kv_heads, head_dim]
        k_cache: K Cache [num_blocks, block_size, num_kv_heads, head_dim]
        v_cache: V Cache [num_blocks, block_size, num_kv_heads, head_dim]
        slot_mapping: æ¯ä¸ªtokençš„å­˜å‚¨ä½ç½® [num_tokens]
    """
    # è·å–ç»´åº¦ä¿¡æ¯
    N, num_heads, head_dim = key.shape
    D = num_heads * head_dim  # æ¯ä¸ªtokençš„æ€»ç»´åº¦
    
    # éªŒè¯å†…å­˜å¸ƒå±€ï¼ˆç¡®ä¿æ˜¯è¿ç»­çš„ï¼‰
    assert key.stride(-1) == 1 and value.stride(-1) == 1
    assert key.stride(1) == head_dim and value.stride(1) == head_dim
    assert k_cache.stride(1) == D and v_cache.stride(1) == D
    assert slot_mapping.numel() == N
    
    # å¯åŠ¨Triton Kernel
    # grid=(N,): Nä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªå¤„ç†ä¸€ä¸ªtoken
    store_kvcache_kernel[(N,)](
        key, key.stride(0),
        value, value.stride(0),
        k_cache, v_cache,
        slot_mapping,
        D
    )


# ==================== Attention æ¨¡å— ====================
class Attention(nn.Module):
    """
    æ³¨æ„åŠ›æ¨¡å— - æ”¯æŒPrefillå’ŒDecodeé˜¶æ®µ
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ä½¿ç”¨Triton Kernelå°†KVå†™å…¥Paged Cache
    2. Prefillé˜¶æ®µï¼šä½¿ç”¨FlashAttentionè¿›è¡Œå®Œæ•´æ³¨æ„åŠ›è®¡ç®—
    3. Decodeé˜¶æ®µï¼šä½¿ç”¨FlashAttentionçš„KV Cacheä¼˜åŒ–ç‰ˆæœ¬
    
    è®¾è®¡è¦ç‚¹ï¼š
    - é€šè¿‡Contextåˆ¤æ–­å½“å‰é˜¶æ®µ
    - æ”¯æŒå‰ç¼€ç¼“å­˜ï¼ˆblock_tablesä¸ä¸ºNoneæ—¶ï¼‰
    """
    
    def __init__(
        self,
        num_heads,         # å¤´æ•°ï¼ˆå½“å‰GPUï¼‰
        head_dim,          # æ¯ä¸ªå¤´çš„å¤§å°
        scale,             # ç¼©æ”¾å› å­ï¼ˆ1/sqrt(head_dim)ï¼‰
        num_kv_heads,      # K,Vå¤´æ•°ï¼ˆå¯èƒ½å°‘äºQï¼‰
    ):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = scale
        self.num_kv_heads = num_kv_heads
        
        # åˆå§‹åŒ–æ—¶ç©ºCacheï¼ˆä¼šåœ¨allocate_kv_cacheæ—¶è®¾ç½®ï¼‰
        self.k_cache = self.v_cache = torch.tensor([])

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
        """
        æ³¨æ„åŠ›å‰å‘ä¼ æ’­
        
        Args:
            q: Query [num_tokens, num_heads, head_dim]
            k: Key [num_tokens, num_kv_heads, head_dim]
            v: Value [num_tokens, num_kv_heads, head_dim]
        
        Returns:
            æ³¨æ„åŠ›è¾“å‡º [num_tokens, num_heads, head_dim]
        """
        # è·å–å½“å‰ä¸Šä¸‹æ–‡
        context = get_context()
        k_cache, v_cache = self.k_cache, self.v_cache
        
        # ==================== å­˜å‚¨KVåˆ°Cache ====================
        # å¦‚æœCacheå·²åˆ†é…ï¼Œå°†æ–°è®¡ç®—çš„KVå†™å…¥
        if k_cache.numel() and v_cache.numel():
            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)
        
        # ==================== Prefill é˜¶æ®µ ====================
        if context.is_prefill:
            # æ£€æŸ¥æ˜¯å¦æœ‰å‰ç¼€ç¼“å­˜
            if context.block_tables is not None:
                # å‰ç¼€ç¼“å­˜å‘½ä¸­ï¼šä½¿ç”¨å®Œæ•´çš„KV Cache
                k, v = k_cache, v_cache
            
            # ä½¿ç”¨FlashAttentionè¿›è¡Œé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
            # flash_attn_varlen_func æ”¯æŒå˜é•¿åºåˆ—æ‰¹å¤„ç†
            o = flash_attn_varlen_func(
                q, k, v,
                max_seqlen_q=context.max_seqlen_q,      # æœ€å¤§Queryé•¿åº¦
                cu_seqlens_q=context.cu_seqlens_q,      # Queryç´¯ç§¯é•¿åº¦
                max_seqlen_k=context.max_seqlen_k,      # æœ€å¤§Keyé•¿åº¦
                cu_seqlens_k=context.cu_seqlens_k,      # Keyç´¯ç§¯é•¿åº¦
                softmax_scale=self.scale,               # ç¼©æ”¾å› å­
                causal=True,                            # å› æœæ©ç ï¼ˆåªçœ‹å‰é¢ï¼‰
                block_table=context.block_tables        # å—è¡¨ï¼ˆå‰ç¼€ç¼“å­˜ï¼‰
            )
        
        # ==================== Decode é˜¶æ®µ ====================
        else:
            # Decodeé˜¶æ®µï¼šæ¯ä¸ªtokenåªè®¡ç®—ä¸€ä¸ªquery
            # ä½¿ç”¨flash_attn_with_kvcacheä¼˜åŒ–
            # qéœ€è¦å¢åŠ åºåˆ—ç»´åº¦ï¼š[batch, 1, num_heads, head_dim]
            o = flash_attn_with_kvcache(
                q.unsqueeze(1),                         # Query
                k_cache,                                # å®Œæ•´çš„K Cache
                v_cache,                                # å®Œæ•´çš„V Cache
                cache_seqlens=context.context_lens,     # æ¯ä¸ªåºåˆ—çš„å½“å‰é•¿åº¦
                block_table=context.block_tables,       # å—è¡¨
                softmax_scale=self.scale,
                causal=True
            )
        
        return o
```

**æ³¨æ„åŠ›è®¡ç®—æµç¨‹å›¾ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ³¨æ„åŠ›è®¡ç®—æµç¨‹                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        Prefill é˜¶æ®µ                              â”‚   â”‚
â”‚  â”‚                                                                 â”‚   â”‚
â”‚  â”‚  Input: prompt tokens [t1, t2, t3, t4]                          â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  QKV Projection â†’ Q, K, V                                       â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  store_kvcache(K, V) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º KV Cache                  â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  flash_attn_varlen_func(Q, K, V)                                â”‚   â”‚
â”‚  â”‚  - ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰tokençš„æ³¨æ„åŠ›                                   â”‚   â”‚
â”‚  â”‚  - ä½¿ç”¨cu_seqlenså¤„ç†å˜é•¿åºåˆ—                                    â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  Output: æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›ç»“æœ                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        Decode é˜¶æ®µ                               â”‚   â”‚
â”‚  â”‚                                                                 â”‚   â”‚
â”‚  â”‚  Input: last token [t_new]                                      â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  QKV Projection â†’ Q_new, K_new, V_new                           â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  store_kvcache(K_new, V_new) â”€â”€â”€â”€â”€â”€â–º KV Cache                   â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  flash_attn_with_kvcache(Q_new, KV_Cache)                       â”‚   â”‚
â”‚  â”‚  - åªè®¡ç®—æ–°tokençš„æ³¨æ„åŠ›                                        â”‚   â”‚
â”‚  â”‚  - å¤ç”¨ä¹‹å‰å­˜å‚¨çš„KV Cache                                       â”‚   â”‚
â”‚  â”‚                      â†“                                          â”‚   â”‚
â”‚  â”‚  Output: æ–°tokençš„æ³¨æ„åŠ›ç»“æœ                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  å…³é”®åŒºåˆ«ï¼š                                                              â”‚
â”‚  - Prefill: è®¡ç®—æ‰€æœ‰tokenï¼Œå¹¶è¡Œåº¦é«˜ï¼Œè®¡ç®—å¯†é›†                            â”‚
â”‚  - Decode: åªè®¡ç®—1ä¸ªtokenï¼Œå†…å­˜å¸¦å®½å¯†é›†                                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 12. layers/embed_head.py - è¯åµŒå…¥å’Œè¾“å‡ºå¤´

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F       # ç¥ç»ç½‘ç»œå‡½æ•°
import torch.distributed as dist      # åˆ†å¸ƒå¼

from nanovllm.utils.context import get_context  # å…¨å±€ä¸Šä¸‹æ–‡


class VocabParallelEmbedding(nn.Module):
    """
    è¯è¡¨å¹¶è¡ŒåµŒå…¥å±‚ - æŒ‰è¯è¡¨ç»´åº¦åˆ‡åˆ†
    
    å½“è¯è¡¨å¾ˆå¤§æ—¶ï¼ˆå¦‚100k+ï¼‰ï¼ŒåµŒå…¥çŸ©é˜µä¼šéå¸¸å¤§
    è¯è¡¨å¹¶è¡Œå°†è¯è¡¨åˆ‡åˆ†åˆ°å¤šä¸ªGPUï¼Œæ¯ä¸ªGPUåªå­˜å‚¨éƒ¨åˆ†è¯å‘é‡
    
    åˆ‡åˆ†æ–¹å¼ï¼š
    - GPU 0: è¯è¡¨ [0, vocab_size//2)
    - GPU 1: è¯è¡¨ [vocab_size//2, vocab_size)
    
    å‰å‘ä¼ æ’­ï¼š
    1. å°†è¾“å…¥IDæ˜ å°„åˆ°æœ¬åœ°è¯è¡¨ç´¢å¼•
    2. åªæœ‰å±äºæœ¬åœ°è¯è¡¨çš„tokenæ‰ä¼šäº§ç”Ÿéé›¶è¾“å‡º
    3. é€šè¿‡all-reduceåˆå¹¶æ‰€æœ‰GPUçš„ç»“æœ
    """
    
    def __init__(
        self,
        num_embeddings: int,           # è¯è¡¨å¤§å°
        embedding_dim: int,            # åµŒå…¥ç»´åº¦
    ):
        super().__init__()
        self.tp_rank = dist.get_rank()
        self.tp_size = dist.get_world_size()
        
        # ç¡®ä¿è¯è¡¨å¯ä»¥è¢«GPUæ•°æ•´é™¤
        assert num_embeddings % self.tp_size == 0
        
        self.num_embeddings = num_embeddings
        # æ¯ä¸ªGPUè´Ÿè´£çš„è¯è¡¨å¤§å°
        self.num_embeddings_per_partition = self.num_embeddings // self.tp_size
        
        # æœ¬åœ°è¯è¡¨èŒƒå›´
        self.vocab_start_idx = self.num_embeddings_per_partition * self.tp_rank
        self.vocab_end_idx = self.vocab_start_idx + self.num_embeddings_per_partition
        
        # åˆ›å»ºæœ¬åœ°åµŒå…¥çŸ©é˜µ
        self.weight = nn.Parameter(torch.empty(
            self.num_embeddings_per_partition, 
            embedding_dim
        ))
        self.weight.weight_loader = self.weight_loader

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        """åŠ è½½è¯è¡¨åˆ‡åˆ†çš„æƒé‡"""
        param_data = param.data
        shard_size = param_data.size(0)
        start_idx = self.tp_rank * shard_size
        loaded_weight = loaded_weight.narrow(0, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor):
        """
        è¯è¡¨å¹¶è¡ŒåµŒå…¥å‰å‘
        
        Args:
            x: è¾“å…¥token IDs [batch, seq_len]
        
        Returns:
            åµŒå…¥å‘é‡ [batch, seq_len, embedding_dim]
        """
        if self.tp_size > 1:
            # åˆ›å»ºæ©ç ï¼šæ ‡è®°å“ªäº›tokenå±äºæœ¬åœ°è¯è¡¨
            mask = (x >= self.vocab_start_idx) & (x < self.vocab_end_idx)
            
            # å°†å…¨å±€IDæ˜ å°„åˆ°æœ¬åœ°IDï¼ˆä¸å±äºæœ¬åœ°çš„è®¾ä¸º0ï¼‰
            x = mask * (x - self.vocab_start_idx)
        
        # æŸ¥æ‰¾åµŒå…¥å‘é‡
        y = F.embedding(x, self.weight)
        
        if self.tp_size > 1:
            # åº”ç”¨æ©ç ï¼šä¸å±äºæœ¬åœ°çš„tokenåµŒå…¥è®¾ä¸º0
            y = mask.unsqueeze(1) * y
            
            # é€šè¿‡all-reduceåˆå¹¶æ‰€æœ‰GPUçš„ç»“æœ
            dist.all_reduce(y)
        
        return y


class ParallelLMHead(VocabParallelEmbedding):
    """
    å¹¶è¡Œè¯­è¨€æ¨¡å‹è¾“å‡ºå¤´
    
    ç»§æ‰¿è‡ªVocabParallelEmbeddingï¼Œä½†å‰å‘é€»è¾‘ä¸åŒï¼š
    - Embedding: è¾“å…¥token IDï¼Œè¾“å‡ºåµŒå…¥å‘é‡
    - LMHead: è¾“å…¥éšè—çŠ¶æ€ï¼Œè¾“å‡ºæ¯ä¸ªè¯çš„å¯¹æ•°å‡ ç‡
    
    æ³¨æ„ï¼šLMHeadé€šå¸¸ä¸è¾“å…¥åµŒå…¥å…±äº«æƒé‡ï¼ˆtie_word_embeddingsï¼‰
    """
    
    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        bias: bool = False,            # é€šå¸¸ä¸ä½¿ç”¨åç½®
    ):
        assert not bias  # å½“å‰ä¸æ”¯æŒåç½®
        super().__init__(num_embeddings, embedding_dim)

    def forward(self, x: torch.Tensor):
        """
        è¯­è¨€æ¨¡å‹è¾“å‡ºå¤´å‰å‘
        
        Args:
            x: éšè—çŠ¶æ€ [num_tokens, hidden_size]
        
        Returns:
            å¯¹æ•°å‡ ç‡ [num_tokens, vocab_size]ï¼ˆåªåœ¨rank 0ï¼‰
        """
        context = get_context()
        
        # Prefillé˜¶æ®µï¼šåªå–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®
        if context.is_prefill:
            # cu_seqlens_q[1:] - 1 æ˜¯æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªtokençš„ç´¢å¼•
            last_indices = context.cu_seqlens_q[1:] - 1
            x = x[last_indices].contiguous()
        
        # çº¿æ€§å˜æ¢ï¼šhidden -> vocab
        # ä½¿ç”¨åµŒå…¥çŸ©é˜µçš„è½¬ç½®ä½œä¸ºè¾“å‡ºæƒé‡ï¼ˆæƒé‡å…±äº«ï¼‰
        logits = F.linear(x, self.weight)
        
        if self.tp_size > 1:
            # å¤šGPUæ—¶ï¼Œéœ€è¦gatheræ‰€æœ‰GPUçš„ç»“æœ
            if self.tp_rank == 0:
                # rank 0æ”¶é›†æ‰€æœ‰ç»“æœ
                all_logits = [torch.empty_like(logits) for _ in range(self.tp_size)]
            else:
                all_logits = None
            
            dist.gather(logits, all_logits, 0)
            
            # rank 0æ‹¼æ¥æ‰€æœ‰ç»“æœ
            if self.tp_rank == 0:
                logits = torch.cat(all_logits, -1)
        
        return logits
```

**è¯è¡¨å¹¶è¡Œå›¾è§£ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¯è¡¨å¹¶è¡Œ (Vocab Parallelism)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  é—®é¢˜ï¼šè¯è¡¨100kï¼Œç»´åº¦4096ï¼ŒåµŒå…¥çŸ©é˜µå¤§å° = 100k * 4096 * 2B â‰ˆ 800MB       â”‚
â”‚                                                                         â”‚
â”‚  è§£å†³æ–¹æ¡ˆï¼šå°†è¯è¡¨åˆ‡åˆ†åˆ°2ä¸ªGPU                                            â”‚
â”‚                                                                         â”‚
â”‚  GPU 0:                    GPU 1:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚  è¯è¡¨ [0-50k)   â”‚      â”‚  è¯è¡¨ [50k-100k)â”‚                          â”‚
â”‚  â”‚  æƒé‡ W0        â”‚      â”‚  æƒé‡ W1        â”‚                          â”‚
â”‚  â”‚  [50k, 4096]    â”‚      â”‚  [50k, 4096]    â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚           â”‚                        â”‚                                    â”‚
â”‚           â”‚  è¾“å…¥: token IDs       â”‚                                    â”‚
â”‚           â”‚  [12, 50001, 34, 99999]â”‚                                    â”‚
â”‚           â”‚                        â”‚                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                             â”‚
â”‚     â–¼           â–¼            â–¼           â–¼                             â”‚
â”‚  mask: [1,0,1,0]          mask: [0,1,0,1]                              â”‚
â”‚  local_id: [12,0,34,0]    local_id: [0,1,0,49999]                      â”‚
â”‚     â”‚           â”‚            â”‚           â”‚                             â”‚
â”‚     â–¼           â–¼            â–¼           â–¼                             â”‚
â”‚  embed: E0    zeros       zeros       E1                               â”‚
â”‚     â”‚           â”‚            â”‚           â”‚                             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚           â”‚                        â”‚                                    â”‚
â”‚           â”‚    all_reduce(E0+E1)   â”‚                                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                      â–¼                                                  â”‚
â”‚                   å®Œæ•´åµŒå…¥                                              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 13. layers/sampler.py - é‡‡æ ·å™¨

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—


class Sampler(nn.Module):
    """
    é‡‡æ ·å™¨ - ä»æ¨¡å‹è¾“å‡ºçš„logitsä¸­é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
    
    ä½¿ç”¨æ¸©åº¦é‡‡æ ·ï¼ˆTemperature Samplingï¼‰ï¼š
    1. logitsé™¤ä»¥temperatureï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
    2. softmaxè½¬æ¢ä¸ºæ¦‚ç‡
    3. ä½¿ç”¨æŒ‡æ•°åˆ†å¸ƒè¿›è¡Œé‡‡æ ·
    
    é‡‡æ ·å…¬å¼ï¼š
    token = argmax(softmax(logits / T) / exp(-U))
    å…¶ä¸­ U ~ Exponential(1)ï¼ŒT = temperature
    
    è¿™ç­‰ä»·äºä»softmax(logits / T)ä¸­é‡‡æ ·
    """
    
    def __init__(self):
        super().__init__()

    @torch.compile
    def forward(self, logits: torch.Tensor, temperatures: torch.Tensor):
        """
        é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºçš„å¯¹æ•°å‡ ç‡ [batch, vocab_size]
            temperatures: æ¯ä¸ªåºåˆ—çš„æ¸©åº¦ [batch]
        
        Returns:
            é‡‡æ ·çš„token IDs [batch]
        """
        # è½¬ä¸ºfloat32æé«˜æ•°å€¼ç¨³å®šæ€§
        logits = logits.float()
        
        # åº”ç”¨æ¸©åº¦ï¼šlogits / temperature
        # temperature > 1: æ›´éšæœº
        # temperature < 1: æ›´ç¡®å®š
        logits.div_(temperatures.unsqueeze(dim=1))
        
        # softmaxè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        probs = torch.softmax(logits, dim=-1)
        
        # Gumbelé‡‡æ ·æŠ€å·§ï¼š
        # argmax(probs / exp(-U)) å…¶ä¸­ U ~ Exponential(1)
        # ç­‰ä»·äºä»probsä¸­é‡‡æ ·
        # 
        # torch.empty_like(probs).exponential_(1) ç”ŸæˆæŒ‡æ•°åˆ†å¸ƒéšæœºæ•°
        # clamp_min_(1e-10) é¿å…é™¤é›¶
        # probs / random ç„¶åå–argmax
        sample_tokens = probs.div_(
            torch.empty_like(probs).exponential_(1).clamp_min_(1e-10)
        ).argmax(dim=-1)
        
        return sample_tokens
```

**é‡‡æ ·æ–¹æ³•å¯¹æ¯”ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        é‡‡æ ·æ–¹æ³•å¯¹æ¯”                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  è´ªå©ªè§£ç  (Greedy Decoding)                                             â”‚
â”‚  token = argmax(logits)                                                 â”‚
â”‚  ç‰¹ç‚¹ï¼šç¡®å®šæ€§ï¼Œæ€»æ˜¯é€‰æ¦‚ç‡æœ€é«˜çš„                                           â”‚
â”‚  ç¼ºç‚¹ï¼šè¾“å‡ºå•ä¸€ï¼Œç¼ºä¹å¤šæ ·æ€§                                               â”‚
â”‚                                                                         â”‚
â”‚  æ¸©åº¦é‡‡æ · (Temperature Sampling)                                        â”‚
â”‚  probs = softmax(logits / T)                                            â”‚
â”‚  token = sample(probs)                                                  â”‚
â”‚                                                                         â”‚
â”‚  T = 0.3:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ä¿å®ˆï¼Œæ¥è¿‘è´ªå©ª                          â”‚
â”‚  T = 0.7:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        å¹³è¡¡                                    â”‚
â”‚  T = 1.0:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              æ ‡å‡†éšæœº                                â”‚
â”‚  T = 1.5:  â–ˆâ–ˆâ–ˆâ–ˆ                  æ›´éšæœº                                  â”‚
â”‚                                                                         â”‚
â”‚  Top-k é‡‡æ ·                                                             â”‚
â”‚  åªä»æ¦‚ç‡æœ€é«˜çš„kä¸ªtokenä¸­é‡‡æ ·                                             â”‚
â”‚  é¿å…é€‰ä¸­æä½æ¦‚ç‡çš„token                                                  â”‚
â”‚                                                                         â”‚
â”‚  Top-p (Nucleus) é‡‡æ ·                                                   â”‚
â”‚  ä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°é›†åˆä¸­é‡‡æ ·                                          â”‚
â”‚  åŠ¨æ€è°ƒæ•´å€™é€‰é›†å¤§å°                                                       â”‚
â”‚                                                                         â”‚
â”‚  Nano-vLLMä½¿ç”¨ï¼šæ¸©åº¦é‡‡æ ·ï¼ˆæœ€å¸¸ç”¨ï¼‰                                        â”‚
â”‚                                                                         â”‚
â”‚  Gumbelé‡‡æ ·æŠ€å·§ï¼š                                                         â”‚
â”‚  ä¸ç›´æ¥é‡‡æ ·ï¼Œè€Œæ˜¯ï¼šargmax(probs / exp(-U))                               â”‚
â”‚  è¿™æ ·å¯ä»¥ç”¨argmaxå®ç°é‡‡æ ·æ•ˆæœï¼Œæ›´é«˜æ•ˆ                                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---


### 14. models/qwen3.py - Qwen3 æ¨¡å‹å®ç°

```python
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.distributed as dist      # åˆ†å¸ƒå¼
from transformers import Qwen3Config  # HuggingFaceé…ç½®

# å¯¼å…¥æ‰€æœ‰å±‚
from nanovllm.layers.activation import SiluAndMul
from nanovllm.layers.attention import Attention
from nanovllm.layers.layernorm import RMSNorm
from nanovllm.layers.linear import QKVParallelLinear, MergedColumnParallelLinear, RowParallelLinear
from nanovllm.layers.rotary_embedding import get_rope
from nanovllm.layers.embed_head import VocabParallelEmbedding, ParallelLMHead


class Qwen3Attention(nn.Module):
    """
    Qwen3 æ³¨æ„åŠ›å±‚
    
    ç»“æ„ï¼š
    1. QKVæŠ•å½±ï¼ˆåˆå¹¶ä¸ºä¸€ä¸ªçº¿æ€§å±‚ï¼‰
    2. å¯é€‰çš„Q/Kå½’ä¸€åŒ–
    3. æ—‹è½¬ä½ç½®ç¼–ç 
    4. æ³¨æ„åŠ›è®¡ç®—
    5. è¾“å‡ºæŠ•å½±
    
    æ”¯æŒï¼š
    - å¼ é‡å¹¶è¡Œ
    - GQA (Grouped Query Attention)
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        max_position: int = 4096 * 32,
        head_dim: int | None = None,
        rms_norm_eps: float = 1e-06,
        qkv_bias: bool = False,
        rope_theta: float = 10000,
        rope_scaling: tuple | None = None,
    ) -> None:
        super().__init__()
        
        tp_size = dist.get_world_size()
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size  # å½“å‰GPUçš„å¤´æ•°
        
        self.total_num_kv_heads = num_kv_heads
        assert self.total_num_kv_heads % tp_size == 0
        self.num_kv_heads = self.total_num_kv_heads // tp_size
        
        self.head_dim = head_dim or hidden_size // self.total_num_heads
        
        # QKVå¤§å°
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        
        # ç¼©æ”¾å› å­
        self.scaling = self.head_dim ** -0.5
        self.qkv_bias = qkv_bias
        
        # QKVæŠ•å½±ï¼ˆåˆ—å¹¶è¡Œï¼‰
        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=qkv_bias,
        )
        
        # è¾“å‡ºæŠ•å½±ï¼ˆè¡Œå¹¶è¡Œï¼‰
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
        )
        
        # æ—‹è½¬ä½ç½®ç¼–ç 
        self.rotary_emb = get_rope(
            self.head_dim,
            rotary_dim=self.head_dim,
            max_position=max_position,
            base=rope_theta,
            rope_scaling=rope_scaling,
        )
        
        # æ³¨æ„åŠ›è®¡ç®—æ¨¡å—
        self.attn = Attention(
            self.num_heads,
            self.head_dim,
            self.scaling,
            self.num_kv_heads,
        )
        
        # å¯é€‰çš„Q/Kå½’ä¸€åŒ–ï¼ˆæ— biasæ—¶ä½¿ç”¨ï¼‰
        if not self.qkv_bias:
            self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
            self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)

    def forward(
        self,
        positions: torch.Tensor,           # ä½ç½®ç´¢å¼•
        hidden_states: torch.Tensor,       # éšè—çŠ¶æ€
    ) -> torch.Tensor:
        """
        æ³¨æ„åŠ›å‰å‘ä¼ æ’­
        
        Args:
            positions: ä½ç½®ç´¢å¼• [num_tokens]
            hidden_states: éšè—çŠ¶æ€ [num_tokens, hidden_size]
        
        Returns:
            æ³¨æ„åŠ›è¾“å‡º [num_tokens, hidden_size]
        """
        # QKVæŠ•å½±
        qkv = self.qkv_proj(hidden_states)
        
        # åˆ‡åˆ†ä¸ºQã€Kã€V
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        
        # reshapeä¸ºå¤šå¤´æ ¼å¼
        q = q.view(-1, self.num_heads, self.head_dim)
        k = k.view(-1, self.num_kv_heads, self.head_dim)
        v = v.view(-1, self.num_kv_heads, self.head_dim)
        
        # å¯é€‰çš„Q/Kå½’ä¸€åŒ–
        if not self.qkv_bias:
            q = self.q_norm(q)
            k = self.k_norm(k)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q, k = self.rotary_emb(positions, q, k)
        
        # æ³¨æ„åŠ›è®¡ç®—
        o = self.attn(q, k, v)
        
        # è¾“å‡ºæŠ•å½±
        output = self.o_proj(o.flatten(1, -1))
        
        return output


class Qwen3MLP(nn.Module):
    """
    Qwen3 MLPå±‚ï¼ˆå‰é¦ˆç½‘ç»œï¼‰
    
    ç»“æ„ï¼š
    1. gate_proj å’Œ up_proj åˆå¹¶
    2. SwiGLUæ¿€æ´»
    3. down_proj
    
    å…¬å¼ï¼šdown_proj(SiLU(gate_proj(x)) * up_proj(x))
    """
    
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
    ) -> None:
        super().__init__()
        
        # gate_proj å’Œ up_proj åˆå¹¶ï¼ˆåˆ—å¹¶è¡Œï¼‰
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,   # ä¸¤ä¸ªè¾“å‡ºï¼Œæ¯ä¸ªå¤§å°ä¸ºintermediate_size
            bias=False,
        )
        
        # down_projï¼ˆè¡Œå¹¶è¡Œï¼‰
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
        )
        
        # æ¿€æ´»å‡½æ•°
        assert hidden_act == "silu"
        self.act_fn = SiluAndMul()

    def forward(self, x):
        """MLPå‰å‘ä¼ æ’­"""
        gate_up = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x = self.down_proj(x)
        return x


class Qwen3DecoderLayer(nn.Module):
    """
    Qwen3 è§£ç å™¨å±‚
    
    ç»“æ„ï¼ˆPre-LNï¼‰ï¼š
    1. RMSNorm
    2. Self-Attention
    3. RMSNorm
    4. MLP
    
    ä½¿ç”¨æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self,
        config: Qwen3Config,
    ) -> None:
        super().__init__()
        
        self.self_attn = Qwen3Attention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            max_position=config.max_position_embeddings,
            rms_norm_eps=config.rms_norm_eps,
            qkv_bias=getattr(config, 'attention_bias', True),
            head_dim=getattr(config, 'head_dim', None),
            rope_theta=getattr(config, "rope_theta", 1000000),
            rope_scaling=getattr(config, "rope_scaling", None),
        )
        
        self.mlp = Qwen3MLP(
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
        )
        
        # ä¸¤ä¸ªLayerNorm
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        residual: torch.Tensor | None,     # æ®‹å·®è¿æ¥
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        è§£ç å™¨å±‚å‰å‘ä¼ æ’­
        
        Args:
            positions: ä½ç½®ç´¢å¼•
            hidden_states: éšè—çŠ¶æ€
            residual: æ®‹å·®ï¼ˆç¬¬ä¸€å±‚ä¸ºNoneï¼‰
        
        Returns:
            (æ–°çš„éšè—çŠ¶æ€, æ›´æ–°åçš„æ®‹å·®)
        """
        # ç¬¬ä¸€å±‚ï¼šåˆå§‹åŒ–æ®‹å·®
        if residual is None:
            hidden_states, residual = self.input_layernorm(hidden_states), hidden_states
        else:
            hidden_states, residual = self.input_layernorm(hidden_states, residual)
        
        # Self-Attention
        hidden_states = self.self_attn(positions, hidden_states)
        
        # MLP
        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
        hidden_states = self.mlp(hidden_states)
        
        return hidden_states, residual


class Qwen3Model(nn.Module):
    """
    Qwen3 åŸºç¡€æ¨¡å‹ï¼ˆä¸å«è¾“å‡ºå¤´ï¼‰
    
    ç»“æ„ï¼š
    1. è¯åµŒå…¥
    2. Nä¸ªè§£ç å™¨å±‚
    3. æœ€ç»ˆLayerNorm
    """
    
    def __init__(
        self,
        config: Qwen3Config,
    ) -> None:
        super().__init__()
        
        # è¯åµŒå…¥ï¼ˆè¯è¡¨å¹¶è¡Œï¼‰
        self.embed_tokens = VocabParallelEmbedding(
            config.vocab_size, 
            config.hidden_size
        )
        
        # è§£ç å™¨å±‚åˆ—è¡¨
        self.layers = nn.ModuleList([
            Qwen3DecoderLayer(config) 
            for _ in range(config.num_hidden_layers)
        ])
        
        # æœ€ç»ˆLayerNorm
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_ids: torch.Tensor,           # è¾“å…¥token IDs
        positions: torch.Tensor,           # ä½ç½®ç´¢å¼•
    ) -> torch.Tensor:
        """
        æ¨¡å‹å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token IDs [num_tokens]
            positions: ä½ç½®ç´¢å¼• [num_tokens]
        
        Returns:
            æœ€ç»ˆéšè—çŠ¶æ€ [num_tokens, hidden_size]
        """
        # è¯åµŒå…¥
        hidden_states = self.embed_tokens(input_ids)
        
        # æ®‹å·®è¿æ¥ï¼ˆç¬¬ä¸€å±‚ä¸ºNoneï¼‰
        residual = None
        
        # é€å±‚å‰å‘ä¼ æ’­
        for layer in self.layers:
            hidden_states, residual = layer(positions, hidden_states, residual)
        
        # æœ€ç»ˆLayerNorm
        hidden_states, _ = self.norm(hidden_states, residual)
        
        return hidden_states


class Qwen3ForCausalLM(nn.Module):
    """
    Qwen3 å› æœè¯­è¨€æ¨¡å‹ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰
    
    åŒ…å«ï¼š
    1. åŸºç¡€æ¨¡å‹
    2. è¯­è¨€æ¨¡å‹è¾“å‡ºå¤´
    
    packed_modules_mapping: æƒé‡æ˜ å°„
    - å°†HuggingFaceçš„åˆ†ç¦»æƒé‡æ˜ å°„åˆ°åˆå¹¶çš„æƒé‡
    """
    
    # æƒé‡æ˜ å°„ï¼šHuggingFaceåç§° -> (æœ¬åœ°åç§°, shard_id)
    packed_modules_mapping = {
        "q_proj": ("qkv_proj", "q"),
        "k_proj": ("qkv_proj", "k"),
        "v_proj": ("qkv_proj", "v"),
        "gate_proj": ("gate_up_proj", 0),
        "up_proj": ("gate_up_proj", 1),
    }

    def __init__(
        self,
        config: Qwen3Config
    ) -> None:
        super().__init__()
        
        self.model = Qwen3Model(config)
        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)
        
        # æƒé‡å…±äº«ï¼ˆtie_word_embeddingsï¼‰
        if config.tie_word_embeddings:
            self.lm_head.weight.data = self.model.embed_tokens.weight.data

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
    ) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›éšè—çŠ¶æ€"""
        return self.model(input_ids, positions)

    def compute_logits(
        self,
        hidden_states: torch.Tensor,
    ) -> torch.Tensor:
        """è®¡ç®—è¾“å‡ºlogits"""
        return self.lm_head(hidden_states)
```

**Qwen3 æ¨¡å‹ç»“æ„å›¾ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Qwen3 æ¨¡å‹ç»“æ„                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Input: token IDs                                                       â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Embedding (VocabParallel)                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Layer 0                                                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚
â”‚  â”‚  â”‚ RMSNorm     â”‚â”€â”€â”€â–ºâ”‚ Attention   â”‚                            â”‚   â”‚
â”‚  â”‚  â”‚             â”‚    â”‚ - QKV Proj  â”‚                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - RoPE      â”‚                            â”‚   â”‚
â”‚  â”‚         â”‚           â”‚ - FlashAttn â”‚                            â”‚   â”‚
â”‚  â”‚         â”‚           â”‚ - Out Proj  â”‚                            â”‚   â”‚
â”‚  â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚   â”‚
â”‚  â”‚         â”‚                  â”‚                                    â”‚   â”‚
â”‚  â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”˜                                   â”‚   â”‚
â”‚  â”‚         â”‚           â–¼                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚
â”‚  â”‚  â”‚ RMSNorm     â”‚â”€â”€â”€â–ºâ”‚ MLP         â”‚                            â”‚   â”‚
â”‚  â”‚  â”‚             â”‚    â”‚ - GateUp    â”‚                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - SwiGLU    â”‚                            â”‚   â”‚
â”‚  â”‚                     â”‚ - Down      â”‚                            â”‚   â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚      ... (é‡å¤ N å±‚)                                                    â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Final RMSNorm                                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LM Head (Parallel)                                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  Output: logits                                                         â”‚
â”‚                                                                         â”‚
â”‚  å…¸å‹é…ç½® (Qwen3-0.6B):                                                  â”‚
â”‚  - å±‚æ•°: 28                                                             â”‚
â”‚  - éšè—ç»´åº¦: 1024                                                       â”‚
â”‚  - æ³¨æ„åŠ›å¤´: 16                                                         â”‚
â”‚  - KVå¤´: 8 (GQA)                                                        â”‚
â”‚  - ä¸­é—´ç»´åº¦: 2816                                                       â”‚
â”‚  - è¯è¡¨: 151936                                                         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 15. utils/loader.py - æ¨¡å‹æƒé‡åŠ è½½

```python
import os                             # æ“ä½œç³»ç»Ÿæ¥å£
from glob import glob                 # æ–‡ä»¶è·¯å¾„åŒ¹é…
import torch                          # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn                  # ç¥ç»ç½‘ç»œæ¨¡å—
from safetensors import safe_open     # SafeTensorsæ ¼å¼ï¼ˆå®‰å…¨çš„tensoråºåˆ—åŒ–ï¼‰


def default_weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor):
    """
    é»˜è®¤æƒé‡åŠ è½½å‡½æ•° - ç›´æ¥å¤åˆ¶
    
    Args:
        param: ç›®æ ‡å‚æ•°
        loaded_weight: åŠ è½½çš„æƒé‡
    """
    param.data.copy_(loaded_weight)


def load_model(model: nn.Module, path: str):
    """
    åŠ è½½æ¨¡å‹æƒé‡
    
    æ”¯æŒï¼š
    1. SafeTensorsæ ¼å¼ï¼ˆæ¨èï¼Œå®‰å…¨ä¸”é«˜æ•ˆï¼‰
    2. åˆå¹¶æƒé‡è‡ªåŠ¨æ‹†åˆ†ï¼ˆå¦‚qkv_projæ‹†åˆ†ä¸ºq_proj,k_proj,v_projï¼‰
    3. å¼ é‡å¹¶è¡Œæƒé‡åŠ è½½
    
    Args:
        model: è¦åŠ è½½æƒé‡çš„æ¨¡å‹
        path: æƒé‡æ–‡ä»¶ç›®å½•
    """
    # è·å–æ¨¡å‹çš„æƒé‡æ˜ å°„ï¼ˆå¦‚æœæœ‰ï¼‰
    # ä¾‹å¦‚ Qwen3ForCausalLM.packed_modules_mapping
    packed_modules_mapping = getattr(model, "packed_modules_mapping", {})
    
    # éå†æ‰€æœ‰.safetensorsæ–‡ä»¶
    for file in glob(os.path.join(path, "*.safetensors")):
        with safe_open(file, "pt", "cpu") as f:
            # éå†æ–‡ä»¶ä¸­çš„æ‰€æœ‰æƒé‡
            for weight_name in f.keys():
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„
                for k in packed_modules_mapping:
                    if k in weight_name:
                        # éœ€è¦æ˜ å°„çš„æƒé‡
                        # ä¾‹å¦‚ï¼šq_proj -> (qkv_proj, "q")
                        v, shard_id = packed_modules_mapping[k]
                        param_name = weight_name.replace(k, v)
                        param = model.get_parameter(param_name)
                        
                        # ä½¿ç”¨è‡ªå®šä¹‰çš„weight_loader
                        weight_loader = getattr(param, "weight_loader")
                        weight_loader(param, f.get_tensor(weight_name), shard_id)
                        break
                else:
                    # ä¸éœ€è¦æ˜ å°„çš„æƒé‡ï¼Œç›´æ¥åŠ è½½
                    param = model.get_parameter(weight_name)
                    weight_loader = getattr(param, "weight_loader", default_weight_loader)
                    weight_loader(param, f.get_tensor(weight_name))
```

**æƒé‡åŠ è½½æµç¨‹ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æƒé‡åŠ è½½æµç¨‹                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  HuggingFaceæƒé‡æ–‡ä»¶:                                                   â”‚
â”‚  model-00001-of-00002.safetensors                                       â”‚
â”‚  â”œâ”€â”€ q_proj.weight                                                      â”‚
â”‚  â”œâ”€â”€ k_proj.weight                                                      â”‚
â”‚  â”œâ”€â”€ v_proj.weight                                                      â”‚
â”‚  â”œâ”€â”€ gate_proj.weight                                                   â”‚
â”‚  â”œâ”€â”€ up_proj.weight                                                     â”‚
â”‚  â””â”€â”€ ...                                                                â”‚
â”‚                                                                         â”‚
â”‚  åŠ è½½è¿‡ç¨‹ï¼š                                                              â”‚
â”‚                                                                         â”‚
â”‚  q_proj.weight â”€â”€â”                                                      â”‚
â”‚                  â”œâ”€â”€â–º packed_modules_mapping â”€â”€â–º qkv_proj (shard="q")  â”‚
â”‚  k_proj.weight â”€â”€â”¤                      â”‚                               â”‚
â”‚                  â”œâ”€â”€â–º packed_modules_mapping â”€â”€â–º qkv_proj (shard="k")  â”‚
â”‚  v_proj.weight â”€â”€â”˜                      â”‚                               â”‚
â”‚                                         â”‚                               â”‚
â”‚  gate_proj.weight â”€â”€â–º packed_modules_mapping â”€â”€â–º gate_up_proj (id=0)   â”‚
â”‚  up_proj.weight â”€â”€â”€â”€â–º packed_modules_mapping â”€â”€â–º gate_up_proj (id=1)   â”‚
â”‚                                                                         â”‚
â”‚  å¼ é‡å¹¶è¡ŒåŠ è½½ï¼š                                                          â”‚
â”‚  qkv_proj.weight = [Q_part, K_part, V_part]                            â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â”œâ”€â”€â–º GPU 0: Q_part[0:Q//2], K_part[0:K//2], V_part[0:V//2]       â”‚
â”‚       â””â”€â”€â–º GPU 1: Q_part[Q//2:], K_part[K//2:], V_part[V//2:]          â”‚
â”‚                                                                         â”‚
â”‚  weight_loader çš„ä½œç”¨ï¼š                                                  â”‚
â”‚  1. åˆ‡åˆ†æƒé‡åˆ°å½“å‰GPU                                                    â”‚
â”‚  2. å¤„ç†åˆå¹¶æƒé‡çš„ç‰¹å®šéƒ¨åˆ†                                               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 16. engine/model_runner.py - æ¨¡å‹è¿è¡Œå™¨

```python
import pickle                         # åºåˆ—åŒ–
import torch                          # PyTorch
import torch.distributed as dist      # åˆ†å¸ƒå¼
from multiprocessing.synchronize import Event      # è¿›ç¨‹åŒæ­¥
from multiprocessing.shared_memory import SharedMemory  # å…±äº«å†…å­˜

from nanovllm.config import Config
from nanovllm.engine.sequence import Sequence
from nanovllm.models.qwen3 import Qwen3ForCausalLM
from nanovllm.layers.sampler import Sampler
from nanovllm.utils.context import set_context, get_context, reset_context
from nanovllm.utils.loader import load_model


class ModelRunner:
    """
    æ¨¡å‹è¿è¡Œå™¨ - ç®¡ç†æ¨¡å‹çš„åŠ è½½ã€KV Cacheåˆ†é…å’Œæ¨ç†æ‰§è¡Œ
    
    èŒè´£ï¼š
    1. æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–
    2. KV Cacheé¢„åˆ†é…
    3. Warmupï¼ˆé¢„çƒ­ï¼‰
    4. CUDA Graphæ•è·
    5. Prefill/Decodeæ‰§è¡Œ
    
    å¤šGPUæ”¯æŒï¼š
    - rank 0: ä¸»è¿›ç¨‹ï¼Œæ‰§è¡Œè°ƒåº¦
    - rank > 0: å·¥ä½œè¿›ç¨‹ï¼Œé€šè¿‡å…±äº«å†…å­˜æ¥æ”¶æŒ‡ä»¤
    """
    
    def __init__(self, config: Config, rank: int, event: Event | list[Event]):
        """
        åˆå§‹åŒ–æ¨¡å‹è¿è¡Œå™¨
        
        Args:
            config: å…¨å±€é…ç½®
            rank: å½“å‰GPUçš„rank
            event: è¿›ç¨‹åŒæ­¥äº‹ä»¶ï¼ˆå¤šGPUæ—¶ä½¿ç”¨ï¼‰
        """
        self.config = config
        hf_config = config.hf_config
        self.block_size = config.kvcache_block_size
        self.enforce_eager = config.enforce_eager
        self.world_size = config.tensor_parallel_size
        self.rank = rank
        self.event = event
        
        # ==================== åˆå§‹åŒ–åˆ†å¸ƒå¼ ====================
        dist.init_process_group(
            "nccl",                         # NCCLåç«¯ï¼ˆNVIDIA GPUï¼‰
            "tcp://localhost:2333",         # ä¸»èŠ‚ç‚¹åœ°å€
            world_size=self.world_size,
            rank=rank
        )
        torch.cuda.set_device(rank)
        
        # ==================== è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹å’Œè®¾å¤‡ ====================
        default_dtype = torch.get_default_dtype()
        torch.set_default_dtype(hf_config.torch_dtype)
        torch.set_default_device("cuda")
        
        # ==================== åˆ›å»ºæ¨¡å‹ ====================
        self.model = Qwen3ForCausalLM(hf_config)
        load_model(self.model, config.model)
        
        # ==================== åˆ›å»ºé‡‡æ ·å™¨ ====================
        self.sampler = Sampler()
        
        # ==================== Warmupå’Œåˆ†é… ====================
        self.warmup_model()
        self.allocate_kv_cache()
        
        # ==================== CUDA Graph ====================
        if not self.enforce_eager:
            self.capture_cudagraph()
        
        # æ¢å¤é»˜è®¤è®¾ç½®
        torch.set_default_device("cpu")
        torch.set_default_dtype(default_dtype)
        
        # ==================== å¤šGPUè®¾ç½® ====================
        if self.world_size > 1:
            if rank == 0:
                # rank 0åˆ›å»ºå…±äº«å†…å­˜
                self.shm = SharedMemory(name="nanovllm", create=True, size=2**20)
                dist.barrier()
            else:
                # å…¶ä»–rankç­‰å¾…å¹¶è¿æ¥å…±äº«å†…å­˜
                dist.barrier()
                self.shm = SharedMemory(name="nanovllm")
                # å·¥ä½œè¿›ç¨‹è¿›å…¥äº‹ä»¶å¾ªç¯
                self.loop()

    def exit(self):
        """æ¸…ç†èµ„æº"""
        if self.world_size > 1:
            self.shm.close()
            dist.barrier()
            if self.rank == 0:
                self.shm.unlink()
        if not self.enforce_eager:
            del self.graphs, self.graph_pool
        torch.cuda.synchronize()
        dist.destroy_process_group()

    def loop(self):
        """å·¥ä½œè¿›ç¨‹çš„äº‹ä»¶å¾ªç¯"""
        while True:
            method_name, args = self.read_shm()
            self.call(method_name, *args)
            if method_name == "exit":
                break

    def read_shm(self):
        """ä»å…±äº«å†…å­˜è¯»å–æŒ‡ä»¤"""
        assert self.world_size > 1 and self.rank > 0
        self.event.wait()  # ç­‰å¾…rank 0çš„ä¿¡å·
        n = int.from_bytes(self.shm.buf[0:4], "little")
        method_name, *args = pickle.loads(self.shm.buf[4:n+4])
        self.event.clear()
        return method_name, args

    def write_shm(self, method_name, *args):
        """å‘å…±äº«å†…å­˜å†™å…¥æŒ‡ä»¤"""
        assert self.world_size > 1 and self.rank == 0
        data = pickle.dumps([method_name, *args])
        n = len(data)
        self.shm.buf[0:4] = n.to_bytes(4, "little")
        self.shm.buf[4:n+4] = data
        for event in self.event:
            event.set()  # é€šçŸ¥æ‰€æœ‰å·¥ä½œè¿›ç¨‹

    def call(self, method_name, *args):
        """è°ƒç”¨æ–¹æ³•ï¼ˆå¤šGPUæ—¶é€šè¿‡å…±äº«å†…å­˜åŒæ­¥ï¼‰"""
        if self.world_size > 1 and self.rank == 0:
            self.write_shm(method_name, *args)
        method = getattr(self, method_name, None)
        return method(*args)

    def warmup_model(self):
        """
        æ¨¡å‹é¢„çƒ­
        
        ç›®çš„ï¼š
        1. è§¦å‘CUDA kernelç¼–è¯‘
        2. æµ‹é‡å³°å€¼æ˜¾å­˜ä½¿ç”¨
        3. ç¡®ä¿åç»­æ¨ç†ç¨³å®š
        """
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # è®¡ç®—warmupçš„åºåˆ—æ•°
        max_num_batched_tokens = self.config.max_num_batched_tokens
        max_model_len = self.config.max_model_len
        num_seqs = min(max_num_batched_tokens // max_model_len, self.config.max_num_seqs)
        
        # åˆ›å»ºè™šæ‹Ÿåºåˆ—
        seqs = [Sequence([0] * max_model_len) for _ in range(num_seqs)]
        
        # æ‰§è¡Œä¸€æ¬¡æ¨ç†
        self.run(seqs, True)
        
        torch.cuda.empty_cache()

    def allocate_kv_cache(self):
        """
        åˆ†é…KV Cache
        
        æ ¹æ®å¯ç”¨æ˜¾å­˜è®¡ç®—å¯åˆ†é…çš„å—æ•°
        """
        config = self.config
        hf_config = config.hf_config
        
        # è·å–æ˜¾å­˜ä¿¡æ¯
        free, total = torch.cuda.mem_get_info()
        used = total - free
        peak = torch.cuda.memory_stats()["allocated_bytes.all.peak"]
        current = torch.cuda.memory_stats()["allocated_bytes.all.current"]
        
        # è®¡ç®—æ¯ä¸ªKVå—çš„å†…å­˜å¤§å°
        num_kv_heads = hf_config.num_key_value_heads // self.world_size
        head_dim = getattr(hf_config, "head_dim", hf_config.hidden_size // hf_config.num_attention_heads)
        
        # KV Cacheå¤§å° = 2(K+V) * å±‚æ•° * å—æ•° * å—å¤§å° * KVå¤´æ•° * å¤´ç»´åº¦ * æ•°æ®ç±»å‹å¤§å°
        block_bytes = 2 * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim * hf_config.torch_dtype.itemsize
        
        # è®¡ç®—å¯åˆ†é…çš„å—æ•°
        # å¯ç”¨æ˜¾å­˜ = æ€»æ˜¾å­˜ * ä½¿ç”¨ç‡ - å·²ç”¨ - å³°å€¼ + å½“å‰
        config.num_kvcache_blocks = int(
            total * config.gpu_memory_utilization - used - peak + current
        ) // block_bytes
        
        assert config.num_kvcache_blocks > 0
        
        # åˆ›å»ºKV Cacheå¼ é‡
        # å½¢çŠ¶: [2(K/V), å±‚æ•°, å—æ•°, å—å¤§å°, KVå¤´æ•°, å¤´ç»´åº¦]
        self.kv_cache = torch.empty(
            2, hf_config.num_hidden_layers, config.num_kvcache_blocks, 
            self.block_size, num_kv_heads, head_dim
        )
        
        # å°†KV Cacheåˆ†é…ç»™æ¯ä¸ªæ³¨æ„åŠ›å±‚
        layer_id = 0
        for module in self.model.modules():
            if hasattr(module, "k_cache") and hasattr(module, "v_cache"):
                module.k_cache = self.kv_cache[0, layer_id]
                module.v_cache = self.kv_cache[1, layer_id]
                layer_id += 1

    def prepare_block_tables(self, seqs: list[Sequence]):
        """å‡†å¤‡å—è¡¨å¼ é‡"""
        max_len = max(len(seq.block_table) for seq in seqs)
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        block_tables = [seq.block_table + [-1] * (max_len - len(seq.block_table)) for seq in seqs]
        block_tables = torch.tensor(block_tables, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        return block_tables

    def prepare_prefill(self, seqs: list[Sequence]):
        """
        å‡†å¤‡Prefillé˜¶æ®µçš„è¾“å…¥
        
        å¤„ç†å˜é•¿åºåˆ—æ‰¹å¤„ç†ï¼Œç”Ÿæˆï¼š
        - input_ids: æ‰€æœ‰tokençš„ID
        - positions: æ¯ä¸ªtokençš„ä½ç½®
        - cu_seqlens: ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆç”¨äºFlashAttentionï¼‰
        - slot_mapping: æ¯ä¸ªtokenåœ¨KV Cacheä¸­çš„ä½ç½®
        """
        input_ids = []
        positions = []
        cu_seqlens_q = [0]
        cu_seqlens_k = [0]
        max_seqlen_q = 0
        max_seqlen_k = 0
        slot_mapping = []
        block_tables = None
        
        for seq in seqs:
            seqlen = len(seq)
            
            # åªå–æœªç¼“å­˜çš„tokenä½œä¸ºQuery
            input_ids.extend(seq[seq.num_cached_tokens:])
            positions.extend(list(range(seq.num_cached_tokens, seqlen)))
            
            seqlen_q = seqlen - seq.num_cached_tokens  # Queryé•¿åº¦
            seqlen_k = seqlen                           # Keyé•¿åº¦ï¼ˆåŒ…å«ç¼“å­˜çš„ï¼‰
            
            cu_seqlens_q.append(cu_seqlens_q[-1] + seqlen_q)
            cu_seqlens_k.append(cu_seqlens_k[-1] + seqlen_k)
            
            max_seqlen_q = max(seqlen_q, max_seqlen_q)
            max_seqlen_k = max(seqlen_k, max_seqlen_k)
            
            if not seq.block_table:  # warmupæ—¶
                continue
            
            # è®¡ç®—slot_mappingï¼ˆæœªç¼“å­˜çš„tokenï¼‰
            for i in range(seq.num_cached_blocks, seq.num_blocks):
                start = seq.block_table[i] * self.block_size
                if i != seq.num_blocks - 1:
                    end = start + self.block_size
                else:
                    end = start + seq.last_block_num_tokens
                slot_mapping.extend(list(range(start, end)))
        
        # å¦‚æœæœ‰å‰ç¼€ç¼“å­˜ï¼Œå‡†å¤‡å—è¡¨
        if cu_seqlens_k[-1] > cu_seqlens_q[-1]:
            block_tables = self.prepare_block_tables(seqs)
        
        # è½¬æ¢ä¸ºå¼ é‡
        input_ids = torch.tensor(input_ids, dtype=torch.int64, pin_memory=True).cuda(non_blocking=True)
        positions = torch.tensor(positions, dtype=torch.int64, pin_memory=True).cuda(non_blocking=True)
        cu_seqlens_q = torch.tensor(cu_seqlens_q, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        cu_seqlens_k = torch.tensor(cu_seqlens_k, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        slot_mapping = torch.tensor(slot_mapping, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        
        # è®¾ç½®ä¸Šä¸‹æ–‡
        set_context(True, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, slot_mapping, None, block_tables)
        
        return input_ids, positions

    def prepare_decode(self, seqs: list[Sequence]):
        """å‡†å¤‡Decodeé˜¶æ®µçš„è¾“å…¥"""
        input_ids = []
        positions = []
        slot_mapping = []
        context_lens = []
        
        for seq in seqs:
            input_ids.append(seq.last_token)
            positions.append(len(seq) - 1)
            context_lens.append(len(seq))
            # æ–°tokençš„å­˜å‚¨ä½ç½®
            slot_mapping.append(seq.block_table[-1] * self.block_size + seq.last_block_num_tokens - 1)
        
        input_ids = torch.tensor(input_ids, dtype=torch.int64, pin_memory=True).cuda(non_blocking=True)
        positions = torch.tensor(positions, dtype=torch.int64, pin_memory=True).cuda(non_blocking=True)
        slot_mapping = torch.tensor(slot_mapping, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        context_lens = torch.tensor(context_lens, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
        block_tables = self.prepare_block_tables(seqs)
        
        set_context(False, slot_mapping=slot_mapping, context_lens=context_lens, block_tables=block_tables)
        
        return input_ids, positions

    def prepare_sample(self, seqs: list[Sequence]):
        """å‡†å¤‡é‡‡æ ·å‚æ•°"""
        temperatures = []
        for seq in seqs:
            temperatures.append(seq.temperature)
        temperatures = torch.tensor(temperatures, dtype=torch.float32, pin_memory=True).cuda(non_blocking=True)
        return temperatures

    @torch.inference_mode()
    def run_model(self, input_ids: torch.Tensor, positions: torch.Tensor, is_prefill: bool):
        """
        è¿è¡Œæ¨¡å‹
        
        æ ¹æ®æ¡ä»¶é€‰æ‹©æ‰§è¡Œæ–¹å¼ï¼š
        1. Prefill: ç›´æ¥æ‰§è¡Œï¼ˆè®¡ç®—å¯†é›†ï¼ŒCUDA Graphæ”¶ç›Šå°ï¼‰
        2. Decode + eager: ç›´æ¥æ‰§è¡Œ
        3. Decode + CUDA Graph: ä½¿ç”¨æ•è·çš„graph
        """
        if is_prefill or self.enforce_eager or input_ids.size(0) > 512:
            # ç›´æ¥æ‰§è¡Œ
            return self.model.compute_logits(self.model(input_ids, positions))
        else:
            # ä½¿ç”¨CUDA Graph
            bs = input_ids.size(0)
            context = get_context()
            
            # æ‰¾åˆ°åˆé€‚çš„graphå¤§å°
            graph = self.graphs[next(x for x in self.graph_bs if x >= bs)]
            graph_vars = self.graph_vars
            
            # å¡«å……è¾“å…¥
            graph_vars["input_ids"][:bs] = input_ids
            graph_vars["positions"][:bs] = positions
            graph_vars["slot_mapping"].fill_(-1)
            graph_vars["slot_mapping"][:bs] = context.slot_mapping
            graph_vars["context_lens"].zero_()
            graph_vars["context_lens"][:bs] = context.context_lens
            graph_vars["block_tables"][:bs, :context.block_tables.size(1)] = context.block_tables
            
            # é‡æ”¾graph
            graph.replay()
            
            return self.model.compute_logits(graph_vars["outputs"][:bs])

    def run(self, seqs: list[Sequence], is_prefill: bool) -> list[int]:
        """
        æ‰§è¡Œä¸€æ¬¡æ¨ç†è¿­ä»£
        
        Args:
            seqs: è¦å¤„ç†çš„åºåˆ—
            is_prefill: æ˜¯å¦æ˜¯prefillé˜¶æ®µ
        
        Returns:
            ç”Ÿæˆçš„token IDs
        """
        # å‡†å¤‡è¾“å…¥
        input_ids, positions = self.prepare_prefill(seqs) if is_prefill else self.prepare_decode(seqs)
        temperatures = self.prepare_sample(seqs) if self.rank == 0 else None
        
        # è¿è¡Œæ¨¡å‹
        logits = self.run_model(input_ids, positions, is_prefill)
        
        # é‡‡æ ·ï¼ˆåªåœ¨rank 0æ‰§è¡Œï¼‰
        token_ids = self.sampler(logits, temperatures).tolist() if self.rank == 0 else None
        
        # é‡ç½®ä¸Šä¸‹æ–‡
        reset_context()
        
        return token_ids

    @torch.inference_mode()
    def capture_cudagraph(self):
        """
        æ•è·CUDA Graph
        
        CUDA Graphå¯ä»¥ï¼š
        1. æ¶ˆé™¤kernel launchå¼€é”€
        2. ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
        3. æé«˜å°batchçš„decodeæ•ˆç‡
        
        æ•è·è¿‡ç¨‹ï¼š
        1. warmupè¿è¡Œï¼ˆåˆ†é…å†…å­˜ï¼Œç¡®å®šæ‰§è¡Œè·¯å¾„ï¼‰
        2. å¼€å§‹æ•è·
        3. å†æ¬¡è¿è¡Œï¼ˆè®°å½•æ“ä½œï¼‰
        4. ç»“æŸæ•è·
        """
        config = self.config
        hf_config = config.hf_config
        
        max_bs = min(self.config.max_num_seqs, 512)
        max_num_blocks = (config.max_model_len + self.block_size - 1) // self.block_size
        
        # åˆ›å»ºgraphè¾“å…¥ç¼“å†²åŒº
        input_ids = torch.zeros(max_bs, dtype=torch.int64)
        positions = torch.zeros(max_bs, dtype=torch.int64)
        slot_mapping = torch.zeros(max_bs, dtype=torch.int32)
        context_lens = torch.zeros(max_bs, dtype=torch.int32)
        block_tables = torch.zeros(max_bs, max_num_blocks, dtype=torch.int32)
        outputs = torch.zeros(max_bs, hf_config.hidden_size)
        
        # æ‰¹å¤§å°åˆ—è¡¨ï¼ˆä»å¤§åˆ°å°æ•è·ï¼Œå¯ä»¥å¤ç”¨graph poolï¼‰
        self.graph_bs = [1, 2, 4, 8] + list(range(16, max_bs + 1, 16))
        self.graphs = {}
        self.graph_pool = None
        
        for bs in reversed(self.graph_bs):
            graph = torch.cuda.CUDAGraph()
            
            # è®¾ç½®ä¸Šä¸‹æ–‡
            set_context(False, slot_mapping=slot_mapping[:bs], context_lens=context_lens[:bs], block_tables=block_tables[:bs])
            
            # Warmupè¿è¡Œ
            outputs[:bs] = self.model(input_ids[:bs], positions[:bs])
            
            # å¼€å§‹æ•è·
            with torch.cuda.graph(graph, self.graph_pool):
                outputs[:bs] = self.model(input_ids[:bs], positions[:bs])
            
            # ä¿å­˜graph poolä¾›åç»­å¤ç”¨
            if self.graph_pool is None:
                self.graph_pool = graph.pool()
            
            self.graphs[bs] = graph
            torch.cuda.synchronize()
            reset_context()
        
        # ä¿å­˜graphå˜é‡
        self.graph_vars = dict(
            input_ids=input_ids,
            positions=positions,
            slot_mapping=slot_mapping,
            context_lens=context_lens,
            block_tables=block_tables,
            outputs=outputs,
        )
```

---

### 17. engine/llm_engine.py - LLM å¼•æ“ä¸»ç±»

```python
import atexit                         # ç¨‹åºé€€å‡ºæ—¶æ¸…ç†
from dataclasses import fields        # è·å–dataclasså­—æ®µ
from time import perf_counter         # æ€§èƒ½è®¡æ—¶
from tqdm.auto import tqdm            # è¿›åº¦æ¡
from transformers import AutoTokenizer  # åˆ†è¯å™¨
import torch.multiprocessing as mp    # å¤šè¿›ç¨‹

from nanovllm.config import Config
from nanovllm.sampling_params import SamplingParams
from nanovllm.engine.sequence import Sequence
from nanovllm.engine.scheduler import Scheduler
from nanovllm.engine.model_runner import ModelRunner


class LLMEngine:
    """
    LLMå¼•æ“ - ç”¨æˆ·æ¥å£å±‚
    
    èŒè´£ï¼š
    1. ç®¡ç†é…ç½®å’Œåˆå§‹åŒ–
    2. åè°ƒè°ƒåº¦å™¨å’Œæ¨¡å‹è¿è¡Œå™¨
    3. æä¾›ç”¨æˆ·å‹å¥½çš„ç”Ÿæˆæ¥å£
    
    å¤šGPUæ”¯æŒï¼š
    - ä¸»è¿›ç¨‹ï¼šè°ƒåº¦ + rank 0è®¡ç®—
    - å·¥ä½œè¿›ç¨‹ï¼šrank 1+è®¡ç®—ï¼ˆé€šè¿‡å…±äº«å†…å­˜é€šä¿¡ï¼‰
    """
    
    def __init__(self, model, **kwargs):
        """
        åˆå§‹åŒ–LLMå¼•æ“
        
        Args:
            model: æ¨¡å‹è·¯å¾„
            **kwargs: é…ç½®å‚æ•°
        """
        # ä»kwargsä¸­æå–Configç›¸å…³çš„å‚æ•°
        config_fields = {field.name for field in fields(Config)}
        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
        config = Config(model, **config_kwargs)
        
        # åˆ›å»ºå·¥ä½œè¿›ç¨‹ï¼ˆå¤šGPUæ—¶ï¼‰
        self.ps = []
        self.events = []
        ctx = mp.get_context("spawn")  # spawnæ¨¡å¼é¿å…CUDA forké—®é¢˜
        
        for i in range(1, config.tensor_parallel_size):
            event = ctx.Event()
            # å¯åŠ¨å·¥ä½œè¿›ç¨‹
            process = ctx.Process(target=ModelRunner, args=(config, i, event))
            process.start()
            self.ps.append(process)
            self.events.append(event)
        
        # ä¸»è¿›ç¨‹åˆ›å»ºrank 0çš„ModelRunner
        self.model_runner = ModelRunner(config, 0, self.events)
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)
        config.eos = self.tokenizer.eos_token_id
        
        # åˆ›å»ºè°ƒåº¦å™¨
        self.scheduler = Scheduler(config)
        
        # æ³¨å†Œé€€å‡ºæ¸…ç†
        atexit.register(self.exit)

    def exit(self):
        """æ¸…ç†èµ„æº"""
        self.model_runner.call("exit")
        del self.model_runner
        for p in self.ps:
            p.join()

    def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):
        """
        æ·»åŠ è¯·æ±‚
        
        Args:
            prompt: æç¤ºæ–‡æœ¬æˆ–token IDs
            sampling_params: é‡‡æ ·å‚æ•°
        """
        if isinstance(prompt, str):
            prompt = self.tokenizer.encode(prompt)
        seq = Sequence(prompt, sampling_params)
        self.scheduler.add(seq)

    def step(self):
        """
        æ‰§è¡Œä¸€æ¬¡è°ƒåº¦-æ¨ç†è¿­ä»£
        
        Returns:
            (å®Œæˆçš„è¾“å‡º, tokenæ•°é‡)
            tokenæ•°é‡ > 0: prefillé˜¶æ®µ
            tokenæ•°é‡ < 0: decodeé˜¶æ®µ
        """
        # è°ƒåº¦è¯·æ±‚
        seqs, is_prefill = self.scheduler.schedule()
        
        # æ‰§è¡Œæ¨ç†
        token_ids = self.model_runner.call("run", seqs, is_prefill)
        
        # åå¤„ç†
        self.scheduler.postprocess(seqs, token_ids)
        
        # æ”¶é›†å®Œæˆçš„è¾“å‡º
        outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
        
        # è®¡ç®—tokenæ•°é‡ï¼ˆç”¨äºååé‡ç»Ÿè®¡ï¼‰
        num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)
        
        return outputs, num_tokens

    def is_finished(self):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯·æ±‚éƒ½å·²å®Œæˆ"""
        return self.scheduler.is_finished()

    def generate(
        self,
        prompts: list[str] | list[list[int]],
        sampling_params: SamplingParams | list[SamplingParams],
        use_tqdm: bool = True,
    ) -> list[str]:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆä¸»æ¥å£ï¼‰
        
        Args:
            prompts: æç¤ºåˆ—è¡¨ï¼ˆæ–‡æœ¬æˆ–token IDsï¼‰
            sampling_params: é‡‡æ ·å‚æ•°ï¼ˆå•ä¸ªæˆ–åˆ—è¡¨ï¼‰
            use_tqdm: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if use_tqdm:
            pbar = tqdm(total=len(prompts), desc="Generating", dynamic_ncols=True)
        
        # ç»Ÿä¸€sampling_paramsä¸ºåˆ—è¡¨
        if not isinstance(sampling_params, list):
            sampling_params = [sampling_params] * len(prompts)
        
        # æ·»åŠ æ‰€æœ‰è¯·æ±‚
        for prompt, sp in zip(prompts, sampling_params):
            self.add_request(prompt, sp)
        
        # ä¸»å¾ªç¯
        outputs = {}
        prefill_throughput = decode_throughput = 0.
        
        while not self.is_finished():
            t = perf_counter()
            output, num_tokens = self.step()
            
            if use_tqdm:
                if num_tokens > 0:
                    prefill_throughput = num_tokens / (perf_counter() - t)
                else:
                    decode_throughput = -num_tokens / (perf_counter() - t)
                pbar.set_postfix({
                    "Prefill": f"{int(prefill_throughput)}tok/s",
                    "Decode": f"{int(decode_throughput)}tok/s",
                })
            
            # æ”¶é›†å®Œæˆçš„è¾“å‡º
            for seq_id, token_ids in output:
                outputs[seq_id] = token_ids
                if use_tqdm:
                    pbar.update(1)
        
        # æŒ‰seq_idæ’åºè¾“å‡º
        outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]
        
        # è§£ç ä¸ºæ–‡æœ¬
        outputs = [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} 
                   for token_ids in outputs]
        
        if use_tqdm:
            pbar.close()
        
        return outputs
```

**å¼•æ“ä¸»å¾ªç¯å›¾è§£ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM å¼•æ“ä¸»å¾ªç¯                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  generate()                                                             â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°ç­‰å¾…é˜Ÿåˆ—                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  2. ä¸»å¾ªç¯ while not is_finished():                              â”‚   â”‚
â”‚  â”‚                                                                 â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚     â”‚  scheduler.schedule()                                  â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - ä¼˜å…ˆè°ƒåº¦waitingé˜Ÿåˆ—ï¼ˆprefillï¼‰                      â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - ç„¶åè°ƒåº¦runningé˜Ÿåˆ—ï¼ˆdecodeï¼‰                       â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - èµ„æºä¸è¶³æ—¶æŠ¢å                                       â”‚   â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                          â”‚                                         â”‚   â”‚
â”‚  â”‚                          â–¼                                         â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚     â”‚  model_runner.call("run", seqs, is_prefill)          â”‚   â”‚   â”‚
â”‚  â”‚     â”‚                                                        â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  if prefill:                                           â”‚   â”‚   â”‚
â”‚  â”‚     â”‚    - prepare_prefill()                                 â”‚   â”‚   â”‚
â”‚  â”‚     â”‚    - flash_attn_varlen_func()                          â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  else:                                                 â”‚   â”‚   â”‚
â”‚  â”‚     â”‚    - prepare_decode()                                  â”‚   â”‚   â”‚
â”‚  â”‚     â”‚    - flash_attn_with_kvcache()                         â”‚   â”‚   â”‚
â”‚  â”‚     â”‚                                                        â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - sampler.sample()                                    â”‚   â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                          â”‚                                         â”‚   â”‚
â”‚  â”‚                          â–¼                                         â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚     â”‚  scheduler.postprocess()                             â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - å°†tokenæ·»åŠ åˆ°åºåˆ—                                 â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - æ£€æŸ¥æ˜¯å¦å®Œæˆ                                      â”‚   â”‚   â”‚
â”‚  â”‚     â”‚  - é‡Šæ”¾å®Œæˆçš„åºåˆ—çš„å—                                â”‚   â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  3. è§£ç tokenä¸ºæ–‡æœ¬å¹¶è¿”å›                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 18. llm.py - æœ€ç»ˆæ¥å£

```python
from nanovllm.engine.llm_engine import LLMEngine


class LLM(LLMEngine):
    """
    LLM ç±» - ç”¨æˆ·ç›´æ¥ä½¿ç”¨çš„æ¥å£
    
    ç®€å•ç»§æ‰¿è‡ª LLMEngineï¼Œä¿æŒä¸ vLLM API å…¼å®¹
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        from nanovllm import LLM, SamplingParams
        
        llm = LLM("/path/to/model", tensor_parallel_size=2)
        sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
        outputs = llm.generate(["Hello, world!"], sampling_params)
        print(outputs[0]["text"])
    """
    pass
```

---


## ğŸ¯ æ ¸å¿ƒåŸç†æ€»ç»“

### 1. PagedAttention æ ¸å¿ƒåŸç†

```
é—®é¢˜ï¼šä¼ ç»ŸLLMæ¨ç†çš„KV Cacheç®¡ç†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¼ ç»Ÿæ–¹å¼ï¼šè¿ç»­å†…å­˜åˆ†é…                                                  â”‚
â”‚                                                                         â”‚
â”‚  è¯·æ±‚A: [========]  è¯·æ±‚B: [========]  è¯·æ±‚C: [========]               â”‚
â”‚         â†‘é¢„åˆ†é…max_len                                                  â”‚
â”‚                                                                         â”‚
â”‚  é—®é¢˜ï¼š                                                                  â”‚
â”‚  1. å†…éƒ¨ç¢ç‰‡ï¼šå®é™…ç”Ÿæˆé•¿åº¦ < é¢„åˆ†é…é•¿åº¦                                   â”‚
â”‚  2. å¤–éƒ¨ç¢ç‰‡ï¼šé‡Šæ”¾åäº§ç”Ÿä¸è¿ç»­çš„å°å—                                      â”‚
â”‚  3. æ— æ³•å…±äº«ï¼šç›¸åŒpromptçš„KV Cacheé‡å¤å­˜å‚¨                               â”‚
â”‚                                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PagedAttentionï¼šåˆ†é¡µç®¡ç†                                                â”‚
â”‚                                                                         â”‚
â”‚  ç‰©ç†å†…å­˜ï¼ˆå›ºå®šå¤§å°çš„å—ï¼‰ï¼š                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ [ ][ ][A][A][B][A][C][B][ ][ ][ ][ ][ ][ ][ ]...               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  è¯·æ±‚Aå—è¡¨: [2, 3, 5]  â†’ ç‰©ç†å— 2, 3, 5                                 â”‚
â”‚  è¯·æ±‚Bå—è¡¨: [4, 7]     â†’ ç‰©ç†å— 4, 7                                    â”‚
â”‚  è¯·æ±‚Cå—è¡¨: [6]        â†’ ç‰©ç†å— 6                                       â”‚
â”‚                                                                         â”‚
â”‚  ä¼˜åŠ¿ï¼š                                                                  â”‚
â”‚  1. æ— å†…éƒ¨ç¢ç‰‡ï¼šæŒ‰éœ€åˆ†é…å—                                                â”‚
â”‚  2. æ— å¤–éƒ¨ç¢ç‰‡ï¼šå—å¤§å°å›ºå®šï¼Œå¯å¤ç”¨                                        â”‚
â”‚  3. æ”¯æŒå…±äº«ï¼šç›¸åŒå—å¯ä»¥å¤šä¸ªè¯·æ±‚å…±äº«                                      â”‚
â”‚  4. å‰ç¼€ç¼“å­˜ï¼šé€šè¿‡å“ˆå¸Œå¿«é€ŸåŒ¹é…å‰ç¼€                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Continuous Batching è¿ç»­æ‰¹å¤„ç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¿ç»­æ‰¹å¤„ç† vs é™æ€æ‰¹å¤„ç†                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  é™æ€æ‰¹å¤„ç†ï¼š                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Batch 1: [Req1][Req2][Req3]  â”€â”€â”€â”€â”€â”€â–º å…¨éƒ¨å®Œæˆåæ‰å¤„ç†ä¸‹ä¸€æ‰¹    â”‚   â”‚
â”‚  â”‚  ç­‰å¾…: [Req4][Req5][Req6]...                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  é—®é¢˜ï¼šçŸ­è¯·æ±‚éœ€è¦ç­‰é•¿è¯·æ±‚å®Œæˆï¼ŒGPUç©ºé—²æ—¶é—´å¤š                              â”‚
â”‚                                                                         â”‚
â”‚  è¿ç»­æ‰¹å¤„ç†ï¼š                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Step 1: [Req1][Req2][Req3]  prefill                            â”‚   â”‚
â”‚  â”‚  Step 2: [Req1][Req2][Req3]  decode (Req3å®Œæˆ)                   â”‚   â”‚
â”‚  â”‚  Step 3: [Req1][Req2][Req4]  decode (æ–°è¯·æ±‚Req4åŠ å…¥)             â”‚   â”‚
â”‚  â”‚  Step 4: [Req2][Req4][Req5]  decode (Req1å®Œæˆï¼ŒReq5åŠ å…¥)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ä¼˜åŠ¿ï¼šè¯·æ±‚å®Œæˆåç«‹å³é‡Šæ”¾èµ„æºç»™æ–°è¯·æ±‚ï¼ŒGPUåˆ©ç”¨ç‡é«˜                         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Prefill vs Decode

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Prefill vs Decode å¯¹æ¯”                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Prefillï¼ˆé¢„å¡«å……ï¼‰                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Prompt: "The capital of France is"                             â”‚   â”‚
â”‚  â”‚  Tokens: [The, capital, of, France, is]                         â”‚   â”‚
â”‚  â”‚                     â†“                                           â”‚   â”‚
â”‚  â”‚  ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰tokenï¼Œè®¡ç®—å®ƒä»¬çš„KV Cache                         â”‚   â”‚
â”‚  â”‚  ä½¿ç”¨å¹¶è¡Œè®¡ç®—ï¼Œè®¡ç®—å¯†é›†                                           â”‚   â”‚
â”‚  â”‚  è¾“å‡ºï¼šæœ€åä¸€ä¸ªtokençš„logits                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ç‰¹ç‚¹ï¼š                                                                  â”‚
â”‚  - è®¡ç®—é‡ï¼šO(prompt_lenÂ²)                                              â”‚
â”‚  - å†…å­˜å¸¦å®½ï¼šé«˜ï¼ˆéœ€è¦è¯»å–æ‰€æœ‰æƒé‡ï¼‰                                     â”‚
â”‚  - ä¼˜åŒ–ï¼šFlashAttentionå‡å°‘å†…å­˜è®¿é—®                                     â”‚
â”‚                                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Decodeï¼ˆè§£ç ï¼‰                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  å·²ç”Ÿæˆ: "The capital of France is"                             â”‚   â”‚
â”‚  â”‚  ä¸‹ä¸€æ­¥: é¢„æµ‹ä¸‹ä¸€ä¸ªtoken                                          â”‚   â”‚
â”‚  â”‚                     â†“                                           â”‚   â”‚
â”‚  â”‚  æ¯æ¬¡åªå¤„ç†1ä¸ªtokenï¼Œå¤ç”¨ä¹‹å‰çš„KV Cache                          â”‚   â”‚
â”‚  â”‚  å†…å­˜å¸¦å®½å¯†é›†ï¼ˆéœ€è¦è¯»å–æ‰€æœ‰å±‚çš„KV Cacheï¼‰                         â”‚   â”‚
â”‚  â”‚  è¾“å‡ºï¼šæ–°token                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ç‰¹ç‚¹ï¼š                                                                  â”‚
â”‚  - è®¡ç®—é‡ï¼šO(context_len) æ¯æ­¥                                         â”‚
â”‚  - å†…å­˜å¸¦å®½ï¼šæé«˜ï¼ˆKV Cacheè¯»å–æˆä¸ºç“¶é¢ˆï¼‰                               â”‚
â”‚  - ä¼˜åŒ–ï¼šPagedAttentionã€CUDA Graph                                     â”‚
â”‚                                                                         â”‚
â”‚  ä¸ºä»€ä¹ˆdecodeæ…¢ï¼Ÿ                                                        â”‚
â”‚  - æ¯æ¬¡åªå¤„ç†1ä¸ªtokenï¼Œæ— æ³•åˆ©ç”¨çŸ©é˜µä¹˜çš„å¹¶è¡Œæ€§                            â”‚
â”‚  - éœ€è¦è¯»å–æ‰€æœ‰å±‚çš„KV Cacheï¼ˆå†…å­˜å¸¦å®½ç“¶é¢ˆï¼‰                              â”‚
â”‚  - kernel launchå¼€é”€ç›¸å¯¹è¾ƒå¤§                                             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. å¼ é‡å¹¶è¡Œ (Tensor Parallelism)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å¼ é‡å¹¶è¡ŒåŸç†                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  é—®é¢˜ï¼šæ¨¡å‹å¤ªå¤§ï¼Œå•ä¸ªGPUæ”¾ä¸ä¸‹                                           â”‚
â”‚  ä¾‹å¦‚ï¼š70Bæ¨¡å‹ï¼ŒFP16éœ€è¦140GBæ˜¾å­˜                                        â”‚
â”‚                                                                         â”‚
â”‚  è§£å†³æ–¹æ¡ˆï¼šå°†æƒé‡åˆ‡åˆ†åˆ°å¤šä¸ªGPU                                           â”‚
â”‚                                                                         â”‚
â”‚  å±‚1ï¼ˆColumn Parallelï¼‰ï¼š                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  è¾“å…¥ X         â”‚                                                    â”‚
â”‚  â”‚  [batch, 4096]  â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚           â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚     â–¼           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚GPU 0  â”‚  â”‚GPU 1  â”‚                                                   â”‚
â”‚  â”‚W1     â”‚  â”‚W2     â”‚  W = [W1; W2] è¡Œæ‹¼æ¥                              â”‚
â”‚  â”‚[4096, â”‚  â”‚[4096, â”‚                                                   â”‚
â”‚  â”‚ 2048] â”‚  â”‚ 2048] â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜                                                   â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â–¼          â–¼                                                       â”‚
â”‚   Y1=X@W1    Y2=X@W2                                                    â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â–¼                                                             â”‚
â”‚       Y = [Y1, Y2]  è¾“å‡ºç»´åº¦ç¿»å€                                         â”‚
â”‚                                                                         â”‚
â”‚  å±‚2ï¼ˆRow Parallelï¼‰ï¼š                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚  è¾“å…¥ Y         â”‚                                                    â”‚
â”‚  â”‚  [batch, 4096]  â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚           â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚     â–¼           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚GPU 0  â”‚  â”‚GPU 1  â”‚                                                   â”‚
â”‚  â”‚Y1     â”‚  â”‚Y2     â”‚  Y = [Y1, Y2] åˆ—æ‹¼æ¥                              â”‚
â”‚  â”‚W1     â”‚  â”‚W2     â”‚  W = [W1, W2] åˆ—æ‹¼æ¥                              â”‚
â”‚  â”‚[2048, â”‚  â”‚[2048, â”‚                                                   â”‚
â”‚  â”‚ 4096] â”‚  â”‚ 4096] â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜                                                   â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â–¼          â–¼                                                       â”‚
â”‚   Z1=Y1@W1   Z2=Y2@W2                                                   â”‚
â”‚      â”‚          â”‚                                                       â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â–¼                                                             â”‚
â”‚       Z = Z1 + Z2  (all-reduce)                                         â”‚
â”‚                                                                         â”‚
â”‚  é€šä¿¡é‡åˆ†æï¼š                                                            â”‚
â”‚  - Column Parallel: 0 é€šä¿¡                                               â”‚
â”‚  - Row Parallel: 1æ¬¡ all-reduceï¼ˆæ•°æ®é‡ = batch * hidden_sizeï¼‰         â”‚
â”‚  æ¯å±‚éœ€è¦1æ¬¡ all-reduce                                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. CUDA Graph ä¼˜åŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CUDA Graph åŸç†                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  é—®é¢˜ï¼šå°batch decodeçš„kernel launchå¼€é”€                                 â”‚
â”‚                                                                         â”‚
â”‚  ä¼ ç»Ÿæ‰§è¡Œï¼š                                                              â”‚
â”‚  CPU:  launch kernel1 â”€â”€â–º launch kernel2 â”€â”€â–º launch kernel3 ...        â”‚
â”‚  GPU:  [kernel1]         [kernel2]         [kernel3]                    â”‚
â”‚           â†‘ ç©ºé—²æ—¶é—´ â†‘ ç©ºé—²æ—¶é—´ â†‘                                       â”‚
â”‚  æ¯æ¬¡launchéƒ½æœ‰CPU-GPUåŒæ­¥å¼€é”€                                           â”‚
â”‚                                                                         â”‚
â”‚  CUDA Graphï¼š                                                            â”‚
â”‚  1. å½•åˆ¶é˜¶æ®µï¼ˆä¸€æ¬¡ï¼‰ï¼š                                                    â”‚
â”‚     CPU:  begin capture â”€â”€â–º run kernels â”€â”€â–º end capture                â”‚
â”‚     GPU:  [k1][k2][k3][k4][k5]...  è®°å½•æ‰€æœ‰æ“ä½œ                         â”‚
â”‚                                                                         â”‚
â”‚  2. é‡æ”¾é˜¶æ®µï¼ˆå¤šæ¬¡ï¼‰ï¼š                                                    â”‚
â”‚     CPU:  graph.replay()  â† ä¸€æ¬¡è°ƒç”¨é‡æ”¾æ‰€æœ‰kernel                      â”‚
â”‚     GPU:  [k1][k2][k3][k4][k5]...  è¿ç»­æ‰§è¡Œ                             â”‚
â”‚                                                                         â”‚
â”‚  ä¼˜åŠ¿ï¼š                                                                  â”‚
â”‚  - æ¶ˆé™¤CPU launchå¼€é”€                                                    â”‚
â”‚  - kernelä¹‹é—´æ— ç©ºé—²ï¼ŒGPUåˆ©ç”¨ç‡100%                                        â”‚
â”‚  - å¯ä»¥ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼                                                   â”‚
â”‚                                                                         â”‚
â”‚  é™åˆ¶ï¼š                                                                  â”‚
â”‚  - è¾“å…¥è¾“å‡ºå¤§å°å¿…é¡»å›ºå®š                                                   â”‚
â”‚  - ä¸æ”¯æŒåŠ¨æ€æ§åˆ¶æµ                                                       â”‚
â”‚  - éœ€è¦ä¸ºä¸åŒbatch sizeåˆ†åˆ«æ•è·                                          â”‚
â”‚                                                                         â”‚
â”‚  Nano-vLLMå®ç°ï¼š                                                         â”‚
â”‚  - æ•è·batch size: 1, 2, 4, 8, 16, 32, ..., 512                          â”‚
â”‚  - è¿è¡Œæ—¶é€‰æ‹©æœ€æ¥è¿‘çš„graph                                                â”‚
â”‚  - è¶…è¿‡512ä½¿ç”¨eageræ¨¡å¼                                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ é¢è¯•æ ¸å¿ƒé—®é¢˜ä¸è§£ç­”

### åŸºç¡€æ¦‚å¿µ

**Q1: ä»€ä¹ˆæ˜¯KV Cacheï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ**

```
A: KV Cacheæ˜¯Transformeræ¨ç†ä¸­çš„å…³é”®ä¼˜åŒ–ã€‚

åŸç†ï¼š
- åœ¨decodeé˜¶æ®µï¼Œæ¯ä¸ªæ–°tokenéœ€è¦ä¸ä¹‹å‰æ‰€æœ‰tokenè®¡ç®—æ³¨æ„åŠ›
- å¦‚æœä¸ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—ä¹‹å‰tokençš„Keyå’ŒValue
- KV Cacheå­˜å‚¨äº†æ¯ä¸ªtokençš„Kå’ŒVï¼Œé¿å…é‡å¤è®¡ç®—

è®¡ç®—ï¼š
- æ— KV Cache: ç”ŸæˆNä¸ªtokenéœ€è¦ O(NÂ³) è®¡ç®—
- æœ‰KV Cache: åªéœ€è¦ O(NÂ²) è®¡ç®—

å†…å­˜å¼€é”€ï¼š
- æ¯å±‚: 2(K+V) * seq_len * num_kv_heads * head_dim * sizeof(dtype)
- ä¾‹å¦‚ï¼š32å±‚ï¼Œseq_len=4096ï¼ŒGQA 8å¤´ï¼Œhead_dim=128ï¼Œfp16
  éœ€è¦: 32 * 2 * 4096 * 8 * 128 * 2B = 512MB
```

**Q2: PagedAttentionè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

```
A: PagedAttentionè§£å†³äº†ä¼ ç»ŸKV Cacheç®¡ç†çš„ä¸‰ä¸ªé—®é¢˜ï¼š

1. å†…éƒ¨ç¢ç‰‡ï¼š
   - ä¼ ç»Ÿï¼šé¢„åˆ†é…max_lenï¼Œå®é™…ä½¿ç”¨å¯èƒ½å¾ˆå°‘
   - PagedAttentionï¼šæŒ‰éœ€åˆ†é…å›ºå®šå¤§å°çš„å—

2. å¤–éƒ¨ç¢ç‰‡ï¼š
   - ä¼ ç»Ÿï¼šé‡Šæ”¾åäº§ç”Ÿä¸è¿ç»­çš„å°å—
   - PagedAttentionï¼šå—å¤§å°å›ºå®šï¼Œå¯ä»¥ä»»æ„å¤ç”¨

3. æ— æ³•å…±äº«ï¼š
   - ä¼ ç»Ÿï¼šæ¯ä¸ªè¯·æ±‚ç‹¬ç«‹çš„è¿ç»­å†…å­˜
   - PagedAttentionï¼šç›¸åŒå†…å®¹çš„å—å¯ä»¥å…±äº«ï¼ˆcopy-on-writeï¼‰

æ ¸å¿ƒè®¾è®¡ï¼š
- ç‰©ç†å—ï¼šå›ºå®šå¤§å°ï¼ˆå¦‚256 tokensï¼‰
- å—è¡¨ï¼šé€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„
- å¼•ç”¨è®¡æ•°ï¼šæ”¯æŒå—å…±äº«
- å“ˆå¸Œç¼“å­˜ï¼šå‰ç¼€åŒ¹é…åŠ é€Ÿ
```

**Q3: ä»€ä¹ˆæ˜¯Continuous Batchingï¼Ÿ**

```
A: Continuous Batchingï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰æ˜¯ä¸€ç§åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ã€‚

ä¼ ç»Ÿé™æ€æ‰¹å¤„ç†ï¼š
- ä¸€æ‰¹è¯·æ±‚ä¸€èµ·å¼€å§‹ï¼Œä¸€èµ·ç»“æŸ
- çŸ­è¯·æ±‚éœ€è¦ç­‰é•¿è¯·æ±‚å®Œæˆ
- GPUåˆ©ç”¨ç‡ä½

è¿ç»­æ‰¹å¤„ç†ï¼š
- æ¯ä¸ªiterationé‡æ–°è°ƒåº¦
- è¯·æ±‚å®Œæˆåç«‹å³é‡Šæ”¾èµ„æº
- æ–°è¯·æ±‚å¯ä»¥ç«‹å³åŠ å…¥

å®ç°è¦ç‚¹ï¼š
1. åŒºåˆ†prefillå’Œdecodeé˜¶æ®µ
2. èµ„æºä¸è¶³æ—¶æŠ¢å ï¼ˆpreemptï¼‰è¿è¡Œä¸­çš„è¯·æ±‚
3. è¢«æŠ¢å çš„è¯·æ±‚æ”¾å›ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨

ä¼˜åŠ¿ï¼š
- æé«˜GPUåˆ©ç”¨ç‡
- é™ä½å¹³å‡å»¶è¿Ÿ
- æ”¯æŒé«˜å¹¶å‘
```

**Q4: FlashAttentionçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ**

```
A: FlashAttentionæ˜¯ä¸€ç§IOæ„ŸçŸ¥çš„æ³¨æ„åŠ›ç®—æ³•ã€‚

ä¼ ç»Ÿæ³¨æ„åŠ›çš„å†…å­˜ç“¶é¢ˆï¼š
- éœ€è¦å­˜å‚¨NÃ—Nçš„æ³¨æ„åŠ›çŸ©é˜µ
- å†…å­˜è®¿é—®é‡æ˜¯è®¡ç®—é‡çš„æ•°å€

FlashAttentionçš„æ ¸å¿ƒæ€æƒ³ï¼š
1. åˆ†å—è®¡ç®—ï¼šå°†Qã€Kã€Våˆ†æˆå°å—
2. Softmaxç¨³å®šåŒ–ï¼šåœ¨çº¿è®¡ç®—softmaxçš„å½’ä¸€åŒ–å› å­
3. é‡è®¡ç®—ï¼šåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—æ³¨æ„åŠ›ï¼Œä¸å­˜å‚¨ä¸­é—´ç»“æœ

ç®—æ³•æ­¥éª¤ï¼š
for each block of Q:
    for each block of K, V:
        1. è®¡ç®— S = Q @ K^T
        2. è®¡ç®— P = softmax(S)
        3. ç´¯åŠ  O += P @ V

ä¼˜åŠ¿ï¼š
- å‡å°‘HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰è®¿é—®
- è®¡ç®—å’Œå†…å­˜è®¿é—®å¹³è¡¡
- æ”¯æŒæ›´é•¿çš„åºåˆ—
```

**Q5: å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œçš„åŒºåˆ«ï¼Ÿ**

```
A: ä¸¤ç§éƒ½æ˜¯æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œä½†åˆ‡åˆ†ç»´åº¦ä¸åŒã€‚

å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰ï¼š
- åˆ‡åˆ†å•ä¸ªå±‚çš„æƒé‡
- ä¾‹å¦‚ï¼šå°†çº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦åˆ‡åˆ†åˆ°2ä¸ªGPU
- é€šä¿¡ï¼šæ¯å±‚éœ€è¦1-2æ¬¡all-reduce
- é€‚ç”¨ï¼šå•èŠ‚ç‚¹å¤šGPUï¼Œå»¶è¿Ÿæ•æ„Ÿ

æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰ï¼š
- åˆ‡åˆ†ä¸åŒå±‚åˆ°ä¸åŒGPU
- ä¾‹å¦‚ï¼šGPU 0è´Ÿè´£å±‚0-3ï¼ŒGPU 1è´Ÿè´£å±‚4-7
- é€šä¿¡ï¼šåªéœ€è¦ä¼ é€’æ¿€æ´»å€¼
- é€‚ç”¨ï¼šå¤šèŠ‚ç‚¹ï¼Œååæ•æ„Ÿ

Nano-vLLMåªå®ç°äº†å¼ é‡å¹¶è¡Œï¼Œå› ä¸ºï¼š
1. ä»£ç ç®€æ´ï¼ˆçº¦1200è¡Œï¼‰
2. å•èŠ‚ç‚¹åœºæ™¯æœ€å¸¸è§
3. å¼ é‡å¹¶è¡Œå¯¹å»¶è¿Ÿä¼˜åŒ–æ›´å¥½
```

**Q6: RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰çš„åŸç†ï¼Ÿ**

```
A: RoPEé€šè¿‡æ—‹è½¬çŸ©é˜µå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°Qå’ŒKä¸­ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- å¯¹Qå’ŒKçš„æ¯ä¸€ç»´é…å¯¹è¿›è¡Œæ—‹è½¬
- æ—‹è½¬è§’åº¦ = position Ã— frequency

æ•°å­¦å…¬å¼ï¼š
å¯¹äºäºŒç»´å‘é‡ [x1, x2]ï¼Œæ—‹è½¬Î¸ï¼š
[x1']   [cosÎ¸  -sinÎ¸] [x1]
[x2'] = [sinÎ¸   cosÎ¸] [x2]

æ‰©å±•åˆ°é«˜ç»´ï¼š
- å°†head_dimç»´åˆ†æˆhead_dim/2å¯¹
- æ¯å¯¹åº”ç”¨ä¸åŒé¢‘ç‡çš„æ—‹è½¬
- é¢‘ç‡ï¼šÎ¸_i = base^(-2i/head_dim)

ä¼˜åŠ¿ï¼š
1. ç›¸å¯¹ä½ç½®ï¼šdot(q_m, k_n) åªä¸(m-n)æœ‰å…³
2. é•¿åºåˆ—å¤–æ¨ï¼šå¯ä»¥å¤„ç†è¶…è¿‡è®­ç»ƒé•¿åº¦çš„åºåˆ—
3. ä¸æ³¨æ„åŠ›å¤©ç„¶ç»“åˆ
```

**Q7: ä¸ºä»€ä¹ˆdecodeé˜¶æ®µæ¯”prefillæ…¢ï¼Ÿ**

```
A: Decodeé˜¶æ®µæ…¢çš„åŸå› ï¼š

1. è®¡ç®—å¹¶è¡Œåº¦ä½ï¼š
   - Prefillï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªtokenï¼ŒçŸ©é˜µä¹˜å¯ä»¥å¹¶è¡Œ
   - Decodeï¼šä¸€æ¬¡åªå¤„ç†1ä¸ªtokenï¼Œæ— æ³•åˆ©ç”¨çŸ©é˜µä¹˜ä¼˜åŒ–

2. å†…å­˜å¸¦å®½ç“¶é¢ˆï¼š
   - Prefillï¼šè®¡ç®—å¯†é›†ï¼Œä¸»è¦æ—¶é—´èŠ±åœ¨çŸ©é˜µä¹˜
   - Decodeï¼šå†…å­˜å¸¦å®½å¯†é›†ï¼Œéœ€è¦è¯»å–æ‰€æœ‰å±‚çš„KV Cache

3. Kernel launchå¼€é”€ï¼š
   - Decodeï¼šæ¯ä¸ªtokenéœ€è¦launchå¤šä¸ªkernel
   - å°batchæ—¶ï¼Œlaunchå¼€é”€å æ¯”å¤§

ä¼˜åŒ–æ–¹æ³•ï¼š
1. PagedAttentionï¼šä¼˜åŒ–KV Cacheè®¿é—®
2. CUDA Graphï¼šå‡å°‘kernel launchå¼€é”€
3. é‡åŒ–ï¼šå‡å°‘å†…å­˜å¸¦å®½éœ€æ±‚
4. æ¨æµ‹è§£ç ï¼šç”¨draftæ¨¡å‹åŠ é€Ÿ

æ•°æ®å¯¹æ¯”ï¼ˆå…¸å‹å€¼ï¼‰ï¼š
- Prefillååé‡ï¼š1000-10000 tokens/s
- Decodeååé‡ï¼š50-200 tokens/s
```

**Q8: CUDA Graphåœ¨LLMæ¨ç†ä¸­çš„ä½œç”¨ï¼Ÿ**

```
A: CUDA Graphä¼˜åŒ–å°batch decodeçš„æ€§èƒ½ã€‚

åŸç†ï¼š
1. å½•åˆ¶ï¼šè®°å½•ä¸€æ¬¡å®Œæ•´çš„kernelæ‰§è¡Œåºåˆ—
2. é‡æ”¾ï¼šåç»­ç›´æ¥é‡æ”¾å½•åˆ¶çš„åºåˆ—

ä¼˜åŠ¿ï¼š
- æ¶ˆé™¤CPU launchå¼€é”€
- å‡å°‘GPUç©ºé—²æ—¶é—´
- å¯ä»¥ä¼˜åŒ–å†…å­˜è®¿é—®

é™åˆ¶ï¼š
- è¾“å…¥è¾“å‡ºå¤§å°å¿…é¡»å›ºå®š
- ä¸æ”¯æŒåŠ¨æ€æ§åˆ¶æµ
- éœ€è¦ä¸ºä¸åŒbatch sizeåˆ†åˆ«æ•è·

Nano-vLLMçš„å®ç°ï¼š
- æ•è·batch size: 1, 2, 4, 8, 16, 32, ..., 512
- è¿è¡Œæ—¶é€‰æ‹©æœ€æ¥è¿‘çš„graph
- è¶…è¿‡512ä½¿ç”¨eageræ¨¡å¼

æ•ˆæœï¼š
- å°batch decodeååé‡æå‡20-50%
- å¯¹prefillæ•ˆæœä¸æ˜æ˜¾ï¼ˆè®¡ç®—å¯†é›†ï¼‰
```

---

## ğŸ“– å­¦ä¹ å»ºè®®

### 1. ä»£ç é˜…è¯»é¡ºåº

```
ç¬¬ä¸€é˜¶æ®µï¼ˆå»ºç«‹è®¤çŸ¥ï¼‰ï¼š
â”œâ”€â”€ sampling_params.py    # 5åˆ†é’Ÿ
â”œâ”€â”€ config.py             # 10åˆ†é’Ÿ
â””â”€â”€ utils/context.py      # 10åˆ†é’Ÿ

ç¬¬äºŒé˜¶æ®µï¼ˆæ ¸å¿ƒæ•°æ®ç»“æ„ï¼‰ï¼š
â”œâ”€â”€ engine/sequence.py    # 20åˆ†é’Ÿ
â””â”€â”€ engine/block_manager.py  # 30åˆ†é’Ÿ â­é‡ç‚¹

ç¬¬ä¸‰é˜¶æ®µï¼ˆè°ƒåº¦ç³»ç»Ÿï¼‰ï¼š
â””â”€â”€ engine/scheduler.py   # 30åˆ†é’Ÿ â­é‡ç‚¹

ç¬¬å››é˜¶æ®µï¼ˆæ¨¡å‹å±‚ï¼‰ï¼š
â”œâ”€â”€ layers/linear.py      # 30åˆ†é’Ÿ â­é‡ç‚¹ï¼ˆå¼ é‡å¹¶è¡Œï¼‰
â”œâ”€â”€ layers/layernorm.py   # 15åˆ†é’Ÿ
â”œâ”€â”€ layers/activation.py  # 10åˆ†é’Ÿ
â”œâ”€â”€ layers/rotary_embedding.py  # 20åˆ†é’Ÿ
â”œâ”€â”€ layers/attention.py   # 30åˆ†é’Ÿ â­é‡ç‚¹
â”œâ”€â”€ layers/embed_head.py  # 20åˆ†é’Ÿ
â””â”€â”€ layers/sampler.py     # 10åˆ†é’Ÿ

ç¬¬äº”é˜¶æ®µï¼ˆæ¨¡å‹æ¶æ„ï¼‰ï¼š
â”œâ”€â”€ models/qwen3.py       # 30åˆ†é’Ÿ
â””â”€â”€ utils/loader.py       # 15åˆ†é’Ÿ

ç¬¬å…­é˜¶æ®µï¼ˆå¼•æ“æ ¸å¿ƒï¼‰ï¼š
â”œâ”€â”€ engine/model_runner.py  # 40åˆ†é’Ÿ â­é‡ç‚¹
â”œâ”€â”€ engine/llm_engine.py    # 20åˆ†é’Ÿ
â””â”€â”€ llm.py                # 5åˆ†é’Ÿ
```

### 2. åŠ¨æ‰‹å®è·µå»ºè®®

1. **å•æ­¥è°ƒè¯•**ï¼šåœ¨å…³é”®å‡½æ•°æ‰“æ–­ç‚¹ï¼Œè§‚å¯Ÿæ•°æ®æµåŠ¨
2. **ä¿®æ”¹å‚æ•°**ï¼šæ”¹å˜block_sizeã€max_num_seqsç­‰ï¼Œè§‚å¯Ÿå½±å“
3. **æ·»åŠ æ—¥å¿—**ï¼šåœ¨è°ƒåº¦ã€å†…å­˜åˆ†é…å¤„æ‰“å°çŠ¶æ€
4. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨nvprofåˆ†ækernelæ‰§è¡Œæ—¶é—´

### 3. æ·±å…¥å­¦ä¹ æ–¹å‘

1. **FlashAttention**ï¼šé˜…è¯»åŸå§‹è®ºæ–‡å’ŒTritonå®ç°
2. **Tritonç¼–ç¨‹**ï¼šå­¦ä¹ GPU kernelå¼€å‘
3. **é‡åŒ–æ¨ç†**ï¼šINT8/INT4é‡åŒ–å®ç°
4. **æ¨æµ‹è§£ç **ï¼šDraft-then-verifyæœºåˆ¶
5. **å¤šæ¨¡æ€**ï¼šæ‰©å±•åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹

---

## ğŸ”— å‚è€ƒèµ„æ–™

1. **vLLMè®ºæ–‡**: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
2. **FlashAttentionè®ºæ–‡**: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
3. **RoPEè®ºæ–‡**: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
4. **Tensor Parallelism**: [Megatron-LM: Training Multi-Billion Parameter Language Models](https://arxiv.org/abs/1909.08053)
5. **Nano-vLLM GitHub**: https://github.com/GeeeekExplorer/nano-vllm

---

*æœ¬æ–‡æ¡£åŸºäº Nano-vLLM é¡¹ç›®ï¼ˆçº¦1200è¡Œä»£ç ï¼‰ç¼–å†™ï¼Œæ˜¯å­¦ä¹ VLLMæ¨ç†å¼•æ“çš„å®Œæ•´æŒ‡å—ã€‚*
